<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>日记6-5</title>
    <url>/2023/06/05/2023-06-05/</url>
    <content><![CDATA[<p>23.6.5</p>
<p>在基物课上做基物编程作业，想着快速解决这十个编程题，但又反复纠结该咋提交这一大堆东西。</p>
<p>突然想到可以用jupyter notebook，每个题分个格子，太酷啦。</p>
<p>于是光速写完代码，突然发现一运行内核就崩溃。</p>
<p>迅速改本地运行，但是调用matplotlib画不了图。</p>
<p>于是乎谷歌、百度、CSDN、GPT4齐上。</p>
<p>经过一段时间……</p>
<p>终于在一个网络的角落发现了，升级numpy可解。</p>
<p>绝了……</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记6-8</title>
    <url>/2023/06/08/2023-06-08/</url>
    <content><![CDATA[<p>23.6.8</p>
<p>焦急复习材科基中……</p>
<span id="more"></span>

<p>对着答案尝试理解习题，突然发现旁边有个注释对答案提出了异议，却和我的想法不谋而合。于是喜不自禁，正准备继续做时，无意间发现这条注释来自于十二年之前，也是一个期末周的中午。思绪不由地回到那年夏天，留下这行字的人，又是谁呢。</p>
<p>等我成为研究生&#x2F;博士生以后，我也要当材科基的助教！立誓于此，拭目以待。</p>
<p>首先从期末考试A-及以上开始！</p>
<p>附图如下：</p>
<img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20230608155314202.png" alt="image-20230608155314202" style="zoom:50%;" />]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记6-11</title>
    <url>/2023/06/11/2023-06-11/</url>
    <content><![CDATA[<p>新版校园卡到手了，相较于室温时效两年之久，表面渗生物质处理，洗澡时喷丸强化，平时偶尔加工硬化的老校园卡来说，各项性能还有待提高。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记6-17</title>
    <url>/2023/06/17/2023-06-17/</url>
    <content><![CDATA[<p>平凡的很累的一天，拿什么来记住呢？</p>
<span id="more"></span>
<p>6.17，基础物理考试的前两天</p>
<p>在图书馆复习了整整一天，Kim老师教的热力学里，熵是绝对的明星，H、F、G都黯然失色。<br>搞懂了看似很厉害的勒让德变换以及几个破题<br>还有 茉莉初雪奶茶<br>一天就这样过去了</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-17</title>
    <url>/2023/09/17/2023-09-17/</url>
    <content><![CDATA[<p>第一次用Obsidian，感觉还挺好诶，不过笔记的精髓是文字。</p>
<span id="more"></span>
<p>今天zyt同志排球大胜利（以及昨天也是），不得不说还是有点水平在的。<br>有关我的论文啊啊啊啊啊，实在是一言难尽。总怀疑实验数据本身有问题，同样的算法，环境也很接近，但是表现却很差。Ensemble理论上讲应该是最稳定的叭……</p>
<p>可恶。</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-20</title>
    <url>/2023/09/20/2023-09-20/</url>
    <content><![CDATA[<p>好好好，anaconda你这么玩是吧！</p>
<span id="more"></span>
<p>numpy.dot()整整困扰了我一天！！！<br>这下好了，全给你删了，看你怎么办。<br>昨天我就不应该良心发现，帮你升级，你看看你自己，像什么样子。<br>怒了，直接退课！</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-21</title>
    <url>/2023/09/21/2023-09-21/</url>
    <content><![CDATA[<p>今天天气好好啊啊啊啊啊啊啊！</p>
<span id="more"></span>
<p>吃完中饭从观畴出来，风有点凉，但是阳光照在身上又很舒服。<br>呃呃呃啊啊啊，实在是太适合躺在草坪上睡觉了啊啊啊啊啊啊。<br>可惜的是，天不遂人愿。俗务缠身，也许只有七八十岁才能享受生活叭。<br>今天早上由于ChatGPT抽风，试了一下文心一言，还挺不错的诶。<br>果然要先相信，再质疑。<br>下午准备把机器学习和深度学习讲的东西整个再看一下。<br>完了以后晚上复习一下量子与统计，把作业做了。<br>接下来视唱练耳什么的，带个键盘去操场。<br>和zyt操场溜达溜达。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-22</title>
    <url>/2023/09/22/2023-09-22/</url>
    <content><![CDATA[<p>刚一开学就压力颇大啊<br>感觉也没选什么课但就是莫名烦躁，感觉事情很多<br>可恶<br>今天下午加晚上得把材料分析与表征给看个七七八八，然后再沉下心来把量子与统计看了。然后把衣服洗一下，然后……可能去理个发什么的<br>呃呃呃啊啊啊，不得不说肯德基的炸鸡有点东西，还有就是什么时候才能趴在桌子上睡觉不流口水啊，keso！<br>急急急，怎么就周五了？SRT还啥都没有啊啊啊啊！<br>完蛋……<br>在紫荆书咖氪了300，然后发现网极度不好，后悔了……<br>马上就周一了，视唱练耳再度要来了啊啊啊啊啊！<br>可恶的科协会议啊啊啊啊啊。<br>晚上再去操场溜达溜达。<br>溜达到了快12点呃呃呃。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-19</title>
    <url>/2023/09/19/2023-09-19/</url>
    <content><![CDATA[<p>美好的一天开始啦！</p>
<span id="more"></span>
<p>开局诸事不宜，8:00-8:45极致折磨。<br>机器学习课，累累累累累。感觉好难，需要回头再看看。<br>组会好累，累累累累累。<br>幸好这学期周四放假，否则真的要寄了。<br>帮yyx装了python和anaconda。</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-18</title>
    <url>/2023/09/18/2023-09-18/</url>
    <content><![CDATA[<p>开局啥都听不懂，忽闻改培养方案，寄。</p>
<span id="more"></span>
<p>似乎在课上听到了了不得的东西，然而并不知道在说什么，寄。<br>开篇PPT，魔法少女小圆魔女视角。终局PPT，Clannad弦理论。<br>啥都不会，B站大学启动。吴氏网、标准点阵、倒易空间。<br>学了一中午，PPT和教参平台上的图全是错的，助教那的祖传课本却是对的，寄。<br>呃呃呃要开始上第一节乐理课了，开始紧张了，寄。<br>我测，直接自杀了。</p>
<p>以下是朋友圈记录<br>7:30</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">每年保留节目</span><br><span class="line">开学第一天走错教室去上研究生课</span><br><span class="line">​今天的主题是《数字图像技术及应用》</span><br><span class="line">比起上学期的《高级半导体物理》​已经好很多了</span><br><span class="line">未来可期</span><br></pre></td></tr></table></figure>
<p>8：03</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">大结局</span><br><span class="line">​在上《数字图像处理》课的时候</span><br><span class="line">​被《深度学习》老师点名回答问题了</span><br><span class="line">​于是zygg如实回答了我上个朋友圈里写的东西</span><br><span class="line">​于是教室里洋溢着欢乐的气息</span><br></pre></td></tr></table></figure>
<p>终于结束了，寄。</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-24</title>
    <url>/2023/09/24/2023-09-24/</url>
    <content><![CDATA[<p>荒废时光啊荒废时光啊荒废时光啊荒废时光啊<br>急了<br>Reamke的一天，看到VS Code 版本升到了3.11.4，感觉从3.9.6是翻篇的一集（虽然不知道有啥变化就是了）</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-23</title>
    <url>/2023/09/23/2023-09-23/</url>
    <content><![CDATA[<p>7点被闹钟吵醒，开启周末。<br>（mzk不知道几点已经悄然离去，mod）<br>不过zyt同志也醒了。<br>早上起来先听了倒易点阵，check了一下工图作业，over。<br>qm同学今天考托福，鼓励一下。<br>ChatGPT又又又上不去了，四处转悠了一圈，发现得开无痕模式，层层加码了属于是。<br>今天主要任务是把论文头图给画了，然后把机器学习作业给做了，顺便再艰难地尝试理解一下量统。<br>和wsy死活开不了情侣空间，奇怪？<br>呃呃呃啊啊啊一天结束了。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-25</title>
    <url>/2023/09/25/2023-09-25/</url>
    <content><![CDATA[<p>早上好好好冷啊<br>昨天吃的泡芙绝对有毒<br>今天下午又要视唱练耳test了，急急急急急<br>下午把量统作业做了以后，把图画完然后光速开始练习<br>时间还挺紧的aaaaa<br>可恶，才听了一小节课就不想听了<br>晕<br>感觉事情好好好好好多啊<br>好好好，视唱练耳test过了！！！<br>感谢怡宝！！感谢世界！！</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-27</title>
    <url>/2023/09/28/2023-09-27/</url>
    <content><![CDATA[<p>很煎熬的一天<br>量统下课以后还是得好好复习啊啊啊啊，上课人超级多，怎么怎么怎么会这样。<br>工图，极致折磨的三节课，服了。<br>下午的网球课，感觉自己好烂，不太能配合jf同志。<br>之后请zyt同志去观畴三楼吃了吃好吃的，感觉环境还不错，人也不是很多，10.2那天可以请爷爷奶奶来这里吃。不过五个人的桌子不知道好不好找。吃饭的时候发现了我大一的优秀文学作品，但是yt同志的似乎更为优秀。<br>顺便，被观畴三楼优秀员工szy盒出来了，寄。<br>晚上，写了写量统作业。mm同志问了群速的问题，但是，我也不懂，倒。<br>视唱练耳，反复练了两条音阶，但是听音感觉还是不太行，呃呃呃啊啊啊，不知道是不是方法问题。<br>zyt非要呆在礼射研习会的棚子里，呃呃呃啊啊啊太离谱了。<br>贝斯得弦真的好粗啊啊啊啊啊。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-26</title>
    <url>/2023/09/26/2023-09-26/</url>
    <content><![CDATA[<p>哦哈哟！<br>为什么早上这么冷。<br>再冷我就不上学了，怒。<br>解决了GIthub Pages Custom Domain失效的原因<br>原来是source内要添加内含网址的CNAME文件<br>Git连Github失败有可能是代理端口不对，改一下就可以<br>晚上吃一下观畴三楼，测试一下</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-29</title>
    <url>/2023/09/30/2023-09-29/</url>
    <content><![CDATA[<p>一觉睡到大中午，呃呃呃啊啊啊<br>下午拎包出去溜达溜达<br>什刹海+月坛</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-28</title>
    <url>/2023/09/28/2023-09-28/</url>
    <content><![CDATA[<p>开局准备做机器学习作业。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-1</title>
    <url>/2023/10/01/2023-10-01/</url>
    <content><![CDATA[<p>急急急，明天母上大人就要来了<br>呃呃呃啊啊啊呃呃呃啊啊啊<br>疯狂理宿舍中<br>唉唉唉唉唉唉<br>买点东西吃吃</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记9-30</title>
    <url>/2023/09/30/2023-09-30/</url>
    <content><![CDATA[<p>求求你别刷B站了！！！</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-6</title>
    <url>/2023/10/06/2023-10-06/</url>
    <content><![CDATA[<p>大模型折腾了一天<br>果然是越忙越喜欢折腾<br>把工图作业交了，感觉考试搞不好会很难<br>还有好多作业，奇怪，国庆期间都干了些啥？</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-5</title>
    <url>/2023/10/05/2023-10-05/</url>
    <content><![CDATA[<p>一眨眼断更了这么久啊<br>时间一下子溜走了，留下了一些照片聊作慰藉<br>今天帮yt同志看了看实验报告，说实话花了不少时间，不过不负责任的基物实验老师起码得背一半的锅，估计是急着放假了，压根没检查，方法完全不对更别说算不确定度了，急了<br>sh同学学习十分认真，表扬一下<br>今天还干了些啥啊啊啊啊啊，中午把全新的天猫校园被子送给了宿管阿姨，以及忍不住诱惑在逐渐转冷的北京秋日，毅然决然地吃雪糕。晚上，做一个只属于自己的AI的想法又在我脑海中复辟了，于是屈从于自己各式各样的“突发奇想”，开始导出自己的微信聊天记录，并拆分成训练集测试集什么的，简直是闲得慌<br>琴到了第三天了，今天练了两个小时，先练了流星和Time Travel，之后练了A little Star 的击板部分，害，不知道还要练多久<br>晚上把量子与统计写了1&#x2F;2道题<br>一天就这样过去了</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-7</title>
    <url>/2023/10/07/2023-10-07/</url>
    <content><![CDATA[<p>感觉这一整个学期，都要生活在视唱练耳的阴影之下了<br>呃呃呃啊啊啊呃呃呃啊啊啊<br>要多练啊啊啊啊啊啊啊啊啊<br>量子与统计也得好好看，明天又要上课了，往前坐一点<br>材料分析与表征你甜蜜的到底在讲什么？？？<br>叉乘（Cross Product）：</p>
<ul>
<li>叉乘通常与三维空间中的向量相关</li>
<li>而外积在三维与叉乘类似，其余则不然<br>总而言之言而总之，又活过了视唱练耳<br>下一步，是A自然小调音阶</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-9</title>
    <url>/2023/10/09/2023-10-09/</url>
    <content><![CDATA[<p>呃呃呃啊啊啊，大清早焦虑拉满了呃呃呃<br>论文进展好久没有往前了，什么时候才能完成换组大业啊啊啊啊<br>我超啊啊啊啊啊<br>急了</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-8</title>
    <url>/2023/10/08/2023-10-08/</url>
    <content><![CDATA[<p>决定建立量子力学手写笔记，怒<br>说起来是第二次学这个量子力学了<br>上次在csm那还拿了4.0呢，现在还是啥都不会<br>搞了一下午……略微会一点了<br>顺便玩了一会愤怒的小鸟</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-12</title>
    <url>/2023/10/12/2023-10-12/</url>
    <content><![CDATA[<p>今天，有超多事情的一天。<br>下午五点半，图书馆的窗外还在下雨。<br>挑战杯的方向扑朔迷离，论文的完成遥遥无期<br>大模型一训就是好几个小时<br>啊~~~~~~~~~<br>悲</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/13/2023-10-13/</url>
    <content><![CDATA[<p>一把文明坐一天<br>饿饿饿啊啊啊<br>玩文明的前置需求也许是身体素质……<br>晚上和yt同志调用乐队经费吃了必胜客<br>KTV唱歌，启动！</p>
]]></content>
  </entry>
  <entry>
    <title>日记10-14</title>
    <url>/2023/10/14/2023-10-14/</url>
    <content><![CDATA[<p>周六了，还有好多作业没写<br>挑战杯训练Lamma基础模型，顺便把folder填一下<br>论文把四种方程的应用场景写一下<br>量子与统计一定要抽时间看<br>还有视唱练耳，切记切记切记切记！！！<br>其他的事情，交给时间去解决吧</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-15</title>
    <url>/2023/10/15/2023-10-15/</url>
    <content><![CDATA[<p>镜面边缘</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-16</title>
    <url>/2023/10/16/2023-10-16/</url>
    <content><![CDATA[<p>每次写日记都要打文档properties真的好烦……</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/22/2023-10-22/</url>
    <content><![CDATA[<p>一切还是应该回到最初的轨道<br>孰轻孰重、孰是孰非<br>题中应有之义</p>
]]></content>
  </entry>
  <entry>
    <title>日记10-25</title>
    <url>/2023/10/25/2023-10-25/</url>
    <content><![CDATA[<p>上午上了量统，奇怪的是，没有任何印象<br>心情还是很差，最近发生了很多事情<br>很难说自己是成长了还是依然很天真<br>但所幸，遇到的都是善良温柔的人<br>工图突然变得很难，网球感觉还是没什么感觉<br>一个人在图书馆学习<br>不知道期待着什么<br>也不知道担忧着什么<br>就这样单曲循环着《曾经我也想过一了百了》<br>不累，却也没什么精神<br>不喜欢这样的自己</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-19</title>
    <url>/2023/10/19/2023-10-19/</url>
    <content><![CDATA[<p>共同的焦虑会诞生情感<br>共同的快乐也会<br>晚上好冷</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-20</title>
    <url>/2023/10/20/2023-10-20/</url>
    <content><![CDATA[<p>量统课后问了老师问题<br>好欸！<br>中午在麦叔的铺子里进行了加餐<br>现在GPT3.5api GPT4.0 google colab pro均在手<br>启动！</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-27</title>
    <url>/2023/10/27/2023-10-27/</url>
    <content><![CDATA[<p>新的一天<br>一天又一天的事务会冲刷悲伤与喜悦<br>正如海浪抹平万丈沟壑和孩子堆的城堡</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-28</title>
    <url>/2023/10/28/2023-10-28/</url>
    <content><![CDATA[<p>昨天干了啥（10.29记）<br>感觉把显示器的事情搞定了<br>早上干了什么完全想不清了<br>下午爆肝了7个小时搞定了二维圆柱绕流的事情<br>然后就晚上了<br>然后担心一个人是不是死掉了担心到2点<br>然后荒诞的一天就结束了</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-26</title>
    <url>/2023/10/26/2023-10-26/</url>
    <content><![CDATA[<p>等天晴<br>一切似乎略微有了转机<br>发现答案就在自己身边啊<br>答案就在自己身边</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-29</title>
    <url>/2023/10/29/2023-10-29/</url>
    <content><![CDATA[<p>啊啊啊啊啊啊啊啊啊啊<br>视唱练耳一定要练啊混蛋<br>昨天那个以为死掉的人<br>纯粹只是睡过了<br>md<br>忙忙忙忙忙<br>今天赶紧把论文的几个图给做了<br>急急急急急急急<br>明天组会警告</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-30</title>
    <url>/2023/10/30/2023-10-30/</url>
    <content><![CDATA[<p>马上过生日啦！<br>立牌到了，但是没有阿梓喵，气死我了<br>可恶</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记10-31</title>
    <url>/2023/10/31/2023-10-31/</url>
    <content><![CDATA[<p>时间紧，任务重<br>时不我待！</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记11-1</title>
    <url>/2023/11/01/2023-11-01/</url>
    <content><![CDATA[<p>生日……吗？<br>显示屏装好了，效果还不错</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记11-4</title>
    <url>/2023/11/04/2023-11-04/</url>
    <content><![CDATA[<p>一眨眼居然三天没写日记了<br>啊啊啊啊啊<br>一会补一下</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记11-5</title>
    <url>/2023/11/05/2023-11-05/</url>
    <content><![CDATA[<p>紧急计划<br>晚7-8 机器学习<br>晚8-9量子与统计<br>晚9-10乐理<br>晚10-10：30音程关系<br>10：30-11：10视唱练耳<br>明天早8：00-9：50材料分析与表征<br>13：00-14：20视唱练耳<br>14：20-15：00乐理<br>妈的对某个人的忍耐已经快到限度了</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>日记11-7</title>
    <url>/2023/11/07/2023-11-07/</url>
    <content><![CDATA[<p>对于自己想要什么还是很不清楚啊<br>明明是个很随便的人，却有时候显得异常顽固呢<br>难评<br>14:00先把机器学习整了，起码复习到15:30<br>upd：学到了17:00，基本上把今天课上的东西消化了<br>下一步，复习一下numpy，顺便复习一下量子与统计<br>呃啊，急急急急急。<br>现在是17:20，吃完回来快六点了，要不直接去得了<br>呃呃呃啊啊啊</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>Cheat Engine 告别风灵月影</title>
    <url>/2023/06/10/cheatengine/</url>
    <content><![CDATA[<p>最近玩Brotato，突然发现风灵月影居然没有相关的修改器。阴差阳错之下，学会了Cheat Engine的操作。（有一说一tutorial做的真挺好，就像黑客风格的游戏一样）<br>一般操作流程就是读取内存，确定参数对应的内存保存位置，修改之即可。这下再也不用担心网络上乱七八糟修改器的病毒问题，或者功能有所欠缺了。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/07/06/deep-learning-with-pytorch/</url>
    <content><![CDATA[<h1 id="Deep-Learning-with-Pytorch"><a href="#Deep-Learning-with-Pytorch" class="headerlink" title="Deep Learning with Pytorch"></a>Deep Learning with Pytorch</h1><h2 id="Chapter1"><a href="#Chapter1" class="headerlink" title="Chapter1"></a>Chapter1</h2><h2 id="Chapter2-P1"><a href="#Chapter2-P1" class="headerlink" title="Chapter2 P1"></a>Chapter2 P1</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#引入pytorch包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">torch.version.__version__</span><br><span class="line"><span class="comment">#定义全一张量</span></span><br><span class="line">a=torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查询显卡状态</span></span><br><span class="line">torch.cuda.is_available()</span><br><span class="line"><span class="comment">#将张量转移至显卡进行计算,利用tensor的.to方法</span></span><br><span class="line">a=a.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="comment">#如果在不同设备进行计算，则会报错</span></span><br></pre></td></tr></table></figure>

<h2 id="Chapter2-P2"><a href="#Chapter2-P2" class="headerlink" title="Chapter2 P2"></a>Chapter2 P2</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在out的第2个（参数1代表第2个）维度选取最大值，函数返回值有两个，分别为最大值和最大值位置，不需要的话以_占位即可</span></span><br><span class="line">_, index = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Chapter3-P1"><a href="#Chapter3-P1" class="headerlink" title="Chapter3 P1"></a>Chapter3 P1</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#python自带的列表</span></span><br><span class="line">a = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line"><span class="comment">#torch中的张量,out为tensor([1.,1.,1.])</span></span><br><span class="line">a = torch.ones(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#按下标访问，例如a[2],则返回tensor(1.),仍然是张量,取float后值为1.0，也即</span></span><br><span class="line"><span class="built_in">float</span>(a[<span class="number">1</span>])=<span class="number">1.0</span></span><br><span class="line"><span class="comment">#tensor也可按下标修改,如</span></span><br><span class="line">a[<span class="number">2</span>]=<span class="number">2.0</span></span><br><span class="line"><span class="comment">#全零tensor,如torch.zeros(6)</span></span><br><span class="line"><span class="comment">#直接定义tensor</span></span><br><span class="line">points=torch.tensor([<span class="number">4.</span>,<span class="number">1.</span>,<span class="number">5.</span>,<span class="number">3.</span>,<span class="number">2.</span>,<span class="number">1.</span>])</span><br><span class="line"><span class="comment">#有关高维张量</span></span><br><span class="line">point = torch.tensor([<span class="number">4.0</span>,<span class="number">1.0</span>],[<span class="number">5</span>,<span class="number">0</span>,<span class="number">3.0</span>].[<span class="number">2.0</span>,<span class="number">1.0</span>])</span><br><span class="line"><span class="comment">#返回形状</span></span><br><span class="line">points.shape</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>DisBand Space</title>
    <url>/2023/09/25/disband-xiang-guan-xin-xi/</url>
    <content><![CDATA[<p>DisBand网站上线 <a href="http://www.disband.cn/">www.disband.cn</a></p>
]]></content>
      <categories>
        <category>Parallel Life</category>
      </categories>
      <tags>
        <tag>Parallel</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/25/cosmol-drug-release/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/22/langchain-xue-xi-bi-ji/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Markdown实用语法</title>
    <url>/2023/06/13/markdown-yu-fa/</url>
    <content><![CDATA[<ul>
<li>最常用的是无序列表，对于强迫症来说很有用。语法是” - “(减号加空格)，加在每行开头即可。</li>
<li>还有一个比较有趣的是删除线符号，在想要删除的地方前后加双波浪线（~~ xxx ~~ ）即可，如<del>我想删除这些字</del> 当然，加了空格就可以不受影响。</li>
<li>有几个希腊字母和特殊符号还挺容易忘的，mark一下。<ul>
<li>\rho $\rho$ ，\pm $\pm$</li>
<li>\geq $\geq$  \leq $\leq$</li>
<li>\qquad \quad \ ; , !分别对应6种不同的字符间距</li>
</ul>
</li>
<li>字符顶上的特殊符号$\tilde{A}$  \tilde<ul>
<li>补充一个上面加点$\dot{x}$ dot<blockquote>
<p>如何引用呢？使用&gt;即可</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>学到量子力学以后，需要打狄拉克符号$\langle,\rangle$ langle 和rangle </p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title>Reference</title>
    <url>/2023/09/21/reference-for-ml-dl/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>相关内容参考</category>
      </categories>
      <tags>
        <tag>Reference</tag>
      </tags>
  </entry>
  <entry>
    <title>VS2012只能对 Type.IsGenericTypeDefinition 为 True</title>
    <url>/2023/09/21/vs2012-zhi-neng-dui-type.isgenerictypedefinition-wei-true-wen-ti/</url>
    <content><![CDATA[<p>这是个VS2012解决方案的问题。</p>
<span id="more"></span>
<p>System.Collections.Generic.RandomizedStringEqualityComparer 不是 GenericTypeDefinition。只能对 Type.IsGenericTypeDefinition 为 True。</p>
<p>解决方法：到<a href="http://www.microsoft.com/zh-cn/download/confirmation.aspx?id=36020%E4%B8%8A%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7%E5%8C%85%EF%BC%8C%E5%AE%89%E8%A3%85%E5%8D%B3%E5%8F%AF%E3%80%82">http://www.microsoft.com/zh-cn/download/confirmation.aspx?id=36020上下载工具包，安装即可。</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>windows系统时间同步</title>
    <url>/2023/06/17/windows-xi-tong-shi-jian-tong-bu/</url>
    <content><![CDATA[<p>实在是太可恶了……</p>
<span id="more"></span>
<p>从三月份开始电脑的时间总是和时间服务器同步不上，导致梯子有很多节点不能连，火狐干脆网站都不让上。<br>在设置里试了好几次同步时间服务器都无果，于是乎到处搜该咋办。跟着热心网友的经验贴这边修修那边改改发现都不行（期末周特有的没事找事）。<br>最后又是一位老哥一语惊醒梦中人。<br>你这是校园网……连手机热点就行了。<br>于是连上手机热点。<br>解决。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title>和声学学习</title>
    <url>/2023/09/12/he-sheng/</url>
    <content><![CDATA[<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="和弦（和弦音）"><a href="#和弦（和弦音）" class="headerlink" title="和弦（和弦音）"></a>和弦（和弦音）</h2><h4 id="和声、和音、和弦"><a href="#和声、和音、和弦" class="headerlink" title="和声、和音、和弦"></a>和声、和音、和弦</h4><ul>
<li><p>和声学：研究和音的结构以及它们如何连接</p>
</li>
<li><p>和弦中各音的名称</p>
<ul>
<li>和弦按三度关系排列后，最低的音为根音，向上依次为三、五、七、九……名称不随排列位置改变而改变。</li>
</ul>
</li>
<li><p>和弦音</p>
</li>
<li><p>和弦外音</p>
<ul>
<li>延留音，出现在强拍或次强拍，使得音高与自己接近的和弦音延迟出现。除了延留音，其他的和弦外音都出现在弱拍上。</li>
<li>经过音，和弦音上下级之间的过渡。</li>
<li>辅助音，和弦音与其重复音之间的连接</li>
<li>先现音，后一和弦音在弱拍上提前出现</li>
</ul>
</li>
<li><p>非三度结构的和音</p>
</li>
</ul>
<h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><h3 id="大三和弦与小三和弦、四部和声"><a href="#大三和弦与小三和弦、四部和声" class="headerlink" title="大三和弦与小三和弦、四部和声"></a>大三和弦与小三和弦、四部和声</h3><ul>
<li><p>定义</p>
<ul>
<li>大三和弦，大三度+小三度（基于根音的大三度+纯五度）</li>
<li>小三和弦，小三度+大三度（基于根音的小三度+纯五度）</li>
<li>增减（两个大三度、两个小三度）</li>
</ul>
</li>
<li><p>四部和声</p>
<ul>
<li>四声部，高音、中音、次中音、低音</li>
</ul>
</li>
<li><p>三和弦的旋律位置</p>
</li>
</ul>
<h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h3 id="正三和弦的功能体系"><a href="#正三和弦的功能体系" class="headerlink" title="正三和弦的功能体系"></a>正三和弦的功能体系</h3><ul>
<li>主、副、属</li>
<li>建立在音阶的Ⅰ级，也就是主音上的三和弦叫做主三和弦，在大调中用T表示，小调中用t表示</li>
<li>进行、进行的公式<ul>
<li>和弦的连续构成了和声进行，最简单的和声进行逻辑基础就是</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Parallel Life</category>
      </categories>
      <tags>
        <tag>Parallel</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Python的科学与数值计算</title>
    <url>/2023/09/19/ji-yu-python-de-ke-xue-yu-shu-zhi-ji-suan/</url>
    <content><![CDATA[<h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><h2 id="ndarray"><a href="#ndarray" class="headerlink" title="ndarray"></a>ndarray</h2><ul>
<li>基本信息<ul>
<li>numpy的基础数据结构（下简称ndarray为array）</li>
<li>向量、矩阵、张量等均用array表示</li>
<li>数组下标从0开始</li>
</ul>
</li>
<li>ndarray与list的区别<ul>
<li>array元素为相同数据类型</li>
<li>array固定长度</li>
<li>array易进行维度、形状变换</li>
<li>array支持完整的向量&#x2F;张量操作和线性代数函数</li>
</ul>
</li>
<li>adarray类<ul>
<li>参数<ul>
<li>shape 形状</li>
<li>dtype 类型</li>
<li>buffer 可以指定数据块而无需复制</li>
<li>offset 指定从数据块的第几个位置开始复制</li>
<li>strides 各维度步进字节数</li>
<li>…</li>
</ul>
</li>
<li>属性<ul>
<li>shape 包含数组每个维度的元素数量的元组，如 (3, 5)</li>
<li>size 元素总数</li>
<li>ndim 维度数量，1、2、3⋯</li>
<li>nbytes 存储数据的总字节数</li>
<li>dtype 数据类型</li>
<li>itemsize 每个元素的字节数，如 float32 是 4 个字节</li>
<li>strides: 跨步&#x2F;步幅，索引访问的间隔字节数</li>
<li>data 数组的真正数据</li>
</ul>
</li>
</ul>
</li>
<li>基于array的运算<ul>
<li>基于元素的算数运算<ul>
<li>numpy数组的基本算数运算是作用到每个元素上的</li>
<li>广播机制<ul>
<li>a、b 维数相等，某一维（轴）上的数量不等，其中一个数组在该维度上数量为 1，则将该数组在该轴上的数量扩充（复制），使其与另外的数组相同（广播）</li>
<li>a、b 维数不等，将维数少的数组从左扩充长度为 1 的新维度，直到二者维度相等。然后再执行上面的广播操作</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>聚合函数</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>不遇</title>
    <url>/2022/12/24/bu-yu/</url>
    <content><![CDATA[<p>开头删了又写，写了又删，总找不到一种合适的语气，三年前如此，现在亦如此，倒比不上十年前的你我了。</p>
<span id="more"></span>

<p>转念一想，你可能永远也不知道这里有一篇为你而留的文字，那我便放心大胆的写啦。</p>
<p>说来也奇怪，这几年的回忆，反而模糊不清，有些迫切想解释的，遗憾未完成的，都如云烟般散去。其实我只想让你知道，在三年未见的时光里，我只在等你。</p>
<p>风言风语对当时的你我也许还是过于锋利了，只记得那年夏天没吃到你的生日蛋糕，之后，就只能在别人的朋友圈里看到你了。</p>
<p>回想初三那一年，一见封谶的字条墨迹未干，久别重逢在我头上敲的那一下依旧有点疼:( 深秋我们在为数不多的夜里走着，我说，我们看似走的更近了，其实在渐行渐远。你说，我们再也回不去小时候了。我想牵牵你的手，故意说“我们连牵手都没牵过，以后也算不上分手”，没想到，却一句话说到了最后。</p>
<p>那段时间，是我辜负了你纯真而又热烈的情感，回过头来，我真想将那些数学作业撕个粉碎，从五楼扬下去。当时的你在我的梦里总是带着泪光，而我，也只能在梦里拥抱你。</p>
<p>大学里聊起谈恋爱的话题，大家都怨恨老天欠我们一个青梅竹马。只有我默默不说话，我知道，老天让我在一年级的时候就遇到了你。近乎自然而然，又仿佛一种羁绊，六年的小学生活，至少你就是我眼里最耀眼最可爱的那个女生。当时的我们真像书中那些温暖的文字所说 ，青梅竹马，两小无猜。我还记得我们一起四手联弹，我总是弹不好，被我爸嘲笑。记得在东恒盛大厅的沙发上看电视，你还摔了一跤。记得在机房里，你让我为你系上纽扣，我一下子脸红让你自己系。记得那些似乎无穷无尽的周六周日，我们一起学习玩耍的日子。记得你抱着我家狗狗，记得和你玩双人小游戏。快乐多么简单啊，数学试卷发下来比我高一分你就会高兴好久（而我会难过好久）。学语文时互批，我们总是想找茬让对方低几分。有太多回忆我好想一直一直写下去，因为这些都是我所无比珍视的，我想把它们带进我的坟墓。</p>
<p>不知道现在的你还好吗，故人，挚友，陌路。也许现在的你我，就像书中所说，“最熟悉的陌生人”。中考结束，约好一起去看老师，我却终未成行。高中三年，从见面时的尴尬到最后形同陌路。</p>
<p>也许看到这篇文字的你，会想起一些美好的回忆罢。如此便足矣。</p>
]]></content>
      <categories>
        <category>故人</category>
      </categories>
      <tags>
        <tag>故人</tag>
        <tag>挚友</tag>
        <tag>陌路</tag>
      </tags>
  </entry>
  <entry>
    <title>春归</title>
    <url>/2022/12/24/xiao-shi/</url>
    <content><![CDATA[<p>春归</p>
<p>春归何日？秋雨霏霏。繁霜重降，百卉具腓。我心忧矣，不知所届。春之去矣，伊谁之非？</p>
<p>昔女往矣，期于桃木。灼灼其叶，殁于恶土。我徂西山，瞻彼荒原。憯憯日瘁，三其寒暑。</p>
<p>春归何日？冬阳厉厉。冻雪其雱，折木以炀。我心忧矣，不知所望。春之去矣，亦民之伤。</p>
<p>昔女往矣，期于菀柳。依依其枝，寒天杌之。执柳为信，待其飞鸿。跂矣断矣，疢如疾首。</p>
<p>初春何如，惊蛰彭彭，草木苍苍；盛春何如，芳华灿灿，河水汤汤。其忆孔嘉，其逢如之何？</p>
]]></content>
      <categories>
        <category>诗云</category>
      </categories>
      <tags>
        <tag>诗云</tag>
        <tag>残章</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/2023/09/19/ji-qi-xue-xi/</url>
    <content><![CDATA[<h2 id="9-19-第一讲"><a href="#9-19-第一讲" class="headerlink" title="9-19 第一讲"></a>9-19 第一讲</h2><span id="more"></span>
<p>$(\psi(r,\theta,\phi)&#x3D;\frac{1}{\sqrt{\pi{a_0}^3}}e^{-\frac{r}{a_0}})$</p>
<h3 id="Big-Data-amp-Simple-Model"><a href="#Big-Data-amp-Simple-Model" class="headerlink" title="Big Data &amp; Simple Model"></a>Big Data &amp; Simple Model</h3><p>why machine learning feasible 可行的</p>
<h4 id="Example-pick-red-and-green-marbles-from-a-bin"><a href="#Example-pick-red-and-green-marbles-from-a-bin" class="headerlink" title="Example : pick red and green marbles from a bin"></a>Example : pick red and green marbles from a bin</h4><ul>
<li>$\mathbb{P}[red marbles] &#x3D; \mu \qquad \mathbb{P}[green marbles] &#x3D; 1-\mu$</li>
<li>Hoeffding’s inequality：$P[|\nu-\mu|&gt;\epsilon]\leq2e^{-2\epsilon^2N}$<ul>
<li>其中$\nu$  是取样中红色球的比例  </li>
<li>可见实验结果误差大于$\epsilon$的概率并不取决于期望，而取决于实验的次数$N$</li>
<li>该例子具有一定的特殊性，因为其为Brenoulli实验，且所考虑的$\nu$和$\mu$均在0到1之间</li>
<li>关键：big data gives an accurate learning result</li>
</ul>
</li>
<li>广义Hoeffding’s inequality：$$\mathbb{P}\left(\left|S_n-\mathbb{E}\left[S_n\right]\right| \geq t\right) \leq 2 \exp \left(-\frac{2 t^2}{\sum_{i&#x3D;1}^n\left(b_i-a_i\right)^2}\right)<br>\tag{1}<br>$$<ul>
<li>$b_i ,a_i$分别为变量$X_i$的上限和下限</li>
<li>注意到$S_n$是变量之和，如果要与伯努利情况下的Koeffding比较，则可以考虑指数上下同除$n^2$</li>
<li>在广义的情况下考虑本例，$b_i-a_i&#x3D;1$，代入即得本例情况</li>
</ul>
</li>
</ul>
<h4 id="Example-拓展"><a href="#Example-拓展" class="headerlink" title="Example 拓展"></a>Example 拓展</h4><ul>
<li>将未知的$\mu$替换为$f(x)$，我们给出假设$h(x)$，从取球的角度来看，每一次取球，就相当于在定义域$X$中取一个$x$，取出红球意味着，$h(x)\ne f(x)$，而取出绿球则意味着$h(x) &#x3D; f(x)$ </li>
<li>在上述前提下，定义$E_{in}(h)$作为抽样错误率，也即“in sample error”，同时定义$E_{out}(h)$作为总错误率，也即对所有利用假设进行判断后$h(x) \ne f(x)$ 的比率。其实也就是之前所述的$\nu$和$\mu$，我们想要知道我们的假设$h$效果怎么样，但是没必要把所有$x$全带入。</li>
<li>$h$的参数空间记为$H$ ，其中最优的假设是$g$，经过训练后，$g$会尽可能接近$f$ </li>
<li>$$</li>
</ul>
<p>\mathbb{P}\left[\left|E_{\text {in }}(g)-E_{\text {out }}(g)\right|&gt;\epsilon\right] \leq \sum_{m&#x3D;1}^M \mathbb{P}\left[\left|E_{\text {in }}\left(h_m\right)-E_{\text {out }}\left(h_m\right)\right|&gt;\epsilon\right]</p>
<p>$$</p>
<ul>
<li>$$</li>
</ul>
<p>\mathbb{P}\left[\left|E_{\text {in }}(g)-E_{\text {out }}(g)\right|&gt;\epsilon\right] \leq 2 M e^{-2 \epsilon^2 N}<br>\tag{2}<br>$$</p>
<ul>
<li>这条式子的意思是，给出所有$M$个假设中，最优的一个，训练集和实际集的误差上限（这本身不能让$g$接近$f$）这很好理解，因为右侧包含左侧。<blockquote>
<p>Low complexity model and big data can give us a good generalization in machine learning</p>
</blockquote>
</li>
</ul>
<h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><blockquote>
<p>Use labelled dataset to train algorithms</p>
</blockquote>
<h4 id="Regression-Ⅰ"><a href="#Regression-Ⅰ" class="headerlink" title="Regression Ⅰ"></a>Regression Ⅰ</h4><ul>
<li>Linear Regression<ul>
<li>$y_{i}&#x3D;f_{\beta_{0},\beta_{1}}(x_{i})+\epsilon_{i}&#x3D;\beta_{0}+x_{i}\beta_{1}+\epsilon_{i}\quad$其中$\epsilon$即为残差<ul>
<li>$f_{\beta_{0},\beta_{1}}$是线性回归的函数，训练目标是最小化$\epsilon$的平方和</li>
</ul>
</li>
<li>Cost Function<ul>
<li>$C\left(\beta_0,\beta_1\right)&#x3D;\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0,\beta_1}\left(x_i\right)\right)^2$</li>
</ul>
</li>
<li>Assumptions<ul>
<li>Weak exogeneity 假设残差的期望为0</li>
<li>Linearity 假设WX+b&#x3D;y</li>
<li>Homoscedasticity 假设因变量方差不随自变量改变<ul>
<li>假设我们通过房子的面积等预测房价，我们可以假设不管面积是大是小，价格的方差不会变。而事实上也存在反例，例如我们一般认为，小房子的价格波动更大</li>
</ul>
</li>
</ul>
</li>
<li>Ordinary least squares<ul>
<li>Cost Function $(y_{i}-f_{\beta_{0},\beta_{1}}(x_{i}))^2$ </li>
<li>对两个参数分别求偏导及二阶偏导，最小化损失函数</li>
<li>Coefficient of determination $r^2$<ul>
<li>$$r^2&#x3D;1-\frac{\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0, \beta_1}\left(x_i\right)\right)^2}{\sum_{i&#x3D;1}^m\left(y_i-\bar{y}\right)^2} \times 100 % \tag{3}$$</li>
<li>也即比较线性回归模型残差平方和和原有数据残差平方和（相对于$\bar{y}$）故$r^2$越大越优</li>
<li>定性看一下，直接拿均值肯定很烂，然后$r^2&#x3D;1$的话，说明完全拟合上了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multiple Linear Regression<ul>
<li>额外假设，不存在多重共线性</li>
<li>同样思路，计算一阶导数并令之为0</li>
<li>$$\begin{gathered}C(\boldsymbol{\beta})&#x3D;(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})&#x3D;\boldsymbol{y}^T \boldsymbol{y}+\boldsymbol{\beta}^T X^T X \boldsymbol{\beta}-2 \boldsymbol{\beta}^T X^T \boldsymbol{y} \\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}&#x3D;2 X^T X \boldsymbol{\beta}-2 X^T \boldsymbol{y}\end{gathered}$$</li>
<li>据此给出 $\widehat{\boldsymbol{\beta}}$的表达式</li>
<li>$$\widehat{\boldsymbol{\beta}}&#x3D;\left(X^T X\right)^{-1} X^T \boldsymbol{y}$$</li>
<li>计算二阶导数</li>
<li>$$\mathcal{H}&#x3D;\frac{\partial^2 C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^2}&#x3D;2 X^T X$$</li>
<li>证明海森矩阵正定（$a$是任意给定非零向量）</li>
<li>$$\boldsymbol{a}^T X^T X \boldsymbol{a}&#x3D;(X \boldsymbol{a})^T X \boldsymbol{a}&#x3D;|X \boldsymbol{a}|_2^2 \geq 0$$</li>
<li>定义hat matrix</li>
<li>$$\widehat{\boldsymbol{y}}&#x3D;X \widehat{\boldsymbol{\beta}}&#x3D;X\left(X^T X\right)^{-1} X^T \boldsymbol{y}&#x3D;X\left(X^T X\right)^{-\mathbf{1}} X^T \boldsymbol{y}&#x3D;H \boldsymbol{y}$$</li>
<li>Properties of hat matrix<ul>
<li>$H$ 、$I-H$  均是正交投影矩阵</li>
<li>Idempotent $H&#x3D;H^2$、$(I-H)&#x3D;(I-H)^2$</li>
<li>残差由$\epsilon&#x3D;y-\hat{y}&#x3D;y-Hy&#x3D;(I-H)y$给出<ul>
<li>当然，残差的平方和$\epsilon^T \epsilon&#x3D;y^T(I-H)y$（依据幂等消掉了一个）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Maximum Likelihood Estimation<ul>
<li>Assumptions<ul>
<li>随机取样残差符合均值为0的高斯分布，在残差为0时，概率取最大值<ul>
<li>在假设残差符合高斯分布的前提下，假设其方差为$\sigma^2$</li>
</ul>
</li>
<li>$$<br>p\left(\left(\boldsymbol{x}_{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}\right)&#x3D;\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}<br>$$</li>
<li>复习的时候发现这里非常容易混淆，再次强调$p((x_i,y_i)|\beta)$其实指的是$x_i$在参数为$\beta$时给出的预测和真实的标签$y_i$的差距服从的分布</li>
<li>依据常规高斯分布公式分析，其实就是下式（对于残差，均值$\mu$为0）</li>
<li>$$p;(\left(\epsilon_i) \mid \boldsymbol{\beta}\right)&#x3D;\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(\epsilon_i-0)^2}{2 \sigma^2}}$$</li>
<li>$(x_i,y_i)$独立同分布</li>
</ul>
</li>
<li>最大似然估计最大化在所有取样点处的概率之和，这样约等于最小化残差</li>
<li>最大化以下式子：$$p\left(D \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;p\left(\left(\boldsymbol{x}<em>{\mathbf{1}}, y_1\right), \ldots\left(\boldsymbol{x}</em>{\boldsymbol{m}}, y_m\right) \mid \boldsymbol{\beta}, \sigma^2\right)$$</li>
<li>Calculate the expression of likelihood</li>
<li>Due to the assumption of independency$$p\left(\left(\boldsymbol{x}<em>1, y_1\right), \ldots\left(\boldsymbol{x}</em>{\boldsymbol{m}}, y_m\right) \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;\prod_{i&#x3D;1}^m p\left(\left(\boldsymbol{x}_{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}, \sigma^2\right)$$</li>
<li>Due to the assumption of Gaussian Distribution$$\prod_{i&#x3D;1}^m p\left(\left(\boldsymbol{x}<em>{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;\prod</em>{i&#x3D;1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T \boldsymbol{x}_i\right)^2}{2 \sigma^2}}$$</li>
<li>Due to the assumption of homoscedasticity$$\prod_{i&#x3D;1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}&#x3D;\left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)^m e^{-\frac{\sum_{i&#x3D;1}^m\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}$$</li>
<li>Calculate the Log-likelihood$$\mathcal{L}\left(\boldsymbol{\beta}, \sigma^2\right)&#x3D;-\frac{m}{2} \ln \left(2 \pi \sigma^2\right)-\frac{1}{2 \sigma^2}(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})$$</li>
<li>到此步为止，为了最大化似然，需要最小化$\frac{m}{2} \ln \left(2 \pi \sigma^2\right)+\frac{1}{2 \sigma^2}(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})$ 也即最小化$(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})\quad$这与多元线性回归类似（最小化残差平方和）</li>
<li>分别对$\beta$和$\sigma^2$求偏导，注意得到的$\sigma$是残差的标准差，又由于同分布假设，故假设最后的结果是$y_{predict}$则95%置信区间为$y_{predict}\pm 1.96\sigma$</li>
</ul>
</li>
</ul>
<h2 id="Gradient-Descent-Ⅰ"><a href="#Gradient-Descent-Ⅰ" class="headerlink" title="Gradient Descent Ⅰ"></a>Gradient Descent Ⅰ</h2><h3 id="Illustration-1D"><a href="#Illustration-1D" class="headerlink" title="Illustration(1D)"></a>Illustration(1D)</h3><p>对于1-D线性回归模型，我们可以发现，损失函数是一个下凸函数，所以我们可以先随机选定一组参数$(\beta_0,\beta_1)$ 相当于在碗的壁上放了一个乒乓球，之后按以下策略，迭代参数$(\beta_0,\beta_1)$<br>$$<br>\begin{gathered}<br>\beta_1^{(i+1)}&#x3D;\beta_1^{(i)}-\alpha \frac{\partial C\left(\beta_0^{(i)}, \beta_1^{(i)}\right)}{\partial \beta_1}&#x3D;\beta_1^{(i)}+2 \alpha \sum_{i&#x3D;1}^m\left(y_i-\beta_1^{(i)} x_i-\beta_0^{(i)}\right) x_i \<br>\beta_0^{(i+1)}&#x3D;\beta_0^{(i)}-\alpha \frac{\partial C\left(\beta_0^{(i)}, \beta_1^{(i)}\right)}{\partial \beta_0}&#x3D;\beta_0^{(i)}+2 \alpha \sum_{i&#x3D;1}^m\left(y_i-\beta_1^{(i)} x_i-\beta_0^{(i)}\right)<br>\end{gathered}<br>$$</p>
<ul>
<li>注意到有 $C\left(\beta_0,\beta_1\right)&#x3D;\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0,\beta_1}\left(x_i\right)\right)^2$</li>
<li>注意区分在优化问题中的参数$x$以及具体的数据点$x$两者容易搞混</li>
</ul>
<blockquote>
<p>第一讲到此结束</p>
</blockquote>
<hr>
<h2 id="Gradient-Descent-Ⅱ"><a href="#Gradient-Descent-Ⅱ" class="headerlink" title="Gradient Descent Ⅱ"></a>Gradient Descent Ⅱ</h2><blockquote>
<p>For General functions, the gradient descent algorithm is often stuck at a local minimum</p>
</blockquote>
<ul>
<li>数值方法求解梯度<ul>
<li>$\partial f&#x2F; \partial x &#x3D; (f(x+\epsilon)-f(x))&#x2F;\epsilon$</li>
</ul>
</li>
<li>Line search<ul>
<li>Exact Line Search<ul>
<li>$a_i^{*} &#x3D; argmin_{a_i&gt;&#x3D;0}f(x_i+a_i\Delta x_i)$</li>
<li>在一定范围内搜索$\alpha$让$f$最小</li>
<li>问题在于操作繁琐计算需求大，主要是为了找一个让函数沿着当前方向下降最大的步长</li>
</ul>
</li>
<li>Backtracking Search<ul>
<li>核心思想是“你至少下降这么多我才可以接受”，核心目标是确定一个还算不错的步长</li>
<li>设置一个值$\gamma$衡量所期望的函数下降的程度，同时设置一个值$\rho$ 来逐步缩小步长直到符合要求</li>
<li>目标是比较$f(x+\alpha \Delta x)$与$f(x)+\alpha \gamma \nabla f(x)^T \Delta x$，如果当前选择的步长未能使函数下降至小于期望，则将步长$\alpha$缩小为$\rho \alpha$ </li>
<li>$Wolfe ; 1^{st} ; and ; Wolfe ; 2^{nd} ; condition$<ul>
<li>第一条件，也就是在回溯线搜索中用到的条件，限制步长不要太长，以至于错过local minimum</li>
<li>第二条件，约束 $\frac{\phi^{\prime}(a)}{\phi^{\prime}(0)}&#x3D;\frac{\nabla f(x+a\Delta x)^{T}\Delta x}{\nabla f(x)^{T}\Delta x}\leq \eta$也即要求新的位置的斜率必须相较最初变得一定程度上平缓，这使得了步长不会过短<ul>
<li>注意到如果原地不动$\eta&#x3D;1$，如果移动至局部最小值，$\eta &#x3D; 0$所以一般取$\eta$在0到1之间</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Newton’s method<ul>
<li>牛顿法本用来通过数值方法找到函数根的近似解，但也可以稍加变化使之可以应用在梯度下降法中，即将求根的对象从$f$变为$f^{‘}$  </li>
<li>公式可写作$x_{i+1}&#x3D;x_{i}-\frac{f’(x_{i})}{f’’(x_{i})}$</li>
<li>在一维情况下阐述工作原理<ul>
<li>首先我们在某点计算目标函数的梯度和Hessian，之后用这些信息形成二阶泰勒展开，得到二次曲线</li>
<li>之后我们找到二次曲线的最小值位置，更新$x_{k+1}$</li>
<li>参考公式如下$f(x)\approx f(x_{i})+\boldsymbol{g}^{T}(x-\boldsymbol{x}<em>{i})+\frac{1}{2}(\boldsymbol{x}-\boldsymbol{x}</em>{i})^{T}H(\boldsymbol{x}-\boldsymbol{x}_{i})$ </li>
<li>一张很有启发意义的图</li>
<li>![[Pasted image 20231102110212.png]]</li>
<li>伪代码可以理解为$x_{i+1} &#x3D; x_i -H^{-1}g$</li>
<li>补充说明<ul>
<li>如果H并不是正定的，有两种方法解决此问题<ul>
<li>采用Dk矩阵替换$H^{-1}$对角元素，具体表达式如下$D_{k}(i,i)&#x3D;\max\left(\varepsilon,\frac{\partial f^{2}}{\partial x_{i}^{2}}\right)^{-1}$ </li>
<li>或采用LMA，在Hessian非正定的情况下，退化为gradient descent</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Stochastic gradient descent<ul>
<li>在大数据量的背景下，很难利用所有数据计算Cost Function</li>
<li>解决方案：Stochasitically sample from the dataset.</li>
<li>对比Linear regression背景下，GD和SGD的区别与联系<ul>
<li>GD略，见前</li>
<li>SGD的Cost Function只对一个点计算，每计算一个点就更新所有参数权重，一般对全数据做1到10次扫描，假设数据量为m则，则一轮扫描就对参数进行了m次更新，而GD此时只进行了一次更新</li>
<li>优势：快</li>
<li>劣势<ul>
<li>失去了向量化的优势，现代的硬件，例如CPU或者GPU对向量化运算进行了优化，能够同时对数组或向量的多个元素执行相同的操作，从而大大提高运算效率，但是SGD舍弃了这种优势</li>
<li>不是所有的迭代都朝向最优方向</li>
<li>收敛速度较慢，在最优解附近可能会发生震荡等<ul>
<li>解决方案，调整学习率为$\frac{C_1}{i+C_2}$，其中$C_1,C_2$均常数，$i$为迭代次数，这可以使得学习率逐渐下降，最终在最优解附近波动较小</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Mini-batch<ul>
<li>原理和思路类似SGD，但是不是取单一点而是取一组k个点，提高了稳定性，同时也保证了相对较快的速度和较小的计算资源消耗</li>
</ul>
</li>
<li>Momentum method<ul>
<li>提出的根本原因是一般的梯度下降由于震荡问题不能引入太大的$\alpha$，而且不具有记忆</li>
<li>对比一般梯度下降和动量梯度下降<ul>
<li>一般梯度下降$x^{(i+1)}&#x3D;x^{(i)}-\alpha\nabla f(x^{(i)})$</li>
<li>而动量梯度下降表达式为：<ul>
<li>$\boldsymbol{x}^{(i+1)}&#x3D;\boldsymbol{x}^{(i)}+\boldsymbol{v}^{(i)}$</li>
<li>$\boldsymbol{v}^{(i)}&#x3D;-\alpha\nabla f\big(\boldsymbol{x}^{(i)}\big)+\boldsymbol{\theta}\boldsymbol{v}^{(i-1)}$</li>
</ul>
</li>
<li>动量梯度下降利用速度项更新$x$，而速度项$v$和梯度以及上一时刻的速度$v_{i-1}$有关，相当于保有了“记忆”</li>
<li>事实上$v_i&#x3D;-\alpha\sum_{k&#x3D;0}^{i}\theta^{k}\nabla f(x^{(i-k)})$ 对之前的的速度进行了指数衰减级的记忆</li>
</ul>
</li>
<li>补充知识：条件数Condition Number<ul>
<li>定义，$k&#x3D;(\frac{a}{b})^2$</li>
<li>一般GD $O(klog(\frac{1}{\epsilon}))$</li>
<li>一般SGD $O(\sqrt{k}log(\frac{1}{\epsilon}))$</li>
</ul>
</li>
</ul>
</li>
<li>Nesterov’s accelerated<ul>
<li>尝试解决动量梯度下降存在的，在局部最优点可能具有较大速度导致错过最优点的现象</li>
<li>原理是通过在速度项中的梯度项中，将原来的参数空间$x_i$改为“下一步的位置”，也即$x_i\theta v_{i-1}$，起到了“看前一步”的效果</li>
</ul>
</li>
<li>AdaGrad&#x2F;RMSprop<ul>
<li>AdaGrad允许学习率随着参数调整，他的宗旨是，对于梯度大的参数，我们适当减小学习率，对于梯度小的参数，我们维护一个较大的学习率<ul>
<li>$x_j^{(i+1)}&#x3D;x_j^{(i)}-\frac{\alpha}{\sqrt{g_j^{(i)}+\epsilon}}\Delta x_j^{(i)}$ </li>
<li>$g_{j}^{(i)}&#x3D;g_{j}^{(i-1)}+\left(\Delta x_{j}^{(i)}\right)^{2}$</li>
<li>这里的$g_j$维护的是该参数的历史梯度平方和，一定程度上可以反映该参数的历史梯度情况</li>
</ul>
</li>
<li>RMSprop与AdaGrad逻辑相似，但是对于历史梯度的记忆是随时间衰减的<ul>
<li>$x_{j}^{(i+1)}&#x3D;x_{j}^{(i)}-\frac{\alpha}{\sqrt{l_{j}^{(i)}}+\epsilon}\Delta x_{j}^{(i)}$ </li>
<li>$l_{j}^{(i)}&#x3D;(1-\varphi)l_{j}^{(i-1)}+\varphi\left(\Delta x_{j}^{(i)}\right)^{2}$ </li>
<li>其中$l_j$保留了梯度平方历史的加权和，并按某参数衰减，类似Momentum GD对于历史速度的处理方法</li>
</ul>
</li>
</ul>
</li>
<li>Adam(Momentum&amp;RMSprop) Nadam(Nesterov’s accelerated&amp;RMSprop)</li>
</ul>
<h3 id="Induction-amp-Deduction"><a href="#Induction-amp-Deduction" class="headerlink" title="Induction &amp; Deduction"></a>Induction &amp; Deduction</h3><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ul>
<li>Ridge Regression<ul>
<li><p>在线性回归模型中，较大的参数往往会让模型变得更为敏感，我们一般添加一些正则化项惩罚$w$项的长度</p>
</li>
<li><p>一个添加了$L2$正则化处罚项的损失函数一般类似于</p>
</li>
<li><p>$$C(\boldsymbol{w})&#x3D;\frac{1}{2}\sum_{n&#x3D;1}^{N}\left(\boldsymbol{x}<em>{n}^{T}\boldsymbol{w}-y</em>{n}\right)^{2}+\frac{\lambda}{2}|w|_{2}^{2}&#x3D;\frac{1}{2}(X\boldsymbol{w}-\boldsymbol{y})^{T}(X\boldsymbol{w}-\boldsymbol{y})+\frac{\lambda}{2}w^{T}\boldsymbol{w}$$</p>
</li>
<li><p>当然，梯度也要随之更改：</p>
</li>
<li><p>$$\nabla C(\mathbf{w})&#x3D;\nabla\left{\frac{1}{2}(X\mathbf{w}-\mathbf{y})^{T}(X\mathbf{w}-\mathbf{y})+\frac{\lambda}{2}\mathbf{w}^{T}\mathbf{w}\right}&#x3D;0$$</p>
</li>
<li><p>解得：$\boldsymbol{w}^*&#x3D;(X^TX+\lambda I)^{-1}X^T\boldsymbol{y}$</p>
<ul>
<li>这个解是符合逻辑的，如果去除正则项，则为$\boldsymbol{w}^*&#x3D;(X^TX)^{-1}X^T\boldsymbol{y}$</li>
</ul>
</li>
<li><p>应用了正则化以后，training loss会变差，这是自然的，因为引入了新的损失项目，导致模型更难过拟合以下降training loss</p>
</li>
<li><p>必须指出当$\lambda$上升时，会导致函数趋近于$f(x)&#x3D;0$以减少惩罚</p>
</li>
<li><p>但是不会将任何系数压缩到0，而是将它们都逼近0，会在模型中保留所有特征，尽管有些值会很小</p>
</li>
</ul>
</li>
<li>Lasso回归<ul>
<li>与岭回归不同的是，Lasso回归采用L1正则项</li>
<li>$$C(\boldsymbol{w})&#x3D;{\frac12}\sum_{n&#x3D;1}^N(x_n^T\boldsymbol{w}-y_n)^2+{\frac\lambda2}|w|$$</li>
<li>Lasso可以实现特征选择，参考下图，蓝绿色区域是正则化误差，而红线是平方误差等高线，Lasso在坐标轴上取得了理想的解，一个特征因此被舍弃了</li>
<li>![[Pasted image 20231104102253.png]]</li>
<li>也可以这样比较</li>
<li>![[Pasted image 20231104103346.png]]</li>
<li>可以发现随着$\lambda$的增大，Lasso回归的x轴系数最终变为0</li>
</ul>
</li>
<li>Elastic-net regression（弹性网络回归）<ul>
<li>实际上就是将Lasso和Rigid结合</li>
<li>$$\frac{1}{2}\sum_{n&#x3D;1}^{N}\bigl(x_{n}^{T}w-y_{n}\bigr)^{2}+\frac{\lambda_{1}}{2}|w|+\frac{\lambda_{2}}{2}|w|_{2}^{2}$$<blockquote>
<p>相关性并不意味着因果性，使用回归技术并不意味着特征与标签有因果关系</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="Supervised-learning-1"><a href="#Supervised-learning-1" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ul>
<li>Logistic regression<ul>
<li>即训练一组参数W、b，使得WX+b带入sigmoid函数可以区分是两类中的哪一类。一般使用梯度上升，最大化似然函数</li>
<li>Sigmoid function<ul>
<li>$$g(z)&#x3D;\frac{1}{1+e^{-z}}$$</li>
<li>Property Ⅰ  $g^{‘}(z)&#x3D; g(z)(1-g(z))$</li>
<li>Property Ⅱ  $1-g(z)&#x3D;g(-z)$</li>
</ul>
</li>
<li>Maximum likelihood estimation<ul>
<li>其实就是在给定参数$\beta$下，数据点$x_i$观察到$y_i$标签的概率，严格来说应该是给定$\beta$在给定$x_i$模型返回$y_i$的概率</li>
<li>不妨假设$y_i$服从伯努利分布，则$m$条数据的概率表达式为</li>
<li>$$\prod_{i&#x3D;1}^{m}p((x_{i},y_{i})|\boldsymbol{\beta})&#x3D;\prod_{i&#x3D;1}^{m}f_{\boldsymbol{\beta}}(x_{i})^{\nu_{i}}(1-f_{\boldsymbol{\beta}}(x_{i}))^{1-\nu_{i}}$$<ul>
<li>注记一下，这里假设$y_i$有0和1两种可能，所以直接放在指数上了</li>
</ul>
</li>
<li>综上，最大似然函数的对数是<ul>
<li>$$\mathcal{L}(\boldsymbol{\beta})&#x3D;\log\left(\prod_{i&#x3D;1}^mp((\boldsymbol{x}<em>i,y_i)|\boldsymbol{\beta})\right)&#x3D;\sum</em>{i&#x3D;1}^my_i\log\left(f_p(x_i)\right)+(1-y_i)\log\left(1-f_p(x_i)\right)$$</li>
<li>我们的目标就是找到这样一个$\beta$使得$\mathcal{L}(\beta)$最大<ul>
<li>其实也就是提高全部预测准确的概率</li>
</ul>
</li>
<li>在我们使用了$sigmoid$函数的前提下，公式又可以如下转化</li>
<li>$$\boldsymbol{\beta}^{*}&#x3D;\underset{\boldsymbol{\beta}}{\mathrm{argmax}}\sum_{i&#x3D;1}^{m}y_{i}\log\Big(g(\boldsymbol{\beta}^{T}x_{i})\Big)+(1-y_{i})\mathrm{log}\Big(1-g\big(\boldsymbol{\beta}^{T}x_{i}\big)\Big)$$</li>
<li>如果我们应用SGD求解，则最终的更新公式为</li>
<li>$$\left.\beta^{(k+1)}&#x3D;\beta^{(k)}+\alpha\left[y_i-g\left(\beta^{(k)}\right.^Tx_i\right)\right]x_i$$</li>
<li>Logistic regression的评价标准<ul>
<li>$$\frac{number, of, correctly, classified, }{number, of, total, training, data}$$</li>
<li>或者采用Efron’s</li>
<li>$$r^{2}&#x3D;1-\frac{\sum_{i&#x3D;1}^{m}(f(x_{i})-y_{i})^{2}}{\sum_{i&#x3D;1}^{m}(y_{i}-\bar{y})^{2}}$$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi-classes多分类问题<ul>
<li>Softmax<ul>
<li>公式如下</li>
<li>$$\mathrm{P}(y|\theta)&#x3D;\mathrm{P}(y|x,{\boldsymbol{\beta}<em>{k}}</em>{k&#x3D;1}^{K})&#x3D;\prod_{k&#x3D;1}^{K}\left(\frac{\exp{\boldsymbol{x}^{T}\boldsymbol{\beta}<em>{k}}}{\sum</em>{k^{\prime}&#x3D;1}^{K}\exp{\boldsymbol{x}^{T}\boldsymbol{\beta}<em>{k^{\prime}}}}\right)^{y</em>{k}}$$</li>
<li>注：对需要划分的每一个类训练一组参数（注意和深度学习区别，深度学习中一般在网络结构的最后一层统一使用softmax，并不会为每一类单独训练参数）</li>
<li>注：$y$是一个标签向量，一般采用了独热编码，例如在五分类问题中表示为$（0，0，1，0，0）$ 这一般表示为第三类。等式表示在给定k组参数和$x$的前提下，对某条数据预测正确的概率</li>
<li>注：以标签作为指数，其实就是取出了独热编码中$y_k$&#x3D;1的概率，只是形式上更为统一</li>
<li>综上，我们可以给出Log loss function</li>
<li>$$\log\prod_{i&#x3D;1}^{m}\mathrm{P}(\mathbf{y}<em>{i}|\mathbf{x}</em>{i},{\boldsymbol{\beta}<em>{k}}</em>{k&#x3D;1}^{K})&#x3D;\log\left[\prod_{i&#x3D;1}^{m}\prod_{k&#x3D;1}^{K}\left(\frac{\exp(\boldsymbol{x}<em>{i}^{T}\boldsymbol{\beta}</em>{k})}{\sum_{k^{\prime}&#x3D;1}^{K}\exp(\boldsymbol{x}<em>{i}^{T}\boldsymbol{\beta}</em>{k^{\prime}})}\right)^{y_{i,k}}\right]$$</li>
<li>以及其对每组参数的梯度</li>
<li>$$&#x3D;\sum_{i&#x3D;1}^{m}[y_{i,k}-\theta_{k}]x_{i}$$</li>
</ul>
</li>
<li>concave(上凸的)、convex(下凸的)</li>
<li>Prceptron<ul>
<li>通过多层感知机可以实现数据区域的分割</li>
<li>本质上来讲，单层感知机可以视作一个线性分类器，把空间按照一定的规则划分为两半</li>
<li>$$f_{A}(x)&#x3D;\mathrm{sgn}\left(\sum_{i&#x3D;0}^{d}\beta_{i}x_{i}\right)&#x3D;\mathrm{sgn}(\beta^{T}x)$$</li>
</ul>
</li>
</ul>
</li>
<li>GDA高斯判别分析<ul>
<li>与一般的Discriminative learning算法不同，GDA尝试学习$p(x|y)(and ; p(y))$ ,之后利用其去构建后验分布</li>
</ul>
</li>
</ul>
<h4 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h4><ul>
<li>Logistic Regression只有在数据本身分布比较理想，具有明确的界限时，效果才会比较良好</li>
<li>树，连通无环结构<ul>
<li>树的节点表示属性测试</li>
<li>叶节点表示分类结果</li>
<li>每一次划分其实是对空间进行一次轴对齐的超平面划分，树可以向图转换，图也可以还原成树</li>
</ul>
</li>
<li>$Classification; Error$（可以用来评价划分的质量）<ul>
<li>$Error(i|j,t_{j}) &#x3D; 1-\max <em>{k}P(k|R</em>{i})$ </li>
<li>注意这里P不是概率，而是$R_i$中k的比例，比方说一个完美的分类，$R_i$中只有k，那么$Error$就是0</li>
<li>其中，j是划分的维度，tj是划分的阈值</li>
<li>树当前的每一个叶子节点都代表了一块区域，而算法的继续进行需要做的，就是将该空间继续划分成两块</li>
<li>$\min _{j, t_j}\left{\frac{N_1}{N} \operatorname{Error}\left(1 \mid j, t_j\right)+\frac{N_2}{N} \operatorname{Error}\left(2 \mid j, t_j\right)\right}$<ul>
<li>区域被划分为两块，分别是$R_1$和$R_2$，将两块区域的误差加权相加即得到需要最小化的误差函数</li>
</ul>
</li>
</ul>
</li>
<li>我们也可以使用$Gini;index$ <ul>
<li>通过测试每个区域的”纯度”，来判断划分质量</li>
<li>$Gini(i|j,t_j)&#x3D;1-\Sigma_k P(k|R_i)^2$</li>
<li>再次提醒，$P$是$k$类在$R_i$中的比例</li>
<li>$Gini$值越低，划分质量越高，所以我们也要最小化$Gini$函数的加权和</li>
</ul>
</li>
<li>我们也可以使用$Entropy$<ul>
<li>$H(X)&#x3D;$ $-\sum_{x \in X} P(x) \log _2 P(x)$ in each newly created region $R_1, R_2$.</li>
<li>$\operatorname{Entropy}\left(i \mid j, t_j\right)&#x3D;-\sum_k P\left(k \mid R_i\right) \log _2 P\left(k \mid R_i\right)$</li>
</ul>
</li>
</ul>
<h5 id="Stopping-Condition"><a href="#Stopping-Condition" class="headerlink" title="Stopping Condition"></a>Stopping Condition</h5><ul>
<li>maximum depth</li>
<li>number of instance</li>
</ul>
<h5 id="For-Regression"><a href="#For-Regression" class="headerlink" title="For Regression"></a>For Regression</h5><h5 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h5><ul>
<li>每一次划分都最优或接近最优，则correlation大，这可能降低效果</li>
<li>所以需要引入随机森林以降低Variance</li>
</ul>
<h5 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h5><ul>
<li>每棵树每步分裂限制特征集，限制每一步的“视角”，在限制的特征集中选择最优的特征和阈值，最终融合不同视角进行投票</li>
</ul>
<h5 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h5><ul>
<li>The strength of week learners</li>
<li>原理是，首先使用week classifier进行学习，之后依据预测结果，给出residual。下一个week classifier对特征和残差进行学习，得到一个新的预测结果，和residual得出下一个residual，反复进行，并在最后以一定参数融合所有的week classifier</li>
</ul>
<h5 id="GBoosting"><a href="#GBoosting" class="headerlink" title="GBoosting"></a>GBoosting</h5><ul>
<li>XGBoost是GBoosting的一种特定实现</li>
</ul>
<h5 id="K-nearest-neighbourhood"><a href="#K-nearest-neighbourhood" class="headerlink" title="K-nearest neighbourhood"></a>K-nearest neighbourhood</h5><ul>
<li>关键思想：一个样本的类别可以通过其K个最近邻居的类别来决定</li>
<li>也可以固定邻居数、固定搜索半径等</li>
<li>也可以使用soft boundary，距离点越近权重越高</li>
<li>但同时，测算点的距离对于计算机是较为繁琐的</li>
</ul>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="K-means（no-label）"><a href="#K-means（no-label）" class="headerlink" title="K-means（no label）"></a>K-means（no label）</h4><ul>
<li>首先在数据点中，随机选K个点作为聚类中心</li>
<li>将每个数据点分配到距离它最近的聚类中心</li>
<li>对于每个聚类，计算其所有数据点的平均位置，更新聚类中心的位置</li>
<li>直达聚类中心不发生变化或者达到预定的迭代次数</li>
</ul>
<h4 id="Hierarchical-clustering（层次聚类）"><a href="#Hierarchical-clustering（层次聚类）" class="headerlink" title="Hierarchical clustering（层次聚类）"></a>Hierarchical clustering（层次聚类）</h4><ul>
<li>Hierarchical agglomerative clustering（层次凝聚聚类）<ul>
<li>初始状态下，每个数据点都被认为是一个单独的聚类。</li>
<li>计算每对聚类之间的相似度或距离。</li>
<li>根据相似度或距离的度量规则，选择最相似的两个聚类进行合并。</li>
<li>合并后的聚类形成一个新的聚类，替代原来的两个聚类。</li>
<li>重复步骤，直到所有数据点都合并为一个聚类或达到预设的聚类数量。</li>
</ul>
</li>
<li>Divisive clustering（分裂聚类）<ul>
<li>初始状态下，所有数据点都被归为一个聚类。</li>
<li>计算当前聚类中的数据点的相似度或距离。</li>
<li>根据相似度或距离的度量规则，选择一个聚类进行分裂。</li>
<li>分裂所选的聚类，将其划分为两个或多个子聚类。</li>
<li>重复步骤2-4，直到每个数据点都成为一个单独的聚类或达到预设的聚类数量。</li>
</ul>
</li>
</ul>
<h3 id="Big-Data-Clustering"><a href="#Big-Data-Clustering" class="headerlink" title="Big Data Clustering"></a>Big Data Clustering</h3><ul>
<li><h4 id="BFR（人名算法）"><a href="#BFR（人名算法）" class="headerlink" title="BFR（人名算法）"></a>BFR（人名算法）</h4><ul>
<li>初始阶段，将整个数据集分为几个较小的子集。</li>
<li>在每个子集上应用一个聚类算法，如K-means或层次聚类。</li>
<li>分析每个子集的聚类结果，根据一些评估指标（如聚类质量、聚类数量等）来决定是否需要进一步划分或合并聚类。</li>
<li>如果需要划分，将子集进一步划分为更小的子集，并重复步骤2-3。<br>  如果需要合并，将具有相似性质的聚类合并在一起，并重复步骤2-3。</li>
</ul>
</li>
<li>Clustering using representatives (CURE)</li>
</ul>
<h2 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a>Support vector</h2><blockquote>
<p>核心逻辑，尝试最大化两个类别中的间隔，同时确保所有的数据点都被正确分类<br>使用Lagrange对偶性，我们可以将这个问题转化为对偶问题<br>一旦我们解决了对偶问题，我们可以使用互补松弛性来确定哪些数据点是支持向量，从而确定决策边界</p>
</blockquote>
<ul>
<li>Support vector regression<ul>
<li>让马路盖住所有点，但是越窄越好</li>
</ul>
</li>
<li>Support vector clustering</li>
<li>Transudative support vector machine</li>
</ul>
<h3 id="Kernel-方法"><a href="#Kernel-方法" class="headerlink" title="Kernel 方法"></a>Kernel 方法</h3><blockquote>
<p>将点向高维映射，有时可以使数据点明显分开</p>
</blockquote>
<h2 id="Learning-with-Probabilistic-Graphic-Model"><a href="#Learning-with-Probabilistic-Graphic-Model" class="headerlink" title="Learning with Probabilistic Graphic Model"></a>Learning with Probabilistic Graphic Model</h2><h3 id="Directed-graphs-Baysian-network"><a href="#Directed-graphs-Baysian-network" class="headerlink" title="Directed graphs (Baysian network)"></a>Directed graphs (Baysian network)</h3><ul>
<li>逻辑是，对于多个事件，建立很多小的表，以避免建立所有情况合成的大表</li>
<li>从独立到条件独立</li>
</ul>
<h3 id="Undirected-graphs-Markov-random-field"><a href="#Undirected-graphs-Markov-random-field" class="headerlink" title="Undirected graphs (Markov random field)"></a>Undirected graphs (Markov random field)</h3><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h3 id="马尔可夫奖励过程（MRP-Markov-reward-process）"><a href="#马尔可夫奖励过程（MRP-Markov-reward-process）" class="headerlink" title="马尔可夫奖励过程（MRP Markov reward process）"></a>马尔可夫奖励过程（MRP Markov reward process）</h3><ul>
<li>首先进行基本定义，一般可以定义为$\langle S,P,R,\gamma \rangle$<ul>
<li>$S$是一个有限的状态集合</li>
<li>$P$是转移概率矩阵，$P_{ss’}$记录了从$s$状态出发，到达$s’$状态的概率</li>
<li>$R$是奖励函数<ul>
<li>$R_s$是当前在$s$状态下，下一时刻能获得的奖励期望</li>
</ul>
</li>
<li>$\gamma$是折现参数<ul>
<li>对远期能获得的奖励乘系数“折现”</li>
</ul>
</li>
</ul>
</li>
<li>关键方程<ul>
<li>位置价值函数$v(s) &#x3D;\mathbb{E}[G_t|S_t&#x3D;s]$ 也就是现在在状态$s$，之后总的收益期望</li>
<li>也可以递归定义如下：$$v(s)&#x3D;\mathbb{E}[R_t|S_t&#x3D;s]+\gamma\sum_{s^{\prime}}v(s^{\prime})P[S_{t+1}&#x3D;s^{\prime}|S_t&#x3D;s]$$</li>
<li>第一项是在$t$时刻处于状态$s$时，$t+1$时刻获得的即时$Reward$的期望，后面的累加项统计了下一步的所有可能，并将它们的价值函数折现求和。</li>
<li>这里的$\gamma$是折现函数</li>
</ul>
</li>
<li>两种计算$v$的方法<ul>
<li>矩阵计算<ul>
<li>$v&#x3D;R+\gamma pv$ </li>
<li>故$v&#x3D;(I-\gamma\mathcal{P})^{-1}\mathcal{R}$</li>
</ul>
</li>
<li>动态规划<ul>
<li>在收敛之前，依据公式反复迭代<ul>
<li>$v^{(k)}(s)&#x3D;R(s)+\gamma\sum_{s’\in S}P(s’|s)v^{(k-1)}\left(s’\right)$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="马尔可夫决策过程（MDP）"><a href="#马尔可夫决策过程（MDP）" class="headerlink" title="马尔可夫决策过程（MDP）"></a>马尔可夫决策过程（MDP）</h3><blockquote>
<p>在MRP基础上引入了决策过程</p>
</blockquote>
<ul>
<li>同样可给出定义$\langle S,A,P,R,\gamma \rangle$<ul>
<li>补充定义了$A$，$A$是有限的行为的集合，例如平面游戏中的上下左右之类的</li>
</ul>
</li>
<li>策略(policy)<ul>
<li>策略就是在给定状态下，决策的分布，一般记作$\pi$，定义是$\left.\pi(a|s)&#x3D;\mathbb{P}\left[A_{t}&#x3D;a\right|S_{t}&#x3D;s\right]$</li>
</ul>
</li>
<li>在引入了决策过程后，状态价值函数发生了改变，因为此时其同样与policy相关<ul>
<li>定义$v_{\pi}(s)&#x3D;\mathbb{E}<em>{\pi}[G</em>{t}|S_{t}&#x3D;s]$ ，这是新的状态价值函数</li>
<li>定义$q_\pi(s,a)&#x3D;\mathbb{E}_\pi[G_t|S_t&#x3D;s,A_t&#x3D;\alpha]$ ，这被称作决策价值函数<ul>
<li>含义是：在时间$t$处于状态$s$时，做出决策$\alpha$且之后遵循policy $\pi$的预期收益</li>
</ul>
</li>
<li>在如上定义之后，我们可以把$v$的公式改写为<ul>
<li>$v_\pi(s)&#x3D;\mathbb{E}<em>\pi[G_t|S_t&#x3D;s]&#x3D;\sum</em>{a\in\mathcal{A}}\mathbb{E}_\pi[G_t,A_t&#x3D;a|S_t&#x3D;s]$<ul>
<li>穷举所有的决策求和</li>
</ul>
</li>
<li>$&#x3D;\sum_{a\in\mathcal{A}}\mathbb{E}<em>{\pi}[G</em>{t}|S_{t}&#x3D;s,A_{t}&#x3D;a]\mathbb{P}[A_{t}&#x3D;a|S_{t}&#x3D;s]$</li>
<li>$&#x3D;\sum_{a\in\mathcal{A}}q_{\pi}(s,a)\pi(a|s)$ <ul>
<li>当然，我们也可以将$q_{\pi}$定义为递归形式</li>
<li>${\mathcal R}<em>{s}^{a}+\gamma\sum</em>{\forall s\prime}v_{\pi}(s^{\prime}){\mathcal P}_{ss^{\prime}}^{a}$<ul>
<li>思路与之前类似</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>小网格世界示例<ul>
<li>![[Pasted image 20231107164046.png]]<ul>
<li>agent服从全局随机策略，即上下左右均为0.25概率</li>
<li>如果操作走出了网格，则状态不变</li>
<li>多次迭代直到收敛，我们就对每个点建立了$v_{\pi(random)}$ 但是，基于这种策略的答案，是最好的吗？</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="MDP-Policy-Iteration"><a href="#MDP-Policy-Iteration" class="headerlink" title="MDP: Policy Iteration"></a>MDP: Policy Iteration</h3><ul>
<li>主体思想是，对于每一种策略$\pi$，先计算$v_{\pi}$再将$greedy(\pi)$作为下一个策略，反复进行策略改进<ul>
<li>一般可以用$\pi(a|s)&#x3D;\frac1{A(s)}$进行初始化（$A(s)$指的是$s$状态下，可执行的决策总数）</li>
<li>伪代码表示策略更新<ul>
<li>$\pi^{<em>}(a|s)&#x3D;\operatorname</em>{argmax}<em>{\mathrm{a}}q</em>{\pi}(s,a)$</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>数理方程特殊函数部分以及其他</title>
    <url>/2023/06/14/shu-li-fang-cheng/</url>
    <content><![CDATA[<p>级数解</p>
<ul>
<li>高次项系数化为1，判断展开点类型，为常点还是奇点</li>
<li>回到原式中带入级数解进行计算</li>
</ul>
<p>坐标系</p>
<ul>
<li>极坐标下拉普拉斯：$\frac{1}{r}\frac{\partial}{\partial r}(r\frac{\partial}{\partial r})+\frac{1}{r^2}\frac{\partial^2}{\partial \phi^2}$</li>
<li>柱坐标下拉普拉斯：$\frac{1}{r}\frac{\partial}{\partial r}(r\frac{\partial}{\partial r})+\frac{1}{r^2}\frac{\partial^2}{\partial \phi^2}+\frac{\partial^2}{\partial z ^2}$</li>
<li>球坐标系拉普拉斯：$\frac{1}{r^2}\frac{\partial}{\partial r}(r^2\frac{\partial}{\partial r})+\frac{1}{r^2sin^2\theta}\frac{\partial^2}{\partial \phi^2}+\frac{1}{r^2sin\theta}\frac{\partial}{\partial \theta}(sin\theta\frac{\partial}{\partial \theta})$</li>
</ul>
<p>Bessel方程</p>
<ul>
<li>$v$阶Bessel方程：$x^{2}y’’+xy’+(x^{2}-v^{2})y&#x3D;0$</li>
<li>解记作$J_{\pm v(x)}$</li>
<li>构造$N_{v}(x)&#x3D;\frac{cosv\pi J_{v}(x)-J_{-v}(x)}{sinv\pi}，J_{v}$与$N_{v}$线性无关</li>
<li><del>第三类柱函数</del></li>
<li>对于$\Gamma$函数的补充，对正整数，$\Gamma (k+1)&#x3D;k!，\Gamma(\frac{1}{2})&#x3D;\sqrt{\frac{\pi}{2}}$</li>
<li>基本递推关系</li>
</ul>
<p>虚宗量Bessel方程：$x^{2}y’’+xy’+(-x^{2}-v^{2})y&#x3D;0$</p>
<ul>
<li>第一类$I_{v}(x)$，第二类 $K_{v}(x)$</li>
</ul>
<p>球Bessel方程：$\frac{d}{dx}(x^2\frac{dR}{dx})+[x^2-l(l+1)]R$</p>
<ul>
<li><p>特征为$l(l+1)$</p>
</li>
<li><p>解为$ j_l, n_l$</p>
</li>
</ul>
<p>渐进行为</p>
<ul>
<li>讨论渐进行为的意义在于，求解本征值问题时，需要去除不符合边界条件的函数</li>
<li>x趋向于0时，若$v&#x3D;0$，$J_0(x)$趋向于1，若$v&gt;0$，$J_v(x)$趋向于0</li>
<li>x趋向于0时，$N_v(x)$趋向于$\infin$</li>
<li>x趋向于$\infin$时，$J_{v}(x)与N_{v}(x)$均趋向于0</li>
<li>$ j_l, n_l$与上述$ J_v, N_v$类似</li>
<li>x趋向于0时，若$v&#x3D;0$，$I_0(x)$趋向于1，若$v&gt;0$，$I_v(x)$趋向于0</li>
<li>x趋向于0时，$K_v(x)$趋向于$\infin$</li>
<li>x趋向于$\infin$时，$I_{v}(x)$趋向于0，$K_{v}(x)$趋向于$\infin$</li>
</ul>
<p>Legendre方程和连带Legendre方程的形式</p>
<ul>
<li><p>$\frac{1}{sin\theta}\frac{d}{d\theta}(sin\theta\frac{d\Theta}{d\theta})+(\lambda-\frac{\mu}{sin^{2}\theta})\Theta&#x3D;0$</p>
</li>
<li><p>$(1-x^2)y’’-2xy’+(\lambda-\frac{m^2}{1-x^2})y$</p>
<ul>
<li>$m&#x3D;0$时，为勒让德方程，$m&#x3D;1、2、3…$时，为连带勒让德方程</li>
</ul>
</li>
<li><p>Legendre方程$(1-x^2)y’’-2xy’+\lambda y$ ，若补充边界约束$y(\pm1)&lt;\infin$则退化为级数解</p>
<ul>
<li>由级数解，$\lambda_{l}&#x3D;l(l+1),l&#x3D;0、1、2…$，本征函数为$P_{l}(x)$</li>
<li>Legendre多项式的微分表示：$\frac{1}{2^{l}l!}[(x^{2}-1)^{l}]^{(l)}$</li>
</ul>
</li>
<li><p>Legendre多项式的生成函数</p>
<ul>
<li>$\sum_{l&#x3D;0}^{\infty} P_l(x) t^l&#x3D;\frac{1}{\sqrt{1-2 x t+t^2}}$</li>
<li>$||P_{l}||^{2}&#x3D;\frac{2}{2l+1}$ (拿生成函数直接平方积分比较系数)</li>
</ul>
</li>
<li><p>连带Legendre方程$(1-x^2)y’’-2xy’+(\lambda-\frac{m^2}{1-x^2})y$</p>
<ul>
<li><p>微分表示 $\frac{1}{2^{l}l!}(-1)^{m}(1-x^2)^{\frac{m}{2}}[(x^{2}-1)^{l}]^{(l+m)}$ 其中 $l&gt;m$</p>
</li>
<li><p>$||P_{l}^{m}||^{2}&#x3D;\frac{2}{2l+1}\frac{(l+m)!}{(l-m)!}$</p>
</li>
</ul>
</li>
<li><p>球谐函数（拉普拉斯方程角向方程的本征函数）</p>
<ul>
<li>函数来源：半径为a的球内拉普拉斯方程，利用$\lambda$分离$R(r)S(\theta,\phi)$，得到角向和径向方程。再利用$\mu$分离$S(\theta,\phi)&#x3D;\Theta(\theta)\Phi(\phi)$</li>
<li>把$\theta和\phi$组合起来得$S$，这就是球谐函数</li>
<li>球谐函数的模长<ul>
<li>补充说明：径向函数为欧拉方程，设$r&#x3D;e^{t}$求解</li>
</ul>
</li>
</ul>
</li>
<li><p><del>递推公式、连带Legendre函数模方</del></p>
</li>
</ul>
<p>傅里叶变换和逆变换形式</p>
<p> Laplace变换和逆变换形式</p>
<p>傅里叶变换的卷积公式</p>
<p>Laplace变换的卷积公式</p>
<p>方程标准型特征方程</p>
<ul>
<li><p>特征方程法</p>
<ul>
<li><p>二阶方程经过标准化变换后新的系数公式会给</p>
</li>
<li><p>解题时，首先准确写出各项系数，特别注意 $ b $ 在原式中为$2b$</p>
</li>
<li><p>之后判断方程类型，$\Delta&lt;0$ 时为椭圆型（方程形式为$U_{\rho\rho}+U_{\sigma \sigma}&#x3D;\Phi$） </p>
</li>
<li><p>求解特征方程$a(\frac{dy}{dx})^2-2b(\frac{dy}{dx})+c&#x3D;0$ 方法与解二次函数类似，得到由常数控制的一组（抛物型）或者两组方程。</p>
</li>
<li><p>分别令 $\xi$ 和 $\eta$ 等于这两组方程（抛物型 $\eta$ 任取与 $\xi$ 独立即可） </p>
</li>
<li><p>利用变换系数公式求解方程各项系数</p>
</li>
</ul>
</li>
<li><p>配方法</p>
<ul>
<li><del>变系数配方法</del></li>
<li>常系数配方法，将二次项配方后，换元即可</li>
</ul>
</li>
</ul>
<p>格林函数</p>
<ul>
<li>电像法</li>
<li>本征函数展开法<ul>
<li>首先将式子列成$L(u)&#x3D;-\rho$ 求解本征函数问题$(L+\lambda_{n})u_{n}&#x3D;0$<ul>
<li>注：$\rho$也可能为0，例如导体球内部无电荷</li>
</ul>
</li>
<li>将$G$ 展开并带入$\Delta G$，与$-\rho$联立求展开系数即可</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>时光</tag>
      </tags>
  </entry>
  <entry>
    <title>工程图学</title>
    <url>/2023/11/01/gong-cheng-tu-xue/</url>
    <content><![CDATA[<p>面对应封闭线框<br>类似性指导作图<br>粗实线&gt;不可见轮廓线（虚线）&gt;对称线（点画线）&gt;辅助线</p>
<h2 id="立体与立体相交"><a href="#立体与立体相交" class="headerlink" title="立体与立体相交"></a>立体与立体相交</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>立体与立体相交简称相贯，立体表面交线称相贯线</p>
<h3 id="相贯线的主要性质"><a href="#相贯线的主要性质" class="headerlink" title="相贯线的主要性质"></a>相贯线的主要性质</h3><ul>
<li>表面性：位于两立体的表面上</li>
<li>封闭性：一般是封闭的空间曲线或折线</li>
<li>共有性：是两立体表面的共有线</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>材料分析与表征 9.18</title>
    <url>/2023/09/18/cai-liao-fen-xi-yu-biao-zheng/</url>
    <content><![CDATA[<span id="more"></span>
<ul>
<li>课程详细信息<ul>
<li>期末考试70%、课程展示20%、考勤作业10%</li>
</ul>
</li>
</ul>
<h3 id="X射线衍射技术-X-Ray-Diffraction"><a href="#X射线衍射技术-X-Ray-Diffraction" class="headerlink" title="X射线衍射技术(X-Ray Diffraction)"></a>X射线衍射技术(X-Ray Diffraction)</h3><ul>
<li>X光的发现 1895.11.8 伦琴</li>
<li>肉眼不可见、使铂氯化钡发光、使照相底板感光、气体电离、杀伤生物细胞</li>
</ul>
<h3 id="微观结构的研究方法"><a href="#微观结构的研究方法" class="headerlink" title="微观结构的研究方法"></a>微观结构的研究方法</h3><ul>
<li>仪器极限分辨率 $\lambda$&#x2F;2 （远场光学）</li>
</ul>
<h3 id="X光的本质"><a href="#X光的本质" class="headerlink" title="X光的本质"></a>X光的本质</h3><ul>
<li>波长介于紫外线和 $\gamma$ 射线之间，约为0.01~100埃</li>
<li>长波长——软X射线 短波长——硬X射线</li>
</ul>
<h3 id="X射线的应用与发展"><a href="#X射线的应用与发展" class="headerlink" title="X射线的应用与发展"></a>X射线的应用与发展</h3><h3 id="X射线的性质"><a href="#X射线的性质" class="headerlink" title="X射线的性质"></a>X射线的性质</h3><h3 id="X射线的产生"><a href="#X射线的产生" class="headerlink" title="X射线的产生"></a>X射线的产生</h3><ul>
<li>高速运动的带电粒子的运动遇到障碍被减速时便产生X射线</li>
<li>从X射线管角度理解<ul>
<li>对绕成螺旋型的钨丝通电加热至白热，放出热辐射电子</li>
<li>电子在数万伏管内高压电场的作用下轰击靶材产生X射线</li>
</ul>
</li>
</ul>
<h3 id="X射线谱"><a href="#X射线谱" class="headerlink" title="X射线谱"></a>X射线谱</h3><ul>
<li>连续X射线谱，有公式$eV &#x3D; h\nu <em>{max}&#x3D;\frac{hc}{\lambda</em>{0}}$ </li>
<li>经验规律，连续谱中强度最大处的波长记为$\lambda_{m}$ 则一般有$\lambda_{m}&#x3D;1.5\lambda_{0}$</li>
<li>电流决定了热辐射电子的多少，电压决定了热辐射电子的强度</li>
</ul>
<h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><ul>
<li>已知$Cu-K_a$线波长，计算铜原子$2p$和$1s$能级间的能量差<ul>
<li>解题方法，首先明确从$2p$到$1s$属于从$L$层跃迁到$K$层，也即$K_a$线</li>
<li>故直接利用波长与能量关系计算</li>
</ul>
</li>
</ul>
<h3 id="X射线与物质的相互作用"><a href="#X射线与物质的相互作用" class="headerlink" title="X射线与物质的相互作用"></a>X射线与物质的相互作用</h3><h3 id="X射线衰减规律"><a href="#X射线衰减规律" class="headerlink" title="X射线衰减规律"></a>X射线衰减规律</h3><ul>
<li>吸收的不连续性</li>
<li>吸收限的利用<ul>
<li>靶材（Target Material）：</li>
<li>靶材通常是一个坚固的材料，如金属，用于产生X射线。当高速电子束撞击靶材时，与原子相互作用，产生X射线辐射。我们要尽量选择波长略长于试样吸收限的靶材，这样不会有K系荧光辐射，同时也不会被试样吸收太多。</li>
<li>滤片（Filter）：</li>
<li>滤片是放置在X射线束路径上的材料，通常由金属或其他适当的材料制成。滤片的主要作用是改变或调整X射线的能量谱。通过选择不同材料的滤片，可以过滤掉低能量的X射线，使得只有特定能量范围内的X射线通过</li>
<li>试样（Sample）：</li>
<li>试样是要进行X光实验的物质或样品。当X射线穿过试样时，它会与试样中的原子相互作用，产生一系列特定能量的X射线散射或吸收。通过分析这些散射或吸收事件，可以获得有关试样的信息，例如其成分、结构或厚度等</li>
</ul>
</li>
</ul>
<h3 id="X光的探测与防护"><a href="#X光的探测与防护" class="headerlink" title="X光的探测与防护"></a>X光的探测与防护</h3><h2 id="晶体学基础"><a href="#晶体学基础" class="headerlink" title="晶体学基础"></a>晶体学基础</h2><h3 id="晶体结构"><a href="#晶体结构" class="headerlink" title="晶体结构"></a>晶体结构</h3><ul>
<li><p>赤平极射投影</p>
<ul>
<li>方位角 $\phi$包含该点的子午面与$0°$子午面的夹角</li>
<li>极距角 $\rho$该点与北极点的夹角</li>
<li>赤平投影图上点距离圆心的距离与方位角有关，基圆与极距角有关<br>  重点（吴氏网、标准点阵、倒易点阵）</li>
</ul>
</li>
<li><p>吴氏网</p>
<ul>
<li>吴氏网只能水平上走格子，如果想在竖直上走格子，需要旋转吴氏网，然后再所有的点一起水平上走格子<br> 对于任意可微的对称性，一定存在一个相应的守恒流</li>
</ul>
</li>
<li><p>倒易点阵</p>
<ul>
<li><p>对于一个给定基矢的正点阵，必有倒易点阵与之相对应，关系为<br>$$a^{*}&#x3D;\frac{b×c}{v}$$<br>此处$v$为正点阵的体积	</p>
</li>
<li><p>有关正点阵和倒易点阵，相关的符号标识如下</p>
<ul>
<li>晶面指数$(hkl),(uvw)^{*}$</li>
<li>晶向指数$[uvw],[hkl]^{*}$</li>
<li>面间距$d_{hkl},d_{uvw}^{*}$</li>
<li>晶向或阵矢$r&#x3D;ua+vb+wc,g^{<em>}&#x3D;ha^</em>+kb^*+lc^*$</li>
<li>晶向长度或阵矢大小$r_{uvw},g_{hkl}$</li>
<li>结点位置$uvw,hkl$</li>
</ul>
</li>
<li><p>倒易点阵晶向垂直于正点阵同名晶面，反之亦然</p>
</li>
<li><p>正点阵的面间距是同名倒易矢量长度的倒数</p>
<p>$$|g_{hkl}|^2&#x3D; \frac1{d_{hkl}^2}&#x3D;(h\boldsymbol{a}^*+k\boldsymbol{b}^*+l\boldsymbol{c}^*)\bullet(h\boldsymbol{a}^*+k\boldsymbol{b}^*+l\boldsymbol{c}^*)\&#x3D; h^2(a^*)^2+k^2(b^*)^2+l^2(c^*)^2\+2kl\boldsymbol{b}^<em>\boldsymbol{c}^</em>\boldsymbol{c}^<em>cos\boldsymbol{a}^</em>+2lh\boldsymbol{c}^<em>\boldsymbol{a}^</em>cos\boldsymbol{\beta}^*+2h\boldsymbol{k}a^<em>\boldsymbol{b}^</em>cos\boldsymbol{\gamma}^* $$</p>
<ul>
<li>立方晶系<ul>
<li>$d_{hkl}&#x3D;\frac{a}{\sqrt{h^2+k^2+l^2}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="阶段作业"><a href="#阶段作业" class="headerlink" title="阶段作业"></a>阶段作业</h3><ul>
<li>P81-82 T 15 16 22 24</li>
</ul>
<h2 id="X射线与材料的散射、干涉与衍射"><a href="#X射线与材料的散射、干涉与衍射" class="headerlink" title="X射线与材料的散射、干涉与衍射"></a>X射线与材料的散射、干涉与衍射</h2><h3 id="散射"><a href="#散射" class="headerlink" title="散射"></a>散射</h3><ul>
<li><p>电子的散射，静电相互作用等</p>
</li>
<li><p>X射线的散射，与物质的基本单元发生作用</p>
</li>
<li><p>单电子对X射线的散射</p>
<ul>
<li>相干散射<ul>
<li>相干散射是衍射的基础，从波动性的角度出发，X射线是电磁波，其照射在自由电子上时，交变电场就会强迫电子作频率相同的振动，发射X射线。电子发出的X射线为散射线，与入射线波长相同、相位滞后恒定，故可以发生干涉。</li>
<li>平面偏振光相干散射</li>
<li>非平面偏振光相干散射<ul>
<li>原子中只需要考虑电子散射</li>
</ul>
</li>
</ul>
</li>
<li>非相干散射</li>
</ul>
</li>
<li><p>散射线的干涉</p>
<ul>
<li>相位差与反射矢量<ul>
<li>相位差满足$$\phi &#x3D; 2\pi \frac{-S_{0}·r+S·r }{\lambda}$$其中$S_0$为入射线上的单位矢量，$S$为反射线上的单位矢量</li>
<li>记 $s&#x3D;\frac{S-S_0}{\lambda}\quad s$为散射矢量，其大小为$\frac{2sin\theta}{\lambda}$，引入散射矢量后，上式可化作$\phi &#x3D;2\pi s·r$</li>
</ul>
</li>
</ul>
</li>
<li><p>原子对X射线的散射</p>
<ul>
<li>单电子原子的散射<ul>
<li>原子中一个电子的散射因子$$f(s) &#x3D; \int_v \rho(r)e^{i2\pi s·r}dv$$</li>
<li>电子的相干强度为$I_{相干}&#x3D;f^{2}(s)I_{电子}$</li>
</ul>
</li>
<li>多电子原子的散射<ul>
<li>假设原子的总电荷密度为各个电子电荷密度之和</li>
</ul>
</li>
</ul>
</li>
<li><p>晶胞的散射</p>
</li>
<li><p>小晶体的衍射</p>
<ul>
<li><p>小晶体的衍射强度</p>
<ul>
<li><p>小晶体的衍射线振幅</p>
<p>$$<br>A_\text{ 晶体 }{ ( s ) }&#x3D;\sum_{m&#x3D;0}^{N_a-1}\sum_{m&#x3D;0}^{N_b-1}\sum_{p&#x3D;0}^{N_c-1}F\left(s\right)\mathrm{e}^{\mathrm{i}2\pi\mathbf{s}\cdot\mathbf{R}_{mnp}}<br>$$</p>
<p>因其中的结构因子与晶胞的位置无关, 从而有<br>$$<br>A_\text{ 晶体 }{ ( s ) }&#x3D;F\left(s\right)\sum_{m&#x3D;0}^{N_a-1}\sum_{m&#x3D;0}^{N_b-1}\sum_{p&#x3D;0}^{N_c-1}\mathrm{e}^{\mathrm{i}2\pi\mathbf{s}\cdot\mathbf{R}<em>{mnp}}<br>$$<br>于是, 晶体的衍射线强度为<br>$$<br>I</em>{\text {晶体 }}(s)&#x3D;|F(s)|^2\left|\sum_{m n p}^N \mathrm{e}^{\mathrm{i} 2 \pi s \cdot \boldsymbol{R}<em>{m p p}}\right|^2 I</em>{\text {电子 }}<br>$$<br>定义<br>$$<br>L(s) \equiv\left|\sum_{m n p}^N \mathrm{e}^{\mathrm{i} 2 \pi s \cdot \boldsymbol{R}<em>{\operatorname{mmp}}}\right|^2<br>$$<br>由于它表示的是晶体散射线的干涉结果, 故称为晶体的干涉函数, 或劳厄函数, 衍射线强度可以简写成<br>$$<br>I</em>{\text {晶体 }}(s)&#x3D;|F(s)|^2 L(s) I_{\text {电子 }}<br>$$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="衍射线的强度分析"><a href="#衍射线的强度分析" class="headerlink" title="衍射线的强度分析"></a>衍射线的强度分析</h2><h2 id="多晶体衍射信息的获取方法"><a href="#多晶体衍射信息的获取方法" class="headerlink" title="多晶体衍射信息的获取方法"></a>多晶体衍射信息的获取方法</h2><h3 id="德拜法"><a href="#德拜法" class="headerlink" title="德拜法"></a>德拜法</h3><ul>
<li>德拜照片的计算与标定<ul>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009103514509.png" alt="image-20231009103514509" style="zoom:50%;" />
</li>
<li><p>圆弧之间的对应的顶角为4$\theta$</p>
</li>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009104249654.png" alt="image-20231009104249654" style="zoom:50%;" />
</li>
<li><p>此处采用的角度标准为（deg）</p>
</li>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009104341784.png" alt="image-20231009104341784" style="zoom:50%;" />
</li>
<li><p>考虑布拉格公式$d&#x3D;\frac{\lambda}{2sin\theta}$和晶面间距计算公式$d_{hkl}&#x3D;\frac{a}{\sqrt{h^2+k^2+l^2}}$</p>
</li>
<li><p>得$$\sin^2\theta&#x3D;\frac{\lambda^2}{4a^2}(h^2+k^2+l^2)$$</p>
</li>
<li><p>令$\frac{\lambda^{2}}{4a^2}&#x3D;K$并设$m$为干涉指数的平方和，即$$m&#x3D;h^2+k^2+l^2$$于是$sin^2\theta &#x3D; Km$</p>
</li>
<li><p>计算出$K$和每条德拜线的$sin^2\theta$并计算出对应的m值（$m$均为正整数）然后依据衍射线条在晶系中的出现顺序判断</p>
</li>
</ul>
</li>
</ul>
<h3 id="衍射仪法"><a href="#衍射仪法" class="headerlink" title="衍射仪法"></a>衍射仪法</h3><h3 id="衍射材料的获得"><a href="#衍射材料的获得" class="headerlink" title="衍射材料的获得"></a>衍射材料的获得</h3><h4 id="试样制备要求"><a href="#试样制备要求" class="headerlink" title="试样制备要求"></a>试样制备要求</h4><ul>
<li>晶粒大小</li>
<li>试样大小厚度和质量</li>
<li>避免产生择优取向</li>
</ul>
<h4 id="衍射全图的获得"><a href="#衍射全图的获得" class="headerlink" title="衍射全图的获得"></a>衍射全图的获得</h4><h4 id="单峰测试"><a href="#单峰测试" class="headerlink" title="单峰测试"></a>单峰测试</h4><h4 id="衍射信息的获取"><a href="#衍射信息的获取" class="headerlink" title="衍射信息的获取"></a>衍射信息的获取</h4><h4 id="衍射线的线形分析"><a href="#衍射线的线形分析" class="headerlink" title="衍射线的线形分析"></a>衍射线的线形分析</h4><h2 id="单晶体衍射信息的获取方法"><a href="#单晶体衍射信息的获取方法" class="headerlink" title="单晶体衍射信息的获取方法"></a>单晶体衍射信息的获取方法</h2><h3 id="劳埃法"><a href="#劳埃法" class="headerlink" title="劳埃法"></a>劳埃法</h3><ul>
<li>特征：使用连续的X射线谱照射固定不动的单晶体</li>
</ul>
<h2 id="X射线衍射的应用"><a href="#X射线衍射的应用" class="headerlink" title="X射线衍射的应用"></a>X射线衍射的应用</h2><h3 id="物相分析"><a href="#物相分析" class="headerlink" title="物相分析"></a>物相分析</h3><h4 id="定性相分析"><a href="#定性相分析" class="headerlink" title="定性相分析"></a>定性相分析</h4><h4 id="定量相分析"><a href="#定量相分析" class="headerlink" title="定量相分析"></a>定量相分析</h4><ul>
<li><p>PDF卡片</p>
</li>
<li><p>基础：物质的衍射强度与物质参与衍射的体积成正比<br>$$<br>I&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_0^2}\left(\frac{e^2}{m c^2}\right)^2 \frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P V \mathrm{e}^{-2 M} A(\theta)<br>$$</p>
<ul>
<li>对于粉末衍射仪法，$A(\theta)&#x3D;1&#x2F;2\mu$</li>
</ul>
</li>
<li><p>举例来说，两相体系下<br>$$<br>\begin{aligned}<br>&amp; I_\alpha&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_\alpha^2}\left(\frac{e^2}{m c^2}\right)^2\left(\frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P \mathrm{e}^{-2 M}\right)_\alpha \frac{V_\alpha}{2 \mu} \<br>&amp; I_\beta&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_\beta^2}\left(\frac{e^2}{m c^2}\right)^2\left(\frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P \mathrm{e}^{-2 M}\right)_\beta \frac{V_\beta}{2 \mu}<br>\end{aligned}<br>$$</p>
<ul>
<li>$\mu$ 是混合物的线吸收系数</li>
<li>前面的部分二者都相同，所以只和$V$有关系</li>
<li>$\mu ^*$是质量吸收系数，为$\mu &#x2F; \rho$</li>
<li>不管什么方法，$\alpha$相衍射强度 $I_{\alpha}&#x3D;K\frac{C_{\alpha}}{\mu}$</li>
</ul>
</li>
<li><p>外标法</p>
<ul>
<li>本质上是<br>  $$<br>  I_\alpha&#x3D;K \frac{w_a}{\rho_\alpha \mu^*}<br>  $$</li>
<li>上式的$\mu ^*$是总质量吸收系数</li>
<li>而对于纯 $\alpha$ 相的试样, 即标样, 其同指数衍射线的强度应为<br>  $$<br>  I_{\alpha_0}&#x3D;K \frac{1}{\rho_\alpha \mu_a^*}<br>  $$</li>
<li>从而, 待测试样中 $\alpha$ 相的衍射强度与 $\alpha$ 相标样的衍射强度的比值为</li>
</ul>
<p>  $$<br>  \frac{I_a}{I_{\alpha_0}}&#x3D;\frac{\mu_a^*}{\mu^*} w_a<br>  $$</p>
<ul>
<li>如果试样仅由 $\alpha 、 \beta$ 两相组成, 则上式可以写成</li>
</ul>
<p>  $$<br>  \frac{I_\alpha}{I_{\alpha_0}}&#x3D;\frac{w_\alpha \mu_\alpha^*}{w_\alpha\left(\mu_\alpha^*-\mu_\beta^<em>\right)+\mu_\beta^</em>}<br>  $$</p>
<ul>
<li>实际使用时，首先要知道是什么和什么的混合，找到工作曲线，之后通过两次实验测定$\frac{I_\alpha}{I_{\alpha_0}}$，在工作表上对应找即知道物质百分比</li>
</ul>
</li>
<li><p>内标法</p>
<ul>
<li>内标法首先向试样中加入标准物，然后让待测相与标准相进行比较，相当于通过加入标准相的方法，知道了$\omega _{s}$ </li>
<li>待测相衍射线强度与标准相衍射线强度之比可以表述为</li>
<li>$$\frac{I^{‘}<em>{a}}{I</em>{s}}&#x3D;K\frac{\omega^{‘}<em>{a}}{\omega</em>{s}}$$</li>
<li>设法得到$K$值即可</li>
</ul>
</li>
</ul>
<h4 id="精确测定点阵常数"><a href="#精确测定点阵常数" class="headerlink" title="精确测定点阵常数"></a>精确测定点阵常数</h4><h2 id="宏观应力分析"><a href="#宏观应力分析" class="headerlink" title="宏观应力分析"></a>宏观应力分析</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>只要有应力存在就会有应变，，也即晶面间距的变化。X光可以很好地测面间距的变化，所以可以用来测应力。</p>
<h4 id="应力应变关系"><a href="#应力应变关系" class="headerlink" title="应力应变关系"></a>应力应变关系</h4><ul>
<li><p>考虑受轴向拉力$F_x$的截面为A的棒</p>
<ul>
<li>胡克定律指出$\epsilon_x &#x3D; \frac{\sigma_x}{E}$</li>
<li>轴向拉伸，垂直于轴的面也会发生形变</li>
<li>$$-\epsilon_y &#x3D; -\epsilon_z &#x3D; \nu \epsilon_x &#x3D; \frac{\nu \sigma_x}{E}$$<ul>
<li>其中$\nu$是泊松比</li>
</ul>
</li>
</ul>
</li>
<li><p>考虑微变形情况下，主应力和主应变的广义胡克定律为</p>
<ul>
<li>$\epsilon_1 &#x3D; \frac{1}{E}[\sigma_1 - \nu(\sigma_2+\sigma_3)]$<ul>
<li>（$cyc$）</li>
</ul>
</li>
</ul>
</li>
<li><p>任意一个方向（注意应力是一个张量）</p>
<p>$$ \left{\begin{array}{l}<br>\sigma_{\phi \psi}&#x3D;\alpha_1^2 \sigma_1+\alpha_2^2 \sigma_2+\alpha_3^2 \sigma_3 \<br>\varepsilon_{\phi \psi}&#x3D;\alpha_1^2 \varepsilon_1+\alpha_2^2 \varepsilon_2+\alpha_3^2 \varepsilon_3<br>\end{array}\right.$$</p>
</li>
<li><p>其中$$\begin{aligned}<br>&amp; \alpha_1&#x3D;\sin \psi \cos \phi \<br>&amp; \alpha_2&#x3D;\sin \psi \sin \phi \<br>&amp; \alpha_3&#x3D;\cos \psi<br>\end{aligned} $$</p>
</li>
<li><p>广义胡克定律<br>$$\left{\begin{array}{l}\varepsilon_1&#x3D;\frac{1}{E}\left[\sigma_1-\nu\left(\sigma_2+\sigma_3\right)\right] \ \varepsilon_2&#x3D;\frac{1}{E}\left[\sigma_2-\nu\left(\sigma_1+\sigma_3\right)\right] \ \varepsilon_3&#x3D;\frac{1}{E}\left[\sigma_3-\nu\left(\sigma_1+\sigma_2\right)\right]\end{array}\right.$$</p>
</li>
</ul>
<h4 id="X射线衍射方法测定应力的原理"><a href="#X射线衍射方法测定应力的原理" class="headerlink" title="X射线衍射方法测定应力的原理"></a>X射线衍射方法测定应力的原理</h4><p>对于一般的金属材料，X射线的穿透能力很低，所以仅能测定表面层的应力。又垂直于表面层的应力的总和为0.也即$\sigma_3 &#x3D; 0$</p>
<ul>
<li><p>广义胡克定律在此前提下简化为：</p>
</li>
<li><p>$$<br>\left{\begin{array}{l}<br>\varepsilon_1&#x3D;\frac{1}{E}\left(\sigma_1-\nu \sigma_2\right) \<br>\varepsilon_2&#x3D;\frac{1}{E}\left(\sigma_2-\nu \sigma_1\right) \<br>\varepsilon_3&#x3D;-\frac{\nu}{E}\left(\sigma_1+\sigma_2\right)<br>\end{array}\right.<br>$$</p>
</li>
<li><p>带入$\varepsilon_{\phi \psi}&#x3D;\alpha_1^2 \varepsilon_1+\alpha_2^2 \varepsilon_2+\alpha_3^2 \varepsilon_3$化简即得到：</p>
</li>
<li><p>$$<br>\varepsilon_{\phi \psi}&#x3D;\frac{1+\nu}{E} \sigma_\phi \sin ^2 \psi-\frac{\nu}{E}\left(\sigma_1+\sigma_2\right)<br>$$</p>
<ul>
<li>注意$\sigma_\phi &#x3D; cos^2\phi \sigma_1 +sin^2 \phi \sigma_2$</li>
</ul>
<p>将上式对 $\sin ^2 \psi$ 求导, 就可以解出表面上任一方向上的应力 $\sigma_\phi$, 有：<br>$$<br>\sigma_\phi&#x3D;\frac{E}{1+\nu} \frac{\partial \varepsilon_{\phi \psi}}{\partial \sin ^2 \psi}<br>$$</p>
</li>
</ul>
<h3 id="衍射仪法测定宏观应力"><a href="#衍射仪法测定宏观应力" class="headerlink" title="衍射仪法测定宏观应力"></a>衍射仪法测定宏观应力</h3><h2 id="微晶尺寸和微观应力"><a href="#微晶尺寸和微观应力" class="headerlink" title="微晶尺寸和微观应力"></a>微晶尺寸和微观应力</h2><h4 id="微晶尺寸的测定"><a href="#微晶尺寸的测定" class="headerlink" title="微晶尺寸的测定"></a>微晶尺寸的测定</h4><h4 id="微观应力的测定"><a href="#微观应力的测定" class="headerlink" title="微观应力的测定"></a>微观应力的测定</h4><h2 id="织构的测定"><a href="#织构的测定" class="headerlink" title="织构的测定"></a>织构的测定</h2><h3 id="织构的分类"><a href="#织构的分类" class="headerlink" title="织构的分类"></a>织构的分类</h3><ul>
<li>丝织构<ul>
<li>丝织构的晶体学特点是晶粒的某一个或者某几个晶向倾向于平行试样的某一特定方向，一般沿丝轴方向或生长方向</li>
</ul>
</li>
<li>板织构<ul>
<li>板织构的晶体学特征是各晶粒的某一个或几个晶面平行于试样的某一特定面(如轧面),一个或几个晶向平行于试样的某一特定方向(如轧向)。</li>
</ul>
</li>
</ul>
<h3 id="织构的表示方法"><a href="#织构的表示方法" class="headerlink" title="织构的表示方法"></a>织构的表示方法</h3><ul>
<li>正级图</li>
<li>反级图</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>one-hot编码向量化及还原</title>
    <url>/2023/09/12/du-re-bian-ma-xiang-liang-hua-ji-biao-qian-huan-yuan/</url>
    <content><![CDATA[<p>打kaggle遇到的一些细节</p>
<span id="more"></span>

<ul>
<li><p>有时候需要将特征中的文本等进行独热编码处理，但是pd.get_dummies函数有时会默认为bool，不便转为torch.tensor。这时候可以选择指定为uint8，这样独热编码就是0、1之类的了。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = pd.get_dummies(all_features,dummy_na=<span class="literal">True</span>, dtype=<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">trainset = torch.tensor(all_features[:<span class="number">1235</span>].values,dtype=torch.float32).cuda()</span><br></pre></td></tr></table></figure>
</li>
<li><p>分类问题最后的标签独热编码之后可能需要还原以便提交。</p>
<p>此时就可以采用以下做法，提取出每行值为1的列（经过了softmax），之后再通过预先定义的map映射回分类指标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred=model(testset)</span><br><span class="line">prediction = []</span><br><span class="line">class_indices = torch.argmax(pred, dim=<span class="number">1</span>)</span><br><span class="line">class_mapping = &#123;<span class="number">0</span>: <span class="string">&#x27;died&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;euthanized&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;lived&#x27;</span>&#125;</span><br><span class="line">prediction = [class_mapping[i.item()] <span class="keyword">for</span> i <span class="keyword">in</span> class_indices]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Tricks</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习</title>
    <url>/2023/09/19/shen-du-xue-xi/</url>
    <content><![CDATA[<p>正则项目的是减小参数以进一步减少噪声的放大，其并不作用于b上，因为b不与x相乘，不会放大噪声。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><h3 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a>Convolutional Layer</h3><ul>
<li>Three shapes of convolution<ul>
<li>valid即全程完整的$f$和$g$进行卷积</li>
<li>full即从一格开始进行卷积</li>
<li>same即通过两侧$f$补零，使得卷积结果和$f$一样长</li>
</ul>
</li>
<li>卷积即$f$与$\tilde{g}$ 的相似度（标准长度）</li>
<li>1-D和2-D都是旋转180°再计算相似度即可</li>
<li>局部链接、权值共享<ul>
<li>举例来说，32*32的</li>
</ul>
</li>
</ul>
<h3 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>现代生物学导论</title>
    <url>/2023/09/19/xian-dai-sheng-wu-xue-dao-lun/</url>
    <content><![CDATA[<span id="more"></span>
<h2 id="DNA与蛋白质的结构与功能"><a href="#DNA与蛋白质的结构与功能" class="headerlink" title="DNA与蛋白质的结构与功能"></a>DNA与蛋白质的结构与功能</h2><h3 id="生物系统的弱化学作用"><a href="#生物系统的弱化学作用" class="headerlink" title="生物系统的弱化学作用"></a>生物系统的弱化学作用</h3><ul>
<li>强弱化学作用<ul>
<li>强化学作用<ul>
<li>生理温度下稳定</li>
<li>包括所有的化学键</li>
</ul>
</li>
<li>弱化学作用<ul>
<li>生物体内不断生成和断裂</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="DNA的结构和功能"><a href="#DNA的结构和功能" class="headerlink" title="DNA的结构和功能"></a>DNA的结构和功能</h3><ul>
<li>DNA的化学组成<ul>
<li>磷酸、五碳糖、含氮碱基</li>
</ul>
</li>
<li>DNA的功能<ul>
<li>多样性、存储遗传信息、可复制、稳定</li>
</ul>
</li>
<li>DNA的一级结构（存储遗传信息）<ul>
<li>多聚核苷酸单链</li>
<li>一级结构的应用<ul>
<li>寻找生物间演化关系<ul>
<li>人类（23对）、黑猩猩（及后均为24对）、大猩猩、红毛猩猩</li>
<li>假说：人类染色体在演化过程中发生了融合<ul>
<li>可能更长</li>
<li>可能含有不止一处着丝粒、端粒</li>
<li>比较序列等</li>
<li>……</li>
</ul>
</li>
</ul>
</li>
<li>DNA指纹鉴定</li>
</ul>
</li>
</ul>
</li>
<li>DNA的二级结构</li>
<li>DNA的三级结构</li>
</ul>
<h3 id="蛋白质的结构与功能"><a href="#蛋白质的结构与功能" class="headerlink" title="蛋白质的结构与功能"></a>蛋白质的结构与功能</h3><ul>
<li>蛋白质的四级结构</li>
<li>蛋白质功能</li>
<li>蛋白质折叠</li>
<li>分子伴侣蛋白<ul>
<li>hsp60工作机制</li>
</ul>
</li>
<li>蛋白质折叠与疾病</li>
<li>结构与功能的关系<ul>
<li>例如：DNA聚合酶</li>
</ul>
</li>
</ul>
<h2 id="细胞和癌"><a href="#细胞和癌" class="headerlink" title="细胞和癌"></a>细胞和癌</h2><h3 id="细胞通信"><a href="#细胞通信" class="headerlink" title="细胞通信"></a>细胞通信</h3><p>（单细胞生物之间存在细胞通讯）</p>
<h4 id="细胞通讯的种类"><a href="#细胞通讯的种类" class="headerlink" title="细胞通讯的种类"></a>细胞通讯的种类</h4><ul>
<li>旁分泌信号</li>
<li>内分泌信号</li>
</ul>
<h4 id="细胞通信的过程"><a href="#细胞通信的过程" class="headerlink" title="细胞通信的过程"></a>细胞通信的过程</h4><ul>
<li>信号接受<ul>
<li>信号分子</li>
<li>受体<ul>
<li>细胞表面受体<ul>
<li>G蛋白耦合受体</li>
<li>受体络氨酸激酶</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>信号转导<ul>
<li>磷酸化级联</li>
<li>第二信使<ul>
<li>非蛋白类小分子</li>
<li>cAMP环腺苷酸</li>
<li>cGMP环鸟苷酸</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="细胞周期"><a href="#细胞周期" class="headerlink" title="细胞周期"></a>细胞周期</h3><h4 id="调控细胞周期的分子机制"><a href="#调控细胞周期的分子机制" class="headerlink" title="调控细胞周期的分子机制"></a>调控细胞周期的分子机制</h4><ul>
<li>周期素和蛋白激酶</li>
<li>生长因子</li>
</ul>
<h3 id="细胞凋亡"><a href="#细胞凋亡" class="headerlink" title="细胞凋亡"></a>细胞凋亡</h3><ul>
<li>凋亡过程<ul>
<li>细胞收缩、染色体凝结</li>
<li>膜起泡</li>
<li>细胞核塌陷</li>
<li>凋亡体形成</li>
<li>凋亡小体裂解</li>
</ul>
</li>
<li>三种信号通路<ul>
<li>线粒体通路</li>
<li>死亡受体通路</li>
<li>内质网通路</li>
</ul>
</li>
</ul>
<h3 id="细胞自噬"><a href="#细胞自噬" class="headerlink" title="细胞自噬"></a>细胞自噬</h3><ul>
<li>自噬过程<ul>
<li>自噬泡形成</li>
<li>自噬体形成</li>
<li>自噬体和溶酶体融合</li>
<li>自噬溶酶体形成</li>
<li>降解和再吸收</li>
</ul>
</li>
<li>细胞自噬与疾病</li>
</ul>
<h3 id="液-液相分离"><a href="#液-液相分离" class="headerlink" title="液-液相分离"></a>液-液相分离</h3><ul>
<li>无膜细胞器</li>
</ul>
<h4 id="DNA和基因"><a href="#DNA和基因" class="headerlink" title="DNA和基因"></a>DNA和基因</h4><ul>
<li>基因<ul>
<li>控制性状</li>
</ul>
</li>
<li>基因的构成<ul>
<li>调控区域<ul>
<li>指导基因是否能够产生蛋白</li>
</ul>
</li>
<li>启动子<ul>
<li>和转录酶结合的序列</li>
</ul>
</li>
<li>编码区域</li>
<li>断裂基因<ul>
<li>细菌基因：连续</li>
<li>动物基因：断裂</li>
</ul>
</li>
<li>原核基因</li>
<li>真核基因<ul>
<li>产生蛋白质的序列<ul>
<li>内含子&amp;外显子</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>染色质与染色体<ul>
<li>从DNA到染色质</li>
</ul>
</li>
<li>遗传物质保证稳定机制<ul>
<li>DNA精确复制<ul>
<li>碱基配对</li>
<li>DNA聚合酶的校正</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="遗传密码的特点"><a href="#遗传密码的特点" class="headerlink" title="遗传密码的特点"></a>遗传密码的特点</h3><ul>
<li>多个遗传密码对应同一个氨基酸</li>
</ul>
<h4 id="调控从基因到蛋白质的过程"><a href="#调控从基因到蛋白质的过程" class="headerlink" title="调控从基因到蛋白质的过程"></a>调控从基因到蛋白质的过程</h4><ul>
<li>对环境刺激做出应答</li>
<li>发育</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习期中CheckSheet</title>
    <url>/2023/11/01/ji-qi-xue-xi-qi-zhong-check-sheet/</url>
    <content><![CDATA[<p>机器学习期中复习提纲（### Lecture1-### Lecture5）</p>
<p>注：课件中所有红色部分需要认真复习！</p>
<h3 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h3><ol>
<li><p>霍夫丁不等式（Hoeffding inequality）含义，与欠拟合和过拟合的关联</p>
</li>
<li><p>In-sample error 和 out-of-sample error 概念</p>
</li>
<li><p>复杂度（complexity）和数据规模对机器学习效果的影响</p>
</li>
<li><p>线性回归（Linear regression）</p>
</li>
</ol>
<ul>
<li><p>定义（自变量，因变量，公式，参数 β，残差 residual，成本函数 cost function）</p>
</li>
<li><p>假设（弱外生性、线性、方差齐性）</p>
</li>
<li><p>最小二乘法（OLS）计算验证过程</p>
</li>
<li><p>判定系数（coefficient of determination）定义</p>
</li>
</ul>
<ol start="5">
<li>多元线性回归（Multiple linear regression）</li>
</ol>
<ul>
<li><p>概念和假设（新增假设 No perfect multicollinearity）</p>
</li>
<li><p>β̂的解的表达</p>
</li>
</ul>
<ol start="6">
<li>极大似然估计（Maximum likelihood estimation）</li>
</ol>
<ul>
<li><p>概念和假设</p>
</li>
<li><p>目标函数</p>
</li>
<li><p>多元线性回归问题中β̂和𝜎</p>
</li>
</ul>
<p>̂ 2的解的表达</p>
<ol start="7">
<li>梯度下降（Gradient desent）</li>
</ol>
<ul>
<li><p>目标、具体实现流程</p>
</li>
<li><p>参数更新方式</p>
</li>
<li><p>步长（learning step）以及其影响</p>
</li>
</ul>
<h3 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h3><ol>
<li>线搜索（Line search）</li>
</ol>
<ul>
<li><p>目标和具体实现流程</p>
</li>
<li><p>优化步长的方法</p>
</li>
</ul>
<ol start="2">
<li>牛顿法（Newton’s method）</li>
</ol>
<ul>
<li><p>目标和具体实现流程</p>
</li>
<li><p>参数更新方式（一维和多维）</p>
</li>
</ul>
<ol start="3">
<li>随机梯度下降（Stochastic gradient descent）</li>
</ol>
<ul>
<li><p>目标和具体实现流程</p>
</li>
<li><p>参数更新方式</p>
</li>
</ul>
<ol start="4">
<li>小批量梯度下降（Mini-batch gradient descent）</li>
</ol>
<ul>
<li><p>目标和具体实现流程</p>
</li>
<li><p>参数更新方式</p>
</li>
</ul>
<ol start="5">
<li>动量梯度下降法 &amp; Nesterov 加速梯度下降法（Gradient descent with momentum &amp;</li>
</ol>
<p>Nesterov’s accelerated Gradient descent）</p>
<ul>
<li>了解基础概念</li>
</ul>
<ol start="6">
<li><p>AdaGrad&#x2F;RMSprop&#x2F;Adam-  了解上述三种方法的特点</p>
</li>
<li><p>基函数回归（Basis function regression）</p>
</li>
</ol>
<ul>
<li>定义（自变量，因变量，公式，参数，成本函数 cost function）</li>
</ul>
<ol start="8">
<li><p>归纳偏置（Inductive bias）概念</p>
</li>
<li><p>No Free lunch theorem 概念</p>
</li>
<li><p>欠拟合和过拟合（Overfitting and underfitting）概念和过拟合原因.</p>
</li>
<li><p>训练损失和测试损失（Training loss &amp; testing loss）概念和变化原因</p>
</li>
<li><p>偏差和方差（Bias and variance）</p>
</li>
</ol>
<ul>
<li><p>了解 bias-variance trade-off（权衡）</p>
</li>
<li><p>No free lunch 了解含义</p>
</li>
</ul>
<ol start="13">
<li>交叉验证（Cross-validation）</li>
</ol>
<ul>
<li><p>了解基础概念</p>
</li>
<li><p>N-fold cross-validation 具体流程</p>
</li>
</ul>
<h3 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h3><ol>
<li>正则化（Regularization）</li>
</ol>
<ul>
<li><p>正则化的目的</p>
</li>
<li><p>Ridge regression 的 cost function 和梯度更新过程</p>
</li>
<li><p>Lasso 相关概念</p>
</li>
</ul>
<ol start="2">
<li>逻辑回归（Logistic Regression）</li>
</ol>
<ul>
<li><p>定义（自变量，因变量，参数 β，sigmoid 函数，似然函数 Likelihood function）</p>
</li>
<li><p>随机梯度下降法&#x2F;梯度下降法(stochastic gradient descent&#x2F;gradient descent)</p>
</li>
</ul>
<p>具体实现流程和下降公式</p>
<ol start="3">
<li>感知机(Perceptron)</li>
</ol>
<ul>
<li>了解基本概念</li>
</ul>
<ol start="4">
<li>Gaussian discriminant analysis</li>
</ol>
<ul>
<li>了解高斯判别整体流程</li>
</ul>
<h3 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h3><ol>
<li>决策树（Decision tree）</li>
</ol>
<ul>
<li><p>基本概念（理清课件上的例子和 code 中例子）和具体实现流程</p>
</li>
<li><p>拆分标准（Splitting criteria）</p>
</li>
<li><p>分类错误（classification error）的几种度量标准（metrics）</p>
</li>
<li><p>终止条件（Stopping condition）</p>
</li>
<li><p>Using decision tree for regression 了解基本概念</p>
</li>
</ul>
<ol start="2">
<li><p>Regression tree 了解基本概念</p>
</li>
<li><p>Bagging</p>
</li>
</ol>
<ul>
<li><p>了解基本概念</p>
</li>
<li><p>优势（Advantages）和局限（Limitation）</p>
</li>
</ul>
<ol start="4">
<li>随机森林（Random forest）</li>
</ol>
<ul>
<li><p>了解基本概念和实现流程</p>
</li>
<li><p>调整随机森林的方法5. Boosting</p>
</li>
<li><p>了解基本概念和实现流程</p>
</li>
</ul>
<ol start="6">
<li>Gradient Boosting</li>
</ol>
<ul>
<li>了解基本概念</li>
</ul>
<ol start="7">
<li>KNN（K-nearest neighborhood）</li>
</ol>
<ul>
<li><p>基本概念和具体实现流程</p>
</li>
<li><p>Parzen Windows and soft boundary 了解概念</p>
</li>
<li><p>了解 K-D tree 和 LSH 的目的</p>
</li>
</ul>
<h3 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h3><ol>
<li>K-means</li>
</ol>
<ul>
<li>概念和具体实现流程</li>
</ul>
<ol start="2">
<li>分层聚类（Hierarchical clustering）</li>
</ol>
<ul>
<li><p>了解基本概念和实现流程</p>
</li>
<li><p>组间距离定义</p>
</li>
</ul>
<ol start="3">
<li>BFR</li>
</ol>
<ul>
<li><p>了解基本概念和实现流程</p>
</li>
<li><p>Meta-data 的使用</p>
</li>
</ul>
<ol start="4">
<li>Mixture of Gaussians</li>
</ol>
<ul>
<li><p>隐藏变量和正态假设的概念</p>
</li>
<li><p>似然函数</p>
</li>
<li><p>能简述 EM 算法流程</p>
</li>
<li><p>Jensen’s inequality</p>
</li>
</ul>
<ol start="5">
<li>PCA</li>
</ol>
<ul>
<li><p>算法目的</p>
</li>
<li><p>预处理流程</p>
</li>
<li><p>具体实现</p>
</li>
</ul>
<p>别的注意点：</p>
<ol>
<li><p>非参数方法和含参数方法的区别</p>
</li>
<li><p>监督学习和无监督学习的区别</p>
</li>
<li><p>需要具备书写伪代码的能力</p>
</li>
<li><p>在算法实现过程中哪些因素会影响最终结果</p>
</li>
</ol>
<h3 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h3><ol>
<li>Principle components analysis <ul>
<li>算法目的 </li>
<li>具体流程</li>
<li>与 eigenvalue eigenvector 的关系</li>
</ul>
</li>
<li>Support vector machine <ul>
<li>算法目的 </li>
<li>基本思路 </li>
<li>目标、约束、决策边界</li>
</ul>
</li>
<li>Learning with Probabilistic Graphic Model <ul>
<li>Independent &amp; Conditional independent</li>
</ul>
</li>
<li>Hidden Markov chain (lecture 7) <ul>
<li>Forward-backward algorithm</li>
</ul>
</li>
</ol>
<h3 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h3><ol>
<li>Markov Decision process </li>
<li>Bellman Expectation equation </li>
<li>Bellman Optimality equation </li>
<li>General policy iteration <ol>
<li>Policy iteration </li>
<li>Value iteration</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo git问题</title>
    <url>/2023/11/01/jie-jue-git-wen-ti-failed-to-connect-to-127.0.0.1-port-7890-after-2070-ms-connection-refused/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global http.proxy</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tricks</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>计划</title>
    <url>/2023/09/18/ji-hua/</url>
    <content><![CDATA[<ul>
<li>21:00 跑步</li>
<li>21:30 - 22:10视唱练耳+弹琴</li>
<li>22:10 - 22:50画画</li>
<li>22:50 - 23:00收拾收拾准备睡觉</li>
<li>（找时间复习！！！）</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>Azusa</title>
    <url>/2023/05/22/qing-yin-shao-nu/</url>
    <content><![CDATA[<p>阿梓喵！</p>
<p><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/azusa.jpg" alt="azusa"></p>
]]></content>
      <categories>
        <category>けいおん！</category>
      </categories>
      <tags>
        <tag>他乡</tag>
      </tags>
  </entry>
  <entry>
    <title>量子与统计</title>
    <url>/2023/10/28/liang-zi-yu-tong-ji/</url>
    <content><![CDATA[<h3 id="第三章习题"><a href="#第三章习题" class="headerlink" title="第三章习题"></a>第三章习题</h3><ul>
<li>有关氢原子的计算<ul>
<li>半径的均值往往在球坐标系下计算<ul>
<li>注意变换坐标系的相关细节</li>
</ul>
</li>
<li>最概然半径，也即氢原子电子出现概率最大的半径值，可以通过计算不同$r$处的概率密度得到，注意$w(r)&#x3D;R^2(r)r^2$ </li>
<li>动能：利用动能算符转化</li>
</ul>
</li>
<li>有关经典物理中的计算<ul>
<li>将能量表示式转化为算符</li>
</ul>
</li>
<li>从态函数中读出信息<ul>
<li>$\psi(r, \theta, \psi)&#x3D;\frac{1}{2} R_{21}(r) Y_{10}(\theta, \varphi)-\frac{\sqrt{3}}{2} R_{21}(r) Y_{1-1}(\theta, \varphi)$<ul>
<li>$R_{nl},Y_{lm}$通过表达式确定$n、l、m$三个参数</li>
<li>$n$可以确定$E$</li>
</ul>
</li>
</ul>
</li>
<li>![[Pasted image 20231031184458.png]]</li>
</ul>
<h2 id="量子力学的矩阵形式与表象理论"><a href="#量子力学的矩阵形式与表象理论" class="headerlink" title="量子力学的矩阵形式与表象理论"></a>量子力学的矩阵形式与表象理论</h2><h3 id="Q表象"><a href="#Q表象" class="headerlink" title="Q表象"></a>Q表象</h3><ul>
<li>将某态转变为力学量Q表象下的表示<ul>
<li>$\hat{Q}\mu_i&#x3D;q_i\mu_i$，${ \mu_{i} }$是其本征态函数，构成态空间正交归一完备的基矢</li>
<li>将$\Psi$向本征函数系${ \mu_i,… }$上投影，得系数$a_n(t)$<ul>
<li>而一组系数构成的向量，就是$\hat{Q}$表象下的态矢量</li>
</ul>
</li>
<li>$|a_n|^2$就是在$\Psi(x,t)$所描写的态中测量力学量$\hat{Q}$所得结果为$q_n$的概率</li>
<li>对连续谱和多自由度也成立</li>
</ul>
</li>
<li>考虑某力学量表象$Q’$ <ul>
<li>同一个量子态$\Psi$也可用$Q’$的本征态展开，同样作投影得一组表示$(q_n^{‘})$</li>
</ul>
</li>
<li>一个领悟<ul>
<li>“波函数的模方代表在某处找到粒子概率”仅是在坐标表象下成立的表述。原因是，事实上这个东西的模方代表的是位于$x\rangle$上的概率。其实与$\hat{Q}$表象下的${a_n}$没什么区别，只不过离散的时候是累加，而连续的时候是积分</li>
<li>所以说，${a_n}$也可以视作$\hat{Q}$表象下的波函数</li>
</ul>
</li>
</ul>
<h3 id="算符的矩阵表示"><a href="#算符的矩阵表示" class="headerlink" title="算符的矩阵表示"></a>算符的矩阵表示</h3><ul>
<li>坐标表象下，算符一般可表示为$\hat{F}(x,-i\hbar\frac{\partial}{\partial x})$ </li>
<li>表示力学量$F$的矩阵是厄米矩阵</li>
<li>力学量在自身表象中为对角的厄米矩阵</li>
</ul>
<h3 id="量子力学的矩阵形式"><a href="#量子力学的矩阵形式" class="headerlink" title="量子力学的矩阵形式"></a>量子力学的矩阵形式</h3><h3 id="Dirac-符号"><a href="#Dirac-符号" class="headerlink" title="Dirac 符号"></a>Dirac 符号</h3><ul>
<li>右矢与左矢</li>
<li>标积<ul>
<li>$\langle \psi|\phi \rangle$</li>
</ul>
</li>
</ul>
<h2 id="第四章习题"><a href="#第四章习题" class="headerlink" title="第四章习题"></a>第四章习题</h2><h3 id="课堂习题"><a href="#课堂习题" class="headerlink" title="课堂习题"></a>课堂习题</h3><ul>
<li>$\hat{A}^2&#x3D;1$求本征值及矩阵<ul>
<li>分别作用于某波函数即可</li>
</ul>
</li>
<li>$\hat{A}^2&#x3D;\hat{A}$求本征值及矩阵<ul>
<li>方法同上</li>
</ul>
</li>
<li>坐标与动量表象下算符的矩阵元</li>
<li>粒子以某态在一维无限深势阱中运动，求该态在能量表象下的矩阵表示（态变换）</li>
<li>一维谐振子的坐标、动量以及H在能量表象中的表示（算符变换）</li>
</ul>
<h3 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h3><h1 id="自旋与全同粒子体系"><a href="#自旋与全同粒子体系" class="headerlink" title="自旋与全同粒子体系"></a>自旋与全同粒子体系</h1><h3 id="Stern-Gerlach实验"><a href="#Stern-Gerlach实验" class="headerlink" title="Stern-Gerlach实验"></a>Stern-Gerlach实验</h3><ul>
<li>银原子最外层价电子在$5s$轨道上，由$L&#x3D;\sqrt{l(l+1)\hbar}$，知银原子基态轨道磁矩为$0$</li>
<li>但是实验结果表明</li>
</ul>
<h3 id="电子自旋假设"><a href="#电子自旋假设" class="headerlink" title="电子自旋假设"></a>电子自旋假设</h3><h3 id="课堂习题-1"><a href="#课堂习题-1" class="headerlink" title="课堂习题"></a>课堂习题</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>鹊踏枝</title>
    <url>/2022/12/24/que-ta-zhi/</url>
    <content><![CDATA[<p>翻过五代词的卷首语，就仿佛翻过了整整一个时代——告别了行吟于诗国晚辉中的杜牧、李商隐，一种全新的体裁在一片全新的时代土壤上展苞待放。</p>
<p>就好像一个四处漂泊的游子，他游天姥，攀蜀道，下江陵；见过了大漠孤烟，长河落日，星垂平野，月涌江流；梦过了渌水荡漾，天青云淡，白鹿青崖，广厦万间，忽地停下了脚步。</p>
<p>他取来了镜子，镜子不是湖面，镜子里，只有他一人。</p>
<p>他看到镜中数百年的流浪给自己留下的痕迹，深深浅浅。百年来的种种夹杂着繁复的心情，化作无数人影向他涌来：李将军，石壕吏；卖炭翁，琵琶女，秦妃王子浣纱人，征人思妇一书生……他拨开重叠的幻境，人影憧憧，镜中人只有自己。</p>
<p>他于是知道，像风吹过了三千里，他写尽天下而未曾落笔之处，是自己。</p>
<p>一曲《鹊踏枝》，不必有“君王掩面救不得”的悲恸，不必有“同是天涯沦落人”的怜与自怜。“闲愁”本不需要什么由头引子，若非要说，便是那宜人春色，而不再是欲回天地、欲入扁舟。</p>
<p>没有“欲”，愁是无名的愁——就像你愿意无缘由地凝视阳光下的一株小树。无理之处，实则有情而且真实得使人想落下眼泪。试想，在大唐盛世会有多少诗人揽镜自鉴自伤憔悴呢？他们的眼光是向外的，是天地苍茫。而到了南唐——仿佛曲终人散，镜子里，终于能够空到放得下一个自己，一个真实的自己。</p>
<p>我爱那“独立小桥风满袖”、“长笛一声人倚楼”。它们若结成联句，在我看来无异于唐与五代的桥梁——他们共同描绘了人的真实：风满袖，仿佛身上每一寸衣袂都被晚风鼓动；一声笛，又仿佛余音袅袅回环缭绕，敲打心弦而不绝……人在这样的情境下，是自我的，真实的，饱满的，也是孤独的。</p>
<p>人们怀揣孤独的共情，从盛唐走向晚唐，又从晚唐步入南唐……</p>
<p>体会到了何为自我，冯延巳的《鹊踏枝》在最后一句让“人”悄悄离去。</p>
<p>平林新月，有人叹“可惜不见人影”。</p>
<p>岂知“无妨不见人影”，乃至“可幸没有人影”！人在此中的“退”是心在此中的“生”！</p>
]]></content>
      <categories>
        <category>残章</category>
      </categories>
      <tags>
        <tag>残章</tag>
      </tags>
  </entry>
</search>
