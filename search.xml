<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ai For Materials 论文研读</title>
    <url>/2023/11/30/ai-for-materials-lun-wen-yan-du/</url>
    <content><![CDATA[<h3 id="Scaling-deep-learning-for-materials-discovery"><a href="#Scaling-deep-learning-for-materials-discovery" class="headerlink" title="Scaling deep learning for materials discovery"></a>Scaling deep learning for materials discovery</h3>]]></content>
      <categories>
        <category>论文研读</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Cheat Engine 告别风灵月影</title>
    <url>/2023/06/10/cheatengine/</url>
    <content><![CDATA[<span id="more"></span>
<p>最近玩Brotato，突然发现风灵月影居然没有相关的修改器。阴差阳错之下，学会了Cheat Engine的操作。（有一说一tutorial做的真挺好，就像黑客风格的游戏一样）<br>一般操作流程就是读取内存，确定参数对应的内存保存位置，修改之即可。这下再也不用担心网络上乱七八糟修改器的病毒问题，或者功能有所欠缺了。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title>HealthCare RL 论文研读</title>
    <url>/2023/11/22/healthcare-rl-lun-wen-yan-du/</url>
    <content><![CDATA[<h2 id="The-Artificial-Intelligence-Clinician-learns-optimal-treatment-strategies-for-sepsis-in-intensive-care"><a href="#The-Artificial-Intelligence-Clinician-learns-optimal-treatment-strategies-for-sepsis-in-intensive-care" class="headerlink" title="The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care"></a>The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care</h2><ul>
<li>精读链接</li>
<li>[[The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care]]</li>
</ul>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol>
<li><strong>脓毒症的严重性和治疗挑战</strong>：<ul>
<li>脓毒症是全球第三大致死原因，是医院中的主要死亡原因。然而，其最佳治疗策略仍不确定。目前的静脉输液和血管紧张素管理实践被认为是次优的，可能对一部分患者造成伤害。</li>
<li>脓毒症定义为严重感染导致的危及生命的急性器官功能障碍。目前没有工具能够个性化治疗脓毒症，辅助临床医生实时和针对患者层面做出决策。因此，脓毒症治疗的临床变异性极大，且有证据表明次优的决策会导致较差的结果。</li>
</ul>
</li>
<li><strong>人工智能医生模型的开发和验证</strong>：<ul>
<li>研究团队开发了一种名为“人工智能医生”的计算模型，使用强化学习动态地为重症监护病房中的成人脓毒症患者提出最佳治疗建议。该模型在美国两个大型、不重叠的重症监护数据库上进行了构建和验证，涵盖了广泛的患者数据和临床特征。</li>
</ul>
</li>
<li><strong>模型的临床应用和效果</strong>：<ul>
<li>在独立的验证队列中，接受人工智能医生建议治疗的患者死亡率最低。当临床医生的实际治疗与人工智能医生的建议策略相符时，常见的做法是使用过低剂量的血管紧张素。该系统可以实时使用，将不同数据流源的患者数据输入到配备该算法的电子健康记录软件中，以提出行动建议。</li>
</ul>
</li>
<li><strong>研究的局限性和未来展望</strong>：<ul>
<li>尽管使用了大型数据集，但由于数据质量问题，部分地点和患者被排除在外。两个数据集之间的差异导致使用了略有不同的脓毒症标准。此外，一些实验室值在决策时可能未立即提供给临床医生。该研究强调了未来需要在临床试验中使用实时数据和决策进行前瞻性评估，并在不同的医疗环境中进行测试。该研究还表明，仅通过微小比例降低脓毒症死亡率，就能在全球范围内每年挽救成千上万的生命。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>论文研读</category>
      </categories>
      <tags>
        <tag>HealthCare</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/25/cosmol-drug-release/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>HealthCare LLM 工作日志</title>
    <url>/2023/12/01/healthcare-llm-gong-zuo-ri-zhi/</url>
    <content><![CDATA[<h3 id="12-01"><a href="#12-01" class="headerlink" title="12-01"></a>12-01</h3><ul>
<li>数据分析<ul>
<li><strong>elixhauser</strong> - Elixhauser评分：一个用于评估患者合并症严重程度的医疗评分系统。</li>
<li><strong>re_admission</strong> - 重新入院：表示患者是否有重新入院的记录。</li>
<li><strong>died_in_hosp</strong> - 在医院内死亡：表示患者是否在医院内死亡。</li>
<li><strong>died_within_48h_of_out_time</strong> - 出院后48小时内死亡：表示患者是否在出院后48小时内死亡。</li>
<li><strong>mortality_90d</strong> - 90天内死亡率：表示患者在90天内死亡的情况。</li>
<li><strong>delay_end_of_record_and_discharge_or_death</strong> - 记录结束与出院或死亡的延迟：记录和出院或死亡之间的时间差。</li>
<li><strong>GCS</strong> - 格拉斯哥昏迷评分：评估患者意识水平的评分系统。</li>
</ul>
</li>
<li>代码研读<ul>
<li>可能的数据处理C:\Users\wenjie\Desktop\HealthCareLLM\SimMedEnv-master\SimMedEnv-master\HD4RL\utils\data.py</li>
<li>利用pandas读入csv，转为ReplayBuffer可以读取的格式，利用\utils\data.py</li>
<li>继承tianshou policy ,修改</li>
</ul>
</li>
<li>工作总结<ul>
<li>继承BasePolicy，撰写ChatGPTPolicy<ul>
<li>具体可参考Policy.py中的相关内容</li>
</ul>
</li>
<li>实例化OPE_Wapper</li>
</ul>
</li>
</ul>
<h3 id="12-02"><a href="#12-02" class="headerlink" title="12-02"></a>12-02</h3><ul>
<li>概念与定义<ul>
<li>OPE（Off-Policy Evaluation）<ul>
<li>一个简单的策略，可以是随机策略或者是初步的策略</li>
<li>简单策略与环境交互得到的经验回放数据集</li>
<li>一个需要评估的目标策略</li>
</ul>
</li>
</ul>
</li>
<li>踩坑<ul>
<li>有关类的定义和调用，听听copilot的会很好</li>
<li>notion的task居然是可以打开的，急急急</li>
</ul>
</li>
<li>知识点<br>在离线策略评估（Off-Policy Evaluation, OPE）的领域中，有多种方法用于评估一个策略的性能</li>
</ul>
<ol>
<li><strong>IS (Importance Sampling)</strong>:<ul>
<li>重要性采样（IS）是一种通过对历史数据进行重新加权来评估策略的方法。它计算目标策略和行为策略之间的重要性比率（即行为策略生成数据的概率与目标策略生成相同数据的概率之比）。然后，使用这些比率来加权回报，从而估算在目标策略下的期望回报。</li>
</ul>
</li>
<li><strong>FQE (Fitted Q Evaluation)</strong>:<ul>
<li>FQE 通过适配一种称为 Q 函数的值函数来评估策略。Q 函数估计在给定状态和动作下未来的累积奖励。FQE 通过迭代地适应历史数据来学习这个函数，并使用它来估计目标策略的性能。</li>
</ul>
</li>
<li><strong>WIS (Weighted Importance Sampling)</strong>:<ul>
<li>加权重要性采样（WIS）是 IS 的一个变体，它通过对每个样本的重要性比率进行归一化来改进普通的 IS。这种方法可以减少方差，并在某些情况下提供更准确的估计。</li>
</ul>
</li>
<li><strong>WIS_bootstrap</strong>:<ul>
<li>这是 WIS 方法的一种扩展，它使用自举（bootstrap）方法来估计误差和置信区间。通过重采样历史数据并多次应用 WIS，可以获得关于估计值稳定性的更多信息。</li>
</ul>
</li>
<li><strong>WIS_mortality</strong>:<ul>
<li>这是针对特定应用（如医疗领域中的死亡率预测）调整的 WIS 方法。它可能会在计算重要性比率时考虑特殊的生存或死亡情况，从而使得评估更适合于这类高风险的应用场景。</li>
</ul>
</li>
<li><strong>WIS_truncated</strong>:<ul>
<li>在这个变体中，重要性比率被截断或限制在某个最大值。这有助于减少由于极端重要性比率引起的高方差问题，特别是在目标策略和行为策略差异很大时。</li>
</ul>
</li>
<li><strong>WIS_bootstrap_truncated</strong>:<ul>
<li>这是 WIS_truncated 的一种扩展，结合了自举方法和截断技术。通过这种方式，它既能够减少极端重要性比率带来的不稳定性，又能提供关于估计值稳定性的附加信息。</li>
</ul>
</li>
</ol>
<h3 id="12-03"><a href="#12-03" class="headerlink" title="12-03"></a>12-03</h3><ul>
<li>总结<ul>
<li>破防了，这个VS Code的环境管理简直是狗屎</li>
<li>吐了，子数据集构建，太无语了，太无语了，太无语了</li>
</ul>
</li>
<li>经验与收获<ul>
<li>有关库代码撰写<ul>
<li>在一个统一的库中跨文件夹调用时，使用绝对路径，从最高级开始import，提高可复用性</li>
</ul>
</li>
<li>有关sys以及工作路径<ul>
<li>import sys <ul>
<li>print(sys.path)可以返回当前代码执行的工作路径</li>
<li>一般来说希望返回当前执行文件的根目录，这样可以依序寻找到所有所需的文件</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>代码相关细节<ul>
<li>Toy_Buffer构建<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">buffer = load_buffer(<span class="string">&quot;path&quot;</span>)</span><br><span class="line">n = <span class="number">10</span>  <span class="comment"># 患者数量</span></span><br><span class="line">end_indices = np.where(buffer.done == <span class="number">1</span>)[<span class="number">0</span>][:n]</span><br><span class="line">buffer_save_path = <span class="string">&quot;path&quot;</span></span><br><span class="line">all_batch = []</span><br><span class="line">start_index = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> end_index <span class="keyword">in</span> end_indices:</span><br><span class="line">    trajectory = buffer[start_index:end_index + <span class="number">1</span>]</span><br><span class="line">    trajectory = Batch.stack(trajectory)</span><br><span class="line">    all_batch.append(Batch.stack(trajectory))</span><br><span class="line">    start_index = end_index + <span class="number">1</span></span><br><span class="line">all_batch = Batch.cat(all_batch)</span><br><span class="line">replay_buffer = ReplayBuffer(size=<span class="built_in">len</span>(all_batch))</span><br><span class="line">replay_buffer.set_batch(all_batch)</span><br><span class="line">replay_buffer.save_hdf5(buffer_save_path)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="12-05"><a href="#12-05" class="headerlink" title="12-05"></a>12-05</h3><ul>
<li><p>工作内容</p>
<ul>
<li>调整了路径参数等的管理方式，采用config.json管理</li>
<li>增加了prompt回放功能，初步规范化了文件名、输出等</li>
</ul>
</li>
<li><p>参考代码（config管理）</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;HealthcareLLM\SimMedEnv\config.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> config_file:</span><br><span class="line">    config = json.load(config_file)</span><br><span class="line">buffer = load_buffer(config[<span class="string">&quot;buffer_path&quot;</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li><p>一般来说，对于.json文件，可以以key-value的形式保存数据</p>
</li>
<li><p>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;log_folder&quot;</span>: <span class="string">&quot;GPT_Conversation_log&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="12-06"><a href="#12-06" class="headerlink" title="12-06"></a>12-06</h3><ul>
<li>Code Review<ul>
<li>Config验证，关键字检查，Config路径输入init 可改<ul>
<li>最好改为config类</li>
</ul>
</li>
<li>VS code静态检查 PEP8</li>
<li><strong>init</strong> 增加data loader</li>
<li>learn就报错</li>
<li>补充正确Action</li>
<li>GPT 多轮对话</li>
<li>typing 包</li>
</ul>
</li>
</ul>
<h3 id="12-07"><a href="#12-07" class="headerlink" title="12-07"></a>12-07</h3><ul>
<li>收获<ul>
<li>将最新的git代码pull到本地 <ul>
<li>git pull即可</li>
</ul>
</li>
<li>指向问题出现的位置<ul>
<li>ctrl+点击</li>
</ul>
</li>
<li>VS Code感觉还是颇为不错的<ul>
<li>有关VS Code断点+表达式</li>
<li>所谓表达式也可以理解为变量的表达方式</li>
</ul>
</li>
</ul>
</li>
<li>Python知识<ul>
<li>&#96;&#96;&#96;*args参数<ul>
<li>用来处理在函数定义时未被命名的位置函数</li>
</ul>
</li>
<li>&#96;&#96;&#96;**kargs参数<ul>
<li>用来处理函数定义时，未被命名的多余关键字参数</li>
</ul>
</li>
<li>&#96;&#96;&#96;enumerate<ul>
<li>其返回包括index在内的tuple</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="12-09"><a href="#12-09" class="headerlink" title="12-09"></a>12-09</h3><ul>
<li><p>vscode使用</p>
<ul>
<li>有关路径的问题，终于可以说解决的差不多了</li>
<li>有关自己定义的包，跨文件夹引用从根目录往下就要开始采用点索引法</li>
</ul>
</li>
<li><p>有关git使用</p>
<ul>
<li><p>对库进行远程链接</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git submodule add &lt;SimMedEnv-repo-URL&gt; &lt;path-to-SimMedEnv&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于含有子模块的项目</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recurse-submodules &lt;your-repo-URL&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>HealthCare</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown实用语法</title>
    <url>/2023/06/13/markdown-yu-fa/</url>
    <content><![CDATA[<ul>
<li>最常用的是无序列表，对于强迫症来说很有用。语法是” - “(减号加空格)，加在每行开头即可。</li>
<li>还有一个比较有趣的是删除线符号，在想要删除的地方前后加双波浪线（~~ xxx ~~ ）即可，如<del>我想删除这些字</del> 当然，加了空格就可以不受影响。</li>
<li>有几个希腊字母和特殊符号还挺容易忘的，mark一下。<ul>
<li>\rho $\rho$ ，\pm $\pm$</li>
<li>\geq $\geq$  \leq $\leq$</li>
<li>\qquad \quad \ ; , !分别对应6种不同的字符间距</li>
</ul>
</li>
<li>字符顶上的特殊符号$\tilde{A}$  \tilde<ul>
<li>补充一个上面加点$\dot{x}$ dot<blockquote>
<p>如何引用呢？使用&gt;即可</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>学到量子力学以后，需要打狄拉克符号$\langle,\rangle$ langle 和rangle<br>新学了一个 $\vec{r}$  vec<br>约等于 \approx $\approx$</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title>MatterGen: a generative model for inorganicmaterials design</title>
    <url>/2023/12/10/mattergen-a-generative-model-for-inorganic-materials-design/</url>
    <content><![CDATA[<h3 id="摘要部分"><a href="#摘要部分" class="headerlink" title="摘要部分"></a>摘要部分</h3><p>生成式模型为材料设计提供了新范式，但是此前的研究存在着成功率低、只能满足非常有限的性质约束等缺陷<br>MatterGen是一种基于扩散的新型模型，，他通过逐步细化原子类型、坐标和周期晶格来产生晶体结构</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>传统而言，新材料的发现基于实验和科学家直觉</p>
<ul>
<li>一些进展<ul>
<li>high throughout screening 高通量筛选</li>
<li>open material datebases 开放材料数据库</li>
<li>machine-learning-based property predictors 基于机器学习的性质预测器</li>
<li>machine learning force fields (MLFFs) 机器学习力场</li>
</ul>
</li>
<li>局限性<ul>
<li>受限于已知材料的数量</li>
<li>传统筛选方法无法有效针对目标属性的材料进行发现</li>
</ul>
</li>
<li>关注<ul>
<li>论文提到了此前的[[Scaling deep learning for materials discovery]]，指出了相比于潜在的稳定无机化合物（$10^{10}$）目前最大的探索($10^{6}到10^{7}$)，也仅仅是很小的一部分<br>在这些限制下，人们对逆向生成产生了兴趣<br>目前的一些其他缺陷</li>
</ul>
</li>
<li>目前方法基于DFT计算，但是DFT给出的稳定结果实际上未必稳定</li>
<li>目前方法可以处理的元素周期表中的元素有限</li>
<li>目前方法只能优化少数几种材料性质，主要是形成能<ul>
<li>虽然形成能是材料稳定性的一个关键指标</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文研读</category>
      </categories>
      <tags>
        <tag>AI4M</tag>
      </tags>
  </entry>
  <entry>
    <title>Music Generation 工作日志</title>
    <url>/2023/12/04/music-generation-gong-zuo-ri-zhi/</url>
    <content><![CDATA[<h3 id="12-04"><a href="#12-04" class="headerlink" title="12-04"></a>12-04</h3><p>复现了代码</p>
<ul>
<li>一些知识<ul>
<li>导入了os库以后，os.listdir(xx_dir)会列出指定目录下所有的文件和文件夹</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>工作日志</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2023/10/22/langchain-xue-xi-bi-ji/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Prompt Engineering</title>
    <url>/2023/11/26/prompt-engineering-lun-wen-yan-du/</url>
    <content><![CDATA[<h2 id="提示工程整体框架"><a href="#提示工程整体框架" class="headerlink" title="提示工程整体框架"></a>提示工程整体框架</h2><span id="more"></span>
<h4 id="大语言模型设置"><a href="#大语言模型设置" class="headerlink" title="大语言模型设置"></a>大语言模型设置</h4><ul>
<li>Temperature<ul>
<li>参数值越小，模型会返回越确定的结果</li>
</ul>
</li>
<li>Top_p<ul>
<li>参数值越低，结果越准确。参数高则更具有多样性和创造性</li>
</ul>
</li>
</ul>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><h4 id="提示词要素"><a href="#提示词要素" class="headerlink" title="提示词要素"></a>提示词要素</h4><ul>
<li>指令</li>
<li>上下文</li>
<li>输入数据</li>
<li>输出指示</li>
</ul>
<h4 id="设计提示词的通用技巧"><a href="#设计提示词的通用技巧" class="headerlink" title="设计提示词的通用技巧"></a>设计提示词的通用技巧</h4><ul>
<li>参考：<a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api">https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api</a></li>
</ul>
<h4 id="提示词示例"><a href="#提示词示例" class="headerlink" title="提示词示例"></a>提示词示例</h4><h2 id="提示技术"><a href="#提示技术" class="headerlink" title="提示技术"></a>提示技术</h2><h4 id="零样本提示（Zero-Shot）"><a href="#零样本提示（Zero-Shot）" class="headerlink" title="零样本提示（Zero-Shot）"></a>零样本提示（Zero-Shot）</h4><ul>
<li>不向模型提供任何示例</li>
</ul>
<h4 id="少样本提示（Few-Shot）"><a href="#少样本提示（Few-Shot）" class="headerlink" title="少样本提示（Few-Shot）"></a>少样本提示（Few-Shot）</h4><ul>
<li>向模型提供少量示例<ul>
<li>一个有趣的事实是，在少量示例中，即使使用随机标签或者不一致的格式，效果也比没有标签好得多</li>
</ul>
</li>
</ul>
<h4 id="链式思考提示（Chain-of-Thought-Prompting）"><a href="#链式思考提示（Chain-of-Thought-Prompting）" class="headerlink" title="链式思考提示（Chain of Thought Prompting）"></a>链式思考提示（Chain of Thought Prompting）</h4><ul>
<li>结合Few Shot时，在样本提示中给出中间推理步骤</li>
<li>作为Zero Shot使用时，使用特殊提示“让我们逐步思考”，提示模型给出中间思维过程</li>
</ul>
<h4 id="自我一致性（SELF-CONSISTENCY）"><a href="#自我一致性（SELF-CONSISTENCY）" class="headerlink" title="自我一致性（SELF-CONSISTENCY）"></a>自我一致性（SELF-CONSISTENCY）</h4><ul>
<li>通过CoT采样多个不同的推理路径</li>
</ul>
<h4 id="Generated-Knowledge-Prompting"><a href="#Generated-Knowledge-Prompting" class="headerlink" title="Generated Knowledge Prompting"></a>Generated Knowledge Prompting</h4><ul>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231130155743246.png" alt="image-20231130155743246"></li>
<li>通过提示，给出问题相关的知识<ul>
<li>PLM通过（问题-知识）组合构成的提示，依据问题生成相关的知识片段</li>
</ul>
</li>
<li>模型根据问题和相关知识生成答案</li>
</ul>
<h4 id="ToT（Tree-of-Thoughts）"><a href="#ToT（Tree-of-Thoughts）" class="headerlink" title="ToT（Tree of Thoughts）"></a>ToT（Tree of Thoughts）</h4><ul>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231130160718378.png" alt="image-20231130160718378"></li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling deep learning for materials discovery</title>
    <url>/2023/11/30/scaling-deep-learning-for-materials-discovery/</url>
    <content><![CDATA[<blockquote>
<p>Author：Amil Merchant1,3 , Simon Batzner1,3, Samuel S. Schoenholz1,3, Muratahan Aykol1 ,Gowoon Cheon2 &amp; Ekin Dogus Cubuk1,3 </p>
</blockquote>
<h3 id="Abtract"><a href="#Abtract" class="headerlink" title="Abtract"></a>Abtract</h3><ul>
<li><p>Discovery of inorganic crystals has been bottenecked by expensive trial-and-error approaches.</p>
<h4 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h4></li>
<li><p>Convex Hall 计算化学和计算材料学领域，凸包边缘往往代表最稳定的化合物</p>
</li>
<li><p>zero-shot-learning 模型需要在完全未见过的数据类别上做出预测</p>
</li>
<li><p>ad initio 第一性原理计算</p>
</li>
<li><p>estimate stability 也即对比凸包边缘化合物与其竞争项的分解能，分解能低的一般被认为是不稳定的</p>
</li>
<li><p>SAPS（Symmetry-Aware Partial Substitutions，对称性感知部分替换，利用已知材料的结构信息，通过部分替换原子来创造新的化合物或材料</p>
</li>
</ul>
<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><ul>
<li><p>data flywheel 利用图神经网络筛选后的被认为“稳定”的候选分子，再利用DFT进行能量计算验证，利用计算后的结果迭代更新模型</p>
</li>
<li><p>GNoME enables accurate predictions of structures with 5+ unique elements(despite omission from training)</p>
</li>
<li><p><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231130104356265.png" alt="image-20231130104356265"></p>
<ul>
<li><p>两条pipeline 并行工作</p>
</li>
<li><p>结构管线</p>
<ul>
<li>假设有一种已知的材料结构A，我们想要探索A的变体或相关结构是否稳定</li>
<li>利用SAPS等生成新的材料结构，并构建图表示</li>
<li>利用GNN评估材料稳定性，并对稳定结果计算DFT</li>
</ul>
</li>
<li><p>成分管线中</p>
<ul>
<li>从一种特定的化学式出发，比如Li₂S₂O₇</li>
<li>为表达式构建图表示，并利用GNN评估稳定性</li>
<li>在判断为稳定的成分中，利用AIRSS为其随机选择结构，并计算DFT</li>
</ul>
</li>
<li><p>注意，GNN的用途是多样的</p>
</li>
<li><p><strong>对结构的评估</strong>：在结构管线中，GNoME接收的是候选材料的图表示，这些图是基于材料的原子结构和它们之间的连接关系构建的。在这种情况下，GNoME评估的是这些特定结构的稳定性。</p>
</li>
<li><p><strong>对成分的评估</strong>：而在成分管线中，GNoME接收的是化学成分的图表示，可能不包括具体的原子排列信息，而是反映了化学式中不同元素的种类和它们的计量比。这里的挑战是在没有明确的结构信息的情况下预测稳定性。</p>
</li>
</ul>
<h4 id="Definitions-1"><a href="#Definitions-1" class="headerlink" title="Definitions"></a>Definitions</h4></li>
<li><p>AIRSS (Automated Random Structure  Searching)随机生成大量的结构配置，然后使用DFT等评估它们的稳定性</p>
</li>
<li><p>$r^2$SCAN &amp; PBE 这是两种DFT方法，PBE被一般认为精度不如$r^2$SCAN，但速度较快，一般实验人员会对PBE初步计算后认为较有可能性的材料进行进一步$r^2$SCAN计算</p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文研读</category>
      </categories>
      <tags>
        <tag>AI4M</tag>
      </tags>
  </entry>
  <entry>
    <title>Python绘图</title>
    <url>/2023/11/21/python-hui-tu/</url>
    <content><![CDATA[<h1 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h1><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>Figure 是Axes的容器，画布</li>
<li>Axes （带坐标轴的）子图，具体的图形元素放置在Axes上</li>
<li>Axis 坐标轴，上面有ticks和labels</li>
<li>Artist 各种图形元素的统称</li>
<li>legend 图例</li>
</ul>
<h3 id="Figure对象"><a href="#Figure对象" class="headerlink" title="Figure对象"></a>Figure对象</h3>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care</title>
    <url>/2023/11/22/the-artificial-intelligence-clinician-learns-optimal-treatment-strategies-for-sepsis-in-intensive-care/</url>
    <content><![CDATA[<h3 id="Abstract-部分"><a href="#Abstract-部分" class="headerlink" title="Abstract 部分"></a>Abstract 部分</h3><ul>
<li>Sepsis（脓毒症）best treatment atrategy remains uncertain</li>
<li>Current practice in the administration of intravenous fluids (静脉输液) and vasopressors (血管升压药) are suboptimal</li>
<li>This Problem is a sequential decision-making-problem</li>
<li>Learn optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions</li>
</ul>
<h3 id="Intro部分"><a href="#Intro部分" class="headerlink" title="Intro部分"></a>Intro部分</h3><ul>
<li>Clinical variability in sepsis treatment is extreme,suboptimal decisions lead to poorer outcomes</li>
<li>The intrinsic design of models using reinforcement learning can handle sparse reward signals<ul>
<li>over come the complexity related to the heterogeneity of patient responses to medical interventions</li>
<li>and the delayed indications of the efficacy of treatments</li>
</ul>
</li>
<li>Database<ul>
<li>Medical Information Mart for Intensive Care version Ⅲ</li>
<li>eICU Research Institute Database(eRI)</li>
</ul>
</li>
<li>Data Process<ul>
<li>a set of 48 variables including demographics, Elixhauser premorbid status, vital signs, laboratory values, fluids and vasopressors received</li>
<li>Patients’ data were coded as multidimensional discrete time series with 4-h time steps. The study included up to 72h of measurements taken around the estimated time if onset of sepsis</li>
<li>The <strong>total volume of intravenous fluids</strong> and <strong>maximum dose of vasopressosrs</strong> administered over each <strong>4-h period</strong> defined the medical treatments of interests</li>
</ul>
</li>
</ul>
<p>Method部分</p>
<ul>
<li>To further assess the validity of the state aggregation, they used the distribution of International Classification of Diseases codes in the states and demonstrated that past medical history and diagnoses are encapsulated to some extent within our chosen state definition.也即ICD与聚类结果有类似性，证明聚类有一定的代表性</li>
</ul>
]]></content>
      <categories>
        <category>论文研读</category>
      </categories>
      <tags>
        <tag>HealthCare</tag>
      </tags>
  </entry>
  <entry>
    <title>VS2012只能对 Type.IsGenericTypeDefinition 为 True</title>
    <url>/2023/09/21/vs2012-zhi-neng-dui-type.isgenerictypedefinition-wei-true-wen-ti/</url>
    <content><![CDATA[<p>这是个VS2012解决方案的问题。</p>
<span id="more"></span>
<p>System.Collections.Generic.RandomizedStringEqualityComparer 不是 GenericTypeDefinition。只能对 Type.IsGenericTypeDefinition 为 True。</p>
<p>解决方法：到<a href="http://www.microsoft.com/zh-cn/download/confirmation.aspx?id=36020%E4%B8%8A%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7%E5%8C%85%EF%BC%8C%E5%AE%89%E8%A3%85%E5%8D%B3%E5%8F%AF%E3%80%82">http://www.microsoft.com/zh-cn/download/confirmation.aspx?id=36020上下载工具包，安装即可。</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Python的科学与数值计算</title>
    <url>/2023/09/19/ji-yu-python-de-ke-xue-yu-shu-zhi-ji-suan/</url>
    <content><![CDATA[<h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><span id="more"></span>
<h2 id="ndarray"><a href="#ndarray" class="headerlink" title="ndarray"></a>ndarray</h2><ul>
<li>基本信息<ul>
<li>numpy的基础数据结构（下简称ndarray为array）</li>
<li>向量、矩阵、张量等均用array表示</li>
<li>数组下标从0开始</li>
</ul>
</li>
<li>ndarray与list的区别<ul>
<li>array元素为相同数据类型</li>
<li>array固定长度</li>
<li>array易进行维度、形状变换</li>
<li>array支持完整的向量&#x2F;张量操作和线性代数函数</li>
</ul>
</li>
<li>adarray类<ul>
<li>参数<ul>
<li>shape 形状</li>
<li>dtype 类型</li>
<li>buffer 可以指定数据块而无需复制</li>
<li>offset 指定从数据块的第几个位置开始复制</li>
<li>strides 各维度步进字节数</li>
<li>…</li>
</ul>
</li>
<li>属性<ul>
<li>shape 包含数组每个维度的元素数量的元组，如 (3, 5)</li>
<li>size 元素总数</li>
<li>ndim 维度数量，1、2、3⋯</li>
<li>nbytes 存储数据的总字节数</li>
<li>dtype 数据类型</li>
<li>itemsize 每个元素的字节数，如 float32 是 4 个字节</li>
<li>strides: 跨步&#x2F;步幅，索引访问的间隔字节数</li>
<li>data 数组的真正数据</li>
</ul>
</li>
</ul>
</li>
<li>基于array的运算<ul>
<li>基于元素的算数运算<ul>
<li>numpy数组的基本算数运算是作用到每个元素上的</li>
<li>广播机制<ul>
<li>a、b 维数相等，某一维（轴）上的数量不等，其中一个数组在该维度上数量为 1，则将该数组在该轴上的数量扩充（复制），使其与另外的数组相同（广播）</li>
<li>a、b 维数不等，将维数少的数组从左扩充长度为 1 的新维度，直到二者维度相等。然后再执行上面的广播操作</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>聚合函数</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>windows系统时间同步</title>
    <url>/2023/06/17/windows-xi-tong-shi-jian-tong-bu/</url>
    <content><![CDATA[<p>实在是太可恶了……</p>
<span id="more"></span>
<p>从三月份开始电脑的时间总是和时间服务器同步不上，导致梯子有很多节点不能连，火狐干脆网站都不让上。<br>在设置里试了好几次同步时间服务器都无果，于是乎到处搜该咋办。跟着热心网友的经验贴这边修修那边改改发现都不行（期末周特有的没事找事）。<br>最后又是一位老哥一语惊醒梦中人。<br>你这是校园网……连手机热点就行了。<br>于是连上手机热点。<br>解决。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>奇遇</tag>
      </tags>
  </entry>
  <entry>
    <title>工程图学</title>
    <url>/2023/11/01/gong-cheng-tu-xue/</url>
    <content><![CDATA[<span id="more"></span>
<ul>
<li>面对应封闭线框，只要是单独的面，就要有封闭线框与它对应。</li>
<li>类似性指导作图，面和它的封闭线框具有相似性</li>
<li>粗实线&gt;不可见轮廓线（虚线）&gt;对称线（点画线）&gt;辅助线</li>
<li>找点的方法之辅助直线<ul>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231108104948928.png" alt="image-20231108104948928" style="zoom: 33%;" /></li>
<li>要在$c,b$上找点，在主视图上将$c’,b’$分点的比例移到俯视图（即在俯视图构建辅助直线，辅助线与$c’,b’$及分点等长）</li>
<li>点划线露出4mm</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>数理方程特殊函数部分以及其他</title>
    <url>/2023/06/14/shu-li-fang-cheng/</url>
    <content><![CDATA[<span id="more"></span>
<p>级数解</p>
<ul>
<li>高次项系数化为1，判断展开点类型，为常点还是奇点</li>
<li>回到原式中带入级数解进行计算</li>
</ul>
<p>坐标系</p>
<ul>
<li>极坐标下拉普拉斯：$\frac{1}{r}\frac{\partial}{\partial r}(r\frac{\partial}{\partial r})+\frac{1}{r^2}\frac{\partial^2}{\partial \phi^2}$</li>
<li>柱坐标下拉普拉斯：$\frac{1}{r}\frac{\partial}{\partial r}(r\frac{\partial}{\partial r})+\frac{1}{r^2}\frac{\partial^2}{\partial \phi^2}+\frac{\partial^2}{\partial z ^2}$</li>
<li>球坐标系拉普拉斯：$\frac{1}{r^2}\frac{\partial}{\partial r}(r^2\frac{\partial}{\partial r})+\frac{1}{r^2sin^2\theta}\frac{\partial^2}{\partial \phi^2}+\frac{1}{r^2sin\theta}\frac{\partial}{\partial \theta}(sin\theta\frac{\partial}{\partial \theta})$</li>
</ul>
<p>Bessel方程</p>
<ul>
<li>$v$阶Bessel方程：$x^{2}y’’+xy’+(x^{2}-v^{2})y&#x3D;0$</li>
<li>解记作$J_{\pm v(x)}$</li>
<li>构造$N_{v}(x)&#x3D;\frac{cosv\pi J_{v}(x)-J_{-v}(x)}{sinv\pi}，J_{v}$与$N_{v}$线性无关</li>
<li><del>第三类柱函数</del></li>
<li>对于$\Gamma$函数的补充，对正整数，$\Gamma (k+1)&#x3D;k!，\Gamma(\frac{1}{2})&#x3D;\sqrt{\frac{\pi}{2}}$</li>
<li>基本递推关系</li>
</ul>
<p>虚宗量Bessel方程：$x^{2}y’’+xy’+(-x^{2}-v^{2})y&#x3D;0$</p>
<ul>
<li>第一类$I_{v}(x)$，第二类 $K_{v}(x)$</li>
</ul>
<p>球Bessel方程：$\frac{d}{dx}(x^2\frac{dR}{dx})+[x^2-l(l+1)]R$</p>
<ul>
<li><p>特征为$l(l+1)$</p>
</li>
<li><p>解为$ j_l, n_l$</p>
</li>
</ul>
<p>渐进行为</p>
<ul>
<li>讨论渐进行为的意义在于，求解本征值问题时，需要去除不符合边界条件的函数</li>
<li>x趋向于0时，若$v&#x3D;0$，$J_0(x)$趋向于1，若$v&gt;0$，$J_v(x)$趋向于0</li>
<li>x趋向于0时，$N_v(x)$趋向于$\infin$</li>
<li>x趋向于$\infin$时，$J_{v}(x)与N_{v}(x)$均趋向于0</li>
<li>$ j_l, n_l$与上述$ J_v, N_v$类似</li>
<li>x趋向于0时，若$v&#x3D;0$，$I_0(x)$趋向于1，若$v&gt;0$，$I_v(x)$趋向于0</li>
<li>x趋向于0时，$K_v(x)$趋向于$\infin$</li>
<li>x趋向于$\infin$时，$I_{v}(x)$趋向于0，$K_{v}(x)$趋向于$\infin$</li>
</ul>
<p>Legendre方程和连带Legendre方程的形式</p>
<ul>
<li><p>$\frac{1}{sin\theta}\frac{d}{d\theta}(sin\theta\frac{d\Theta}{d\theta})+(\lambda-\frac{\mu}{sin^{2}\theta})\Theta&#x3D;0$</p>
</li>
<li><p>$(1-x^2)y’’-2xy’+(\lambda-\frac{m^2}{1-x^2})y$</p>
<ul>
<li>$m&#x3D;0$时，为勒让德方程，$m&#x3D;1、2、3…$时，为连带勒让德方程</li>
</ul>
</li>
<li><p>Legendre方程$(1-x^2)y’’-2xy’+\lambda y$ ，若补充边界约束$y(\pm1)&lt;\infin$则退化为级数解</p>
<ul>
<li>由级数解，$\lambda_{l}&#x3D;l(l+1),l&#x3D;0、1、2…$，本征函数为$P_{l}(x)$</li>
<li>Legendre多项式的微分表示：$\frac{1}{2^{l}l!}[(x^{2}-1)^{l}]^{(l)}$</li>
</ul>
</li>
<li><p>Legendre多项式的生成函数</p>
<ul>
<li>$\sum_{l&#x3D;0}^{\infty} P_l(x) t^l&#x3D;\frac{1}{\sqrt{1-2 x t+t^2}}$</li>
<li>$||P_{l}||^{2}&#x3D;\frac{2}{2l+1}$ (拿生成函数直接平方积分比较系数)</li>
</ul>
</li>
<li><p>连带Legendre方程$(1-x^2)y’’-2xy’+(\lambda-\frac{m^2}{1-x^2})y$</p>
<ul>
<li><p>微分表示 $\frac{1}{2^{l}l!}(-1)^{m}(1-x^2)^{\frac{m}{2}}[(x^{2}-1)^{l}]^{(l+m)}$ 其中 $l&gt;m$</p>
</li>
<li><p>$||P_{l}^{m}||^{2}&#x3D;\frac{2}{2l+1}\frac{(l+m)!}{(l-m)!}$</p>
</li>
</ul>
</li>
<li><p>球谐函数（拉普拉斯方程角向方程的本征函数）</p>
<ul>
<li>函数来源：半径为a的球内拉普拉斯方程，利用$\lambda$分离$R(r)S(\theta,\phi)$，得到角向和径向方程。再利用$\mu$分离$S(\theta,\phi)&#x3D;\Theta(\theta)\Phi(\phi)$</li>
<li>把$\theta和\phi$组合起来得$S$，这就是球谐函数</li>
<li>球谐函数的模长<ul>
<li>补充说明：径向函数为欧拉方程，设$r&#x3D;e^{t}$求解</li>
</ul>
</li>
</ul>
</li>
<li><p><del>递推公式、连带Legendre函数模方</del></p>
</li>
</ul>
<p>傅里叶变换和逆变换形式</p>
<p> Laplace变换和逆变换形式</p>
<p>傅里叶变换的卷积公式</p>
<p>Laplace变换的卷积公式</p>
<p>方程标准型特征方程</p>
<ul>
<li><p>特征方程法</p>
<ul>
<li><p>二阶方程经过标准化变换后新的系数公式会给</p>
</li>
<li><p>解题时，首先准确写出各项系数，特别注意 $ b $ 在原式中为$2b$</p>
</li>
<li><p>之后判断方程类型，$\Delta&lt;0$ 时为椭圆型（方程形式为$U_{\rho\rho}+U_{\sigma \sigma}&#x3D;\Phi$） </p>
</li>
<li><p>求解特征方程$a(\frac{dy}{dx})^2-2b(\frac{dy}{dx})+c&#x3D;0$ 方法与解二次函数类似，得到由常数控制的一组（抛物型）或者两组方程。</p>
</li>
<li><p>分别令 $\xi$ 和 $\eta$ 等于这两组方程（抛物型 $\eta$ 任取与 $\xi$ 独立即可） </p>
</li>
<li><p>利用变换系数公式求解方程各项系数</p>
</li>
</ul>
</li>
<li><p>配方法</p>
<ul>
<li><del>变系数配方法</del></li>
<li>常系数配方法，将二次项配方后，换元即可</li>
</ul>
</li>
</ul>
<p>格林函数</p>
<ul>
<li>电像法</li>
<li>本征函数展开法<ul>
<li>首先将式子列成$L(u)&#x3D;-\rho$ 求解本征函数问题$(L+\lambda_{n})u_{n}&#x3D;0$<ul>
<li>注：$\rho$也可能为0，例如导体球内部无电荷</li>
</ul>
</li>
<li>将$G$ 展开并带入$\Delta G$，与$-\rho$联立求展开系数即可</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>时光</tag>
      </tags>
  </entry>
  <entry>
    <title>材料分析与表征 9.18</title>
    <url>/2023/09/18/cai-liao-fen-xi-yu-biao-zheng/</url>
    <content><![CDATA[<span id="more"></span>
<ul>
<li>课程详细信息<ul>
<li>期末考试70%、课程展示20%、考勤作业10%</li>
</ul>
</li>
</ul>
<h3 id="X射线衍射技术-X-Ray-Diffraction"><a href="#X射线衍射技术-X-Ray-Diffraction" class="headerlink" title="X射线衍射技术(X-Ray Diffraction)"></a>X射线衍射技术(X-Ray Diffraction)</h3><ul>
<li>X光的发现 1895.11.8 伦琴</li>
<li>肉眼不可见、使铂氯化钡发光、使照相底板感光、气体电离、杀伤生物细胞</li>
</ul>
<h3 id="微观结构的研究方法"><a href="#微观结构的研究方法" class="headerlink" title="微观结构的研究方法"></a>微观结构的研究方法</h3><ul>
<li>仪器极限分辨率 $\lambda$&#x2F;2 （远场光学）</li>
</ul>
<h3 id="X光的本质"><a href="#X光的本质" class="headerlink" title="X光的本质"></a>X光的本质</h3><ul>
<li>波长介于紫外线和 $\gamma$ 射线之间，约为0.01~100埃</li>
<li>长波长——软X射线 短波长——硬X射线</li>
</ul>
<h3 id="X射线的应用与发展"><a href="#X射线的应用与发展" class="headerlink" title="X射线的应用与发展"></a>X射线的应用与发展</h3><h3 id="X射线的性质"><a href="#X射线的性质" class="headerlink" title="X射线的性质"></a>X射线的性质</h3><h3 id="X射线的产生"><a href="#X射线的产生" class="headerlink" title="X射线的产生"></a>X射线的产生</h3><ul>
<li>高速运动的带电粒子的运动遇到障碍被减速时便产生X射线</li>
<li>从X射线管角度理解<ul>
<li>对绕成螺旋型的钨丝通电加热至白热，放出热辐射电子</li>
<li>电子在数万伏管内高压电场的作用下轰击靶材产生X射线</li>
</ul>
</li>
</ul>
<h3 id="X射线谱"><a href="#X射线谱" class="headerlink" title="X射线谱"></a>X射线谱</h3><ul>
<li>连续X射线谱，有公式$eV &#x3D; h\nu <em>{max}&#x3D;\frac{hc}{\lambda</em>{0}}$ </li>
<li>经验规律，连续谱中强度最大处的波长记为$\lambda_{m}$ 则一般有$\lambda_{m}&#x3D;1.5\lambda_{0}$</li>
<li>电流决定了热辐射电子的多少，电压决定了热辐射电子的强度</li>
</ul>
<h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><ul>
<li>已知$Cu-K_a$线波长，计算铜原子$2p$和$1s$能级间的能量差<ul>
<li>解题方法，首先明确从$2p$到$1s$属于从$L$层跃迁到$K$层，也即$K_a$线</li>
<li>故直接利用波长与能量关系计算</li>
</ul>
</li>
</ul>
<h3 id="X射线与物质的相互作用"><a href="#X射线与物质的相互作用" class="headerlink" title="X射线与物质的相互作用"></a>X射线与物质的相互作用</h3><h3 id="X射线衰减规律"><a href="#X射线衰减规律" class="headerlink" title="X射线衰减规律"></a>X射线衰减规律</h3><ul>
<li>吸收的不连续性</li>
<li>吸收限的利用<ul>
<li>靶材（Target Material）：</li>
<li>靶材通常是一个坚固的材料，如金属，用于产生X射线。当高速电子束撞击靶材时，与原子相互作用，产生X射线辐射。我们要尽量选择波长略长于试样吸收限的靶材，这样不会有K系荧光辐射，同时也不会被试样吸收太多。</li>
<li>滤片（Filter）：</li>
<li>滤片是放置在X射线束路径上的材料，通常由金属或其他适当的材料制成。滤片的主要作用是改变或调整X射线的能量谱。通过选择不同材料的滤片，可以过滤掉低能量的X射线，使得只有特定能量范围内的X射线通过</li>
<li>试样（Sample）：</li>
<li>试样是要进行X光实验的物质或样品。当X射线穿过试样时，它会与试样中的原子相互作用，产生一系列特定能量的X射线散射或吸收。通过分析这些散射或吸收事件，可以获得有关试样的信息，例如其成分、结构或厚度等</li>
</ul>
</li>
</ul>
<h3 id="X光的探测与防护"><a href="#X光的探测与防护" class="headerlink" title="X光的探测与防护"></a>X光的探测与防护</h3><h2 id="晶体学基础"><a href="#晶体学基础" class="headerlink" title="晶体学基础"></a>晶体学基础</h2><h3 id="晶体结构"><a href="#晶体结构" class="headerlink" title="晶体结构"></a>晶体结构</h3><ul>
<li><p>赤平极射投影</p>
<ul>
<li>方位角 $\phi$包含该点的子午面与$0°$子午面的夹角</li>
<li>极距角 $\rho$该点与北极点的夹角</li>
<li>赤平投影图上点距离圆心的距离与方位角有关，基圆与极距角有关<br>  重点（吴氏网、标准点阵、倒易点阵）</li>
</ul>
</li>
<li><p>吴氏网</p>
<ul>
<li>吴氏网只能水平上走格子，如果想在竖直上走格子，需要旋转吴氏网，然后再所有的点一起水平上走格子<br> 对于任意可微的对称性，一定存在一个相应的守恒流</li>
</ul>
</li>
<li><p>倒易点阵</p>
<ul>
<li><p>对于一个给定基矢的正点阵，必有倒易点阵与之相对应，关系为<br>$$a^{*}&#x3D;\frac{b×c}{v}$$<br>此处$v$为正点阵的体积	</p>
</li>
<li><p>有关正点阵和倒易点阵，相关的符号标识如下</p>
<ul>
<li>晶面指数$(hkl),(uvw)^{*}$</li>
<li>晶向指数$[uvw],[hkl]^{*}$</li>
<li>面间距$d_{hkl},d_{uvw}^{*}$</li>
<li>晶向或阵矢$r&#x3D;ua+vb+wc,g^{<em>}&#x3D;ha^</em>+kb^*+lc^*$</li>
<li>晶向长度或阵矢大小$r_{uvw},g_{hkl}$</li>
<li>结点位置$uvw,hkl$</li>
</ul>
</li>
<li><p>倒易点阵晶向垂直于正点阵同名晶面，反之亦然</p>
</li>
<li><p>正点阵的面间距是同名倒易矢量长度的倒数</p>
<p>$$|g_{hkl}|^2&#x3D; \frac1{d_{hkl}^2}&#x3D;(h\boldsymbol{a}^*+k\boldsymbol{b}^*+l\boldsymbol{c}^*)\bullet(h\boldsymbol{a}^*+k\boldsymbol{b}^*+l\boldsymbol{c}^*)\&#x3D; h^2(a^*)^2+k^2(b^*)^2+l^2(c^*)^2\+2kl\boldsymbol{b}^<em>\boldsymbol{c}^</em>\boldsymbol{c}^<em>cos\boldsymbol{a}^</em>+2lh\boldsymbol{c}^<em>\boldsymbol{a}^</em>cos\boldsymbol{\beta}^*+2h\boldsymbol{k}a^<em>\boldsymbol{b}^</em>cos\boldsymbol{\gamma}^* $$</p>
<ul>
<li>立方晶系<ul>
<li>$d_{hkl}&#x3D;\frac{a}{\sqrt{h^2+k^2+l^2}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="阶段作业"><a href="#阶段作业" class="headerlink" title="阶段作业"></a>阶段作业</h3><ul>
<li>P81-82 T 15 16 22 24</li>
</ul>
<h2 id="X射线与材料的散射、干涉与衍射"><a href="#X射线与材料的散射、干涉与衍射" class="headerlink" title="X射线与材料的散射、干涉与衍射"></a>X射线与材料的散射、干涉与衍射</h2><h3 id="散射"><a href="#散射" class="headerlink" title="散射"></a>散射</h3><ul>
<li><p>电子的散射，静电相互作用等</p>
</li>
<li><p>X射线的散射，与物质的基本单元发生作用</p>
</li>
<li><p>单电子对X射线的散射</p>
<ul>
<li>相干散射<ul>
<li>相干散射是衍射的基础，从波动性的角度出发，X射线是电磁波，其照射在自由电子上时，交变电场就会强迫电子作频率相同的振动，发射X射线。电子发出的X射线为散射线，与入射线波长相同、相位滞后恒定，故可以发生干涉。</li>
<li>平面偏振光相干散射</li>
<li>非平面偏振光相干散射<ul>
<li>原子中只需要考虑电子散射</li>
</ul>
</li>
</ul>
</li>
<li>非相干散射</li>
</ul>
</li>
<li><p>散射线的干涉</p>
<ul>
<li>相位差与反射矢量<ul>
<li>相位差满足$$\phi &#x3D; 2\pi \frac{-S_{0}·r+S·r }{\lambda}$$其中$S_0$为入射线上的单位矢量，$S$为反射线上的单位矢量</li>
<li>记 $s&#x3D;\frac{S-S_0}{\lambda}\quad s$为散射矢量，其大小为$\frac{2sin\theta}{\lambda}$，引入散射矢量后，上式可化作$\phi &#x3D;2\pi s·r$</li>
</ul>
</li>
</ul>
</li>
<li><p>原子对X射线的散射</p>
<ul>
<li>单电子原子的散射<ul>
<li>原子中一个电子的散射因子$$f(s) &#x3D; \int_v \rho(r)e^{i2\pi s·r}dv$$</li>
<li>电子的相干强度为$I_{相干}&#x3D;f^{2}(s)I_{电子}$</li>
</ul>
</li>
<li>多电子原子的散射<ul>
<li>假设原子的总电荷密度为各个电子电荷密度之和</li>
</ul>
</li>
</ul>
</li>
<li><p>晶胞的散射</p>
</li>
<li><p>小晶体的衍射</p>
<ul>
<li><p>小晶体的衍射强度</p>
<ul>
<li><p>小晶体的衍射线振幅</p>
<p>$$<br>A_\text{ 晶体 }{ ( s ) }&#x3D;\sum_{m&#x3D;0}^{N_a-1}\sum_{m&#x3D;0}^{N_b-1}\sum_{p&#x3D;0}^{N_c-1}F\left(s\right)\mathrm{e}^{\mathrm{i}2\pi\mathbf{s}\cdot\mathbf{R}_{mnp}}<br>$$</p>
<p>因其中的结构因子与晶胞的位置无关, 从而有<br>$$<br>A_\text{ 晶体 }{ ( s ) }&#x3D;F\left(s\right)\sum_{m&#x3D;0}^{N_a-1}\sum_{m&#x3D;0}^{N_b-1}\sum_{p&#x3D;0}^{N_c-1}\mathrm{e}^{\mathrm{i}2\pi\mathbf{s}\cdot\mathbf{R}<em>{mnp}}<br>$$<br>于是, 晶体的衍射线强度为<br>$$<br>I</em>{\text {晶体 }}(s)&#x3D;|F(s)|^2\left|\sum_{m n p}^N \mathrm{e}^{\mathrm{i} 2 \pi s \cdot \boldsymbol{R}<em>{m p p}}\right|^2 I</em>{\text {电子 }}<br>$$<br>定义<br>$$<br>L(s) \equiv\left|\sum_{m n p}^N \mathrm{e}^{\mathrm{i} 2 \pi s \cdot \boldsymbol{R}<em>{\operatorname{mmp}}}\right|^2<br>$$<br>由于它表示的是晶体散射线的干涉结果, 故称为晶体的干涉函数, 或劳厄函数, 衍射线强度可以简写成<br>$$<br>I</em>{\text {晶体 }}(s)&#x3D;|F(s)|^2 L(s) I_{\text {电子 }}<br>$$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="衍射线的强度分析"><a href="#衍射线的强度分析" class="headerlink" title="衍射线的强度分析"></a>衍射线的强度分析</h2><h2 id="多晶体衍射信息的获取方法"><a href="#多晶体衍射信息的获取方法" class="headerlink" title="多晶体衍射信息的获取方法"></a>多晶体衍射信息的获取方法</h2><h3 id="德拜法"><a href="#德拜法" class="headerlink" title="德拜法"></a>德拜法</h3><ul>
<li>德拜照片的计算与标定<ul>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009103514509.png" alt="image-20231009103514509" style="zoom:50%;" />
</li>
<li><p>圆弧之间的对应的顶角为4$\theta$</p>
</li>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009104249654.png" alt="image-20231009104249654" style="zoom:50%;" />
</li>
<li><p>此处采用的角度标准为（deg）</p>
</li>
<li><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231009104341784.png" alt="image-20231009104341784" style="zoom:50%;" />
</li>
<li><p>考虑布拉格公式$d&#x3D;\frac{\lambda}{2sin\theta}$和晶面间距计算公式$d_{hkl}&#x3D;\frac{a}{\sqrt{h^2+k^2+l^2}}$</p>
</li>
<li><p>得$$\sin^2\theta&#x3D;\frac{\lambda^2}{4a^2}(h^2+k^2+l^2)$$</p>
</li>
<li><p>令$\frac{\lambda^{2}}{4a^2}&#x3D;K$并设$m$为干涉指数的平方和，即$$m&#x3D;h^2+k^2+l^2$$于是$sin^2\theta &#x3D; Km$</p>
</li>
<li><p>计算出$K$和每条德拜线的$sin^2\theta$并计算出对应的m值（$m$均为正整数）然后依据衍射线条在晶系中的出现顺序判断</p>
</li>
</ul>
</li>
</ul>
<h3 id="衍射仪法"><a href="#衍射仪法" class="headerlink" title="衍射仪法"></a>衍射仪法</h3><h3 id="衍射材料的获得"><a href="#衍射材料的获得" class="headerlink" title="衍射材料的获得"></a>衍射材料的获得</h3><h4 id="试样制备要求"><a href="#试样制备要求" class="headerlink" title="试样制备要求"></a>试样制备要求</h4><ul>
<li>晶粒大小</li>
<li>试样大小厚度和质量</li>
<li>避免产生择优取向</li>
</ul>
<h4 id="衍射全图的获得"><a href="#衍射全图的获得" class="headerlink" title="衍射全图的获得"></a>衍射全图的获得</h4><h4 id="单峰测试"><a href="#单峰测试" class="headerlink" title="单峰测试"></a>单峰测试</h4><h4 id="衍射信息的获取"><a href="#衍射信息的获取" class="headerlink" title="衍射信息的获取"></a>衍射信息的获取</h4><h4 id="衍射线的线形分析"><a href="#衍射线的线形分析" class="headerlink" title="衍射线的线形分析"></a>衍射线的线形分析</h4><h2 id="单晶体衍射信息的获取方法"><a href="#单晶体衍射信息的获取方法" class="headerlink" title="单晶体衍射信息的获取方法"></a>单晶体衍射信息的获取方法</h2><h3 id="劳埃法"><a href="#劳埃法" class="headerlink" title="劳埃法"></a>劳埃法</h3><ul>
<li>特征：使用连续的X射线谱照射固定不动的单晶体</li>
</ul>
<h2 id="X射线衍射的应用"><a href="#X射线衍射的应用" class="headerlink" title="X射线衍射的应用"></a>X射线衍射的应用</h2><h3 id="物相分析"><a href="#物相分析" class="headerlink" title="物相分析"></a>物相分析</h3><h4 id="定性相分析"><a href="#定性相分析" class="headerlink" title="定性相分析"></a>定性相分析</h4><h4 id="定量相分析"><a href="#定量相分析" class="headerlink" title="定量相分析"></a>定量相分析</h4><ul>
<li><p>PDF卡片</p>
</li>
<li><p>基础：物质的衍射强度与物质参与衍射的体积成正比<br>$$<br>I&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_0^2}\left(\frac{e^2}{m c^2}\right)^2 \frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P V \mathrm{e}^{-2 M} A(\theta)<br>$$</p>
<ul>
<li>对于粉末衍射仪法，$A(\theta)&#x3D;1&#x2F;2\mu$</li>
</ul>
</li>
<li><p>举例来说，两相体系下<br>$$<br>\begin{aligned}<br>&amp; I_\alpha&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_\alpha^2}\left(\frac{e^2}{m c^2}\right)^2\left(\frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P \mathrm{e}^{-2 M}\right)_\alpha \frac{V_\alpha}{2 \mu} \<br>&amp; I_\beta&#x3D;\frac{I_0 \lambda^3}{32 \pi R v_\beta^2}\left(\frac{e^2}{m c^2}\right)^2\left(\frac{1+\cos ^2 2 \theta}{\sin ^2 \theta \cos \theta} F^2 P \mathrm{e}^{-2 M}\right)_\beta \frac{V_\beta}{2 \mu}<br>\end{aligned}<br>$$</p>
<ul>
<li>$\mu$ 是混合物的线吸收系数</li>
<li>前面的部分二者都相同，所以只和$V$有关系</li>
<li>$\mu ^*$是质量吸收系数，为$\mu &#x2F; \rho$</li>
<li>不管什么方法，$\alpha$相衍射强度 $I_{\alpha}&#x3D;K\frac{C_{\alpha}}{\mu}$</li>
</ul>
</li>
<li><p>外标法</p>
<ul>
<li>本质上是<br>  $$<br>  I_\alpha&#x3D;K \frac{w_a}{\rho_\alpha \mu^*}<br>  $$</li>
<li>上式的$\mu ^*$是总质量吸收系数</li>
<li>而对于纯 $\alpha$ 相的试样, 即标样, 其同指数衍射线的强度应为<br>  $$<br>  I_{\alpha_0}&#x3D;K \frac{1}{\rho_\alpha \mu_a^*}<br>  $$</li>
<li>从而, 待测试样中 $\alpha$ 相的衍射强度与 $\alpha$ 相标样的衍射强度的比值为</li>
</ul>
<p>  $$<br>  \frac{I_a}{I_{\alpha_0}}&#x3D;\frac{\mu_a^*}{\mu^*} w_a<br>  $$</p>
<ul>
<li>如果试样仅由 $\alpha 、 \beta$ 两相组成, 则上式可以写成</li>
</ul>
<p>  $$<br>  \frac{I_\alpha}{I_{\alpha_0}}&#x3D;\frac{w_\alpha \mu_\alpha^*}{w_\alpha\left(\mu_\alpha^*-\mu_\beta^<em>\right)+\mu_\beta^</em>}<br>  $$</p>
<ul>
<li>实际使用时，首先要知道是什么和什么的混合，找到工作曲线，之后通过两次实验测定$\frac{I_\alpha}{I_{\alpha_0}}$，在工作表上对应找即知道物质百分比</li>
</ul>
</li>
<li><p>内标法</p>
<ul>
<li>内标法首先向试样中加入标准物，然后让待测相与标准相进行比较，相当于通过加入标准相的方法，知道了$\omega _{s}$ </li>
<li>待测相衍射线强度与标准相衍射线强度之比可以表述为</li>
<li>$$\frac{I^{‘}<em>{a}}{I</em>{s}}&#x3D;K\frac{\omega^{‘}<em>{a}}{\omega</em>{s}}$$</li>
<li>设法得到$K$值即可</li>
</ul>
</li>
</ul>
<h4 id="精确测定点阵常数"><a href="#精确测定点阵常数" class="headerlink" title="精确测定点阵常数"></a>精确测定点阵常数</h4><h2 id="宏观应力分析"><a href="#宏观应力分析" class="headerlink" title="宏观应力分析"></a>宏观应力分析</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>只要有应力存在就会有应变，，也即晶面间距的变化。X光可以很好地测面间距的变化，所以可以用来测应力。</p>
<h4 id="应力应变关系"><a href="#应力应变关系" class="headerlink" title="应力应变关系"></a>应力应变关系</h4><ul>
<li><p>考虑受轴向拉力$F_x$的截面为A的棒</p>
<ul>
<li>胡克定律指出$\epsilon_x &#x3D; \frac{\sigma_x}{E}$</li>
<li>轴向拉伸，垂直于轴的面也会发生形变</li>
<li>$$-\epsilon_y &#x3D; -\epsilon_z &#x3D; \nu \epsilon_x &#x3D; \frac{\nu \sigma_x}{E}$$<ul>
<li>其中$\nu$是泊松比</li>
</ul>
</li>
</ul>
</li>
<li><p>考虑微变形情况下，主应力和主应变的广义胡克定律为</p>
<ul>
<li>$\epsilon_1 &#x3D; \frac{1}{E}[\sigma_1 - \nu(\sigma_2+\sigma_3)]$<ul>
<li>（$cyc$）</li>
</ul>
</li>
</ul>
</li>
<li><p>任意一个方向（注意应力是一个张量）</p>
<p>$$ \left{\begin{array}{l}<br>\sigma_{\phi \psi}&#x3D;\alpha_1^2 \sigma_1+\alpha_2^2 \sigma_2+\alpha_3^2 \sigma_3 \<br>\varepsilon_{\phi \psi}&#x3D;\alpha_1^2 \varepsilon_1+\alpha_2^2 \varepsilon_2+\alpha_3^2 \varepsilon_3<br>\end{array}\right.$$</p>
</li>
<li><p>其中$$\begin{aligned}<br>&amp; \alpha_1&#x3D;\sin \psi \cos \phi \<br>&amp; \alpha_2&#x3D;\sin \psi \sin \phi \<br>&amp; \alpha_3&#x3D;\cos \psi<br>\end{aligned} $$</p>
</li>
<li><p>广义胡克定律<br>$$\left{\begin{array}{l}\varepsilon_1&#x3D;\frac{1}{E}\left[\sigma_1-\nu\left(\sigma_2+\sigma_3\right)\right] \ \varepsilon_2&#x3D;\frac{1}{E}\left[\sigma_2-\nu\left(\sigma_1+\sigma_3\right)\right] \ \varepsilon_3&#x3D;\frac{1}{E}\left[\sigma_3-\nu\left(\sigma_1+\sigma_2\right)\right]\end{array}\right.$$</p>
</li>
</ul>
<h4 id="X射线衍射方法测定应力的原理"><a href="#X射线衍射方法测定应力的原理" class="headerlink" title="X射线衍射方法测定应力的原理"></a>X射线衍射方法测定应力的原理</h4><p>对于一般的金属材料，X射线的穿透能力很低，所以仅能测定表面层的应力。又垂直于表面层的应力的总和为0.也即$\sigma_3 &#x3D; 0$</p>
<ul>
<li><p>广义胡克定律在此前提下简化为：</p>
</li>
<li><p>$$<br>\left{\begin{array}{l}<br>\varepsilon_1&#x3D;\frac{1}{E}\left(\sigma_1-\nu \sigma_2\right) \<br>\varepsilon_2&#x3D;\frac{1}{E}\left(\sigma_2-\nu \sigma_1\right) \<br>\varepsilon_3&#x3D;-\frac{\nu}{E}\left(\sigma_1+\sigma_2\right)<br>\end{array}\right.<br>$$</p>
</li>
<li><p>带入$\varepsilon_{\phi \psi}&#x3D;\alpha_1^2 \varepsilon_1+\alpha_2^2 \varepsilon_2+\alpha_3^2 \varepsilon_3$化简即得到：</p>
</li>
<li><p>$$<br>\varepsilon_{\phi \psi}&#x3D;\frac{1+\nu}{E} \sigma_\phi \sin ^2 \psi-\frac{\nu}{E}\left(\sigma_1+\sigma_2\right)<br>$$</p>
<ul>
<li>注意$\sigma_\phi &#x3D; cos^2\phi \sigma_1 +sin^2 \phi \sigma_2$</li>
</ul>
<p>将上式对 $\sin ^2 \psi$ 求导, 就可以解出表面上任一方向上的应力 $\sigma_\phi$, 有：<br>$$<br>\sigma_\phi&#x3D;\frac{E}{1+\nu} \frac{\partial \varepsilon_{\phi \psi}}{\partial \sin ^2 \psi}<br>$$</p>
</li>
</ul>
<h3 id="衍射仪法测定宏观应力"><a href="#衍射仪法测定宏观应力" class="headerlink" title="衍射仪法测定宏观应力"></a>衍射仪法测定宏观应力</h3><h2 id="微晶尺寸和微观应力"><a href="#微晶尺寸和微观应力" class="headerlink" title="微晶尺寸和微观应力"></a>微晶尺寸和微观应力</h2><h3 id="量子力学的不确定性原理"><a href="#量子力学的不确定性原理" class="headerlink" title="量子力学的不确定性原理"></a>量子力学的不确定性原理</h3><ol>
<li><p><strong>波动性和离散性</strong>：X射线作为电磁波，具有波动性。在量子尺度下，波动性导致了能量和动量的不确定性。这种不确定性在微观尺度上对物质的行为有重要影响。</p>
</li>
<li><p><strong>晶体尺寸与不确定性</strong>：在小尺寸晶体中，晶格面的数量减少意味着晶体中可用于衍射的晶格点减少。根据不确定性原理，较少的晶格点导致衍射角度的不确定性增大，从而允许在布拉格角附近的较宽角度范围内发生衍射。</p>
</li>
</ol>
<h3 id="晶格面排列的影响"><a href="#晶格面排列的影响" class="headerlink" title="晶格面排列的影响"></a>晶格面排列的影响</h3><ol>
<li><p><strong>晶格面间距的变化</strong>：在大晶体中，由于晶格面数量众多，晶格面间距非常均匀，衍射条件非常严格。但在小晶体中，晶格面的间距可能因为晶体边界效应而略有不均匀。</p>
</li>
<li><p><strong>衍射角度的变化</strong>：晶格面间距的这种轻微不均匀性可能导致衍射角度略有变化，从而使得衍射不再严格局限于理想的布拉格角。</p>
</li>
</ol>
<h4 id="微晶尺寸的测定"><a href="#微晶尺寸的测定" class="headerlink" title="微晶尺寸的测定"></a>微晶尺寸的测定</h4><h4 id="微观应力的测定"><a href="#微观应力的测定" class="headerlink" title="微观应力的测定"></a>微观应力的测定</h4><h2 id="织构的测定"><a href="#织构的测定" class="headerlink" title="织构的测定"></a>织构的测定</h2><h3 id="织构的分类"><a href="#织构的分类" class="headerlink" title="织构的分类"></a>织构的分类</h3><ul>
<li>丝织构<ul>
<li>丝织构的晶体学特点是晶粒的某一个或者某几个晶向倾向于平行试样的某一特定方向，一般沿丝轴方向或生长方向</li>
</ul>
</li>
<li>板织构<ul>
<li>板织构的晶体学特征是各晶粒的某一个或几个晶面平行于试样的某一特定面(如轧面),一个或几个晶向平行于试样的某一特定方向(如轧向)。</li>
</ul>
</li>
</ul>
<h3 id="织构的表示方法"><a href="#织构的表示方法" class="headerlink" title="织构的表示方法"></a>织构的表示方法</h3><ul>
<li>正级图</li>
<li>反级图</li>
</ul>
<h1 id="电子显微分析"><a href="#电子显微分析" class="headerlink" title="电子显微分析"></a>电子显微分析</h1><h3 id="第三章-电子与物质的相互作用"><a href="#第三章-电子与物质的相互作用" class="headerlink" title="第三章 电子与物质的相互作用"></a>第三章 电子与物质的相互作用</h3><h4 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h4><p>电子束穿过薄样品，可能产生的各种信息</p>
<ul>
<li>二次电子</li>
<li>特征X射线</li>
<li>可见光</li>
<li>电子-空穴对</li>
<li>韧致辐射X射线</li>
<li>非弹性散射电子</li>
<li>透射电子</li>
<li>弹性散射电子</li>
<li>吸收电子</li>
<li>俄歇电子</li>
<li>背散射电子</li>
</ul>
<h4 id="电子的弹性散射"><a href="#电子的弹性散射" class="headerlink" title="电子的弹性散射"></a>电子的弹性散射</h4><h3 id="第四章-电子衍射"><a href="#第四章-电子衍射" class="headerlink" title="第四章 电子衍射"></a>第四章 电子衍射</h3><h4 id="电子衍射原理"><a href="#电子衍射原理" class="headerlink" title="电子衍射原理"></a>电子衍射原理</h4><ul>
<li><p>再次说明有关倒易点阵</p>
<ul>
<li>设正空间基矢为$a,b,c$,倒易空间基矢为$a^*,b^*,c^*$</li>
</ul>
</li>
<li><p>衍射花样与晶体几何关系</p>
<img src="C:\Users\wenjie\AppData\Roaming\Typora\typora-user-images\image-20231204111558999.png" alt="image-20231204111558999" style="zoom:50%;" />

<ul>
<li>注：由于满足布拉格定律的角度$\theta$很小，$tan2\theta$,$sin2\theta$均近似为$\theta$</li>
<li>故$rd&#x3D;L\lambda$</li>
</ul>
</li>
</ul>
<h4 id="倒易点阵平面及其画法"><a href="#倒易点阵平面及其画法" class="headerlink" title="倒易点阵平面及其画法"></a>倒易点阵平面及其画法</h4><ul>
<li>晶带定律<ul>
<li>许多晶面族同时与一个晶体学方向$[uvw]$平行时，这些晶面族统称为一个晶带，$[uvw]$称为晶带轴</li>
<li>进而可以推断，晶面族倒易矢量构成与晶带轴正交的二维倒易点阵平面</li>
<li>若晶带轴用正空间矢量表示，晶面由倒易空间矢量表示，则二者点积为0</li>
<li>由几何关系，要确定晶带轴方向，取两个晶面即可</li>
</ul>
</li>
<li>二维倒易点阵平面的画法<ul>
<li>直接由试探法找出倒易平面的一个点，之后如果为立方晶系，则依据90度关系直接解出下一个点</li>
<li>有了两个点以后，相加得到第三个点并向所有方向扩展即可</li>
</ul>
</li>
</ul>
<h4 id="选区电子衍射"><a href="#选区电子衍射" class="headerlink" title="选区电子衍射"></a>选区电子衍射</h4><h4 id="多晶电子衍射花样和相机长度标定"><a href="#多晶电子衍射花样和相机长度标定" class="headerlink" title="多晶电子衍射花样和相机长度标定"></a>多晶电子衍射花样和相机长度标定</h4><h4 id="单晶电子衍射花样的分布"><a href="#单晶电子衍射花样的分布" class="headerlink" title="单晶电子衍射花样的分布"></a>单晶电子衍射花样的分布</h4><h4 id="其他电子衍射谱"><a href="#其他电子衍射谱" class="headerlink" title="其他电子衍射谱"></a>其他电子衍射谱</h4><h3 id="第五章-复杂电子衍射谱"><a href="#第五章-复杂电子衍射谱" class="headerlink" title="第五章 复杂电子衍射谱"></a>第五章 复杂电子衍射谱</h3><h4 id="孪晶电子衍射谱"><a href="#孪晶电子衍射谱" class="headerlink" title="孪晶电子衍射谱"></a>孪晶电子衍射谱</h4><h4 id="高阶劳厄带电子衍射谱"><a href="#高阶劳厄带电子衍射谱" class="headerlink" title="高阶劳厄带电子衍射谱"></a>高阶劳厄带电子衍射谱</h4><h4 id="菊池线分析"><a href="#菊池线分析" class="headerlink" title="菊池线分析"></a>菊池线分析</h4>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/11/30/wei-ming-ming/</url>
    <content><![CDATA[<p>让大模型成为RL Agent<br>mimictable_processed.csv<br>icu_stay id 可以视作不同的病人<br>step<br>gender<br>补充空值：阶跃响应<br>数值改为自然语言单位<br>Customized Dataset<br>Input Action Reward<br>转Prompt CoT   四个决策任选<br>时序，后面的Step需要前面的信息，以及前面的决策<br>Few Shot 选择方案<br>大模型会输出确定方案<br>sample wise 距离<br>Tianshou Buffer<br>读代码HD4RL-OPE-utils<br>HD4RL-core base_obj val_OPE 400 继承Tianshou policy 继承，改写forward<br>Metrics </p>
<ul>
<li>监督学习，效果不佳，复刻医生决策<ul>
<li>医生并不optimal</li>
<li>学不到好的</li>
<li>监督学习基础下，比较大模型和监督学习</li>
</ul>
</li>
<li>![[Pasted image 20231130173523.png]]</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>深度学习</title>
    <url>/2023/09/19/shen-du-xue-xi/</url>
    <content><![CDATA[<span id="more"></span>
<p>正则项目的是减小参数以进一步减少噪声的放大，其并不作用于b上，因为b不与x相乘，不会放大噪声。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><h3 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a>Convolutional Layer</h3><ul>
<li>Three shapes of convolution<ul>
<li>valid即全程完整的$f$和$g$进行卷积</li>
<li>full即从一格开始进行卷积</li>
<li>same即通过两侧$f$补零，使得卷积结果和$f$一样长</li>
</ul>
</li>
<li>卷积即$f$与$\tilde{g}$ 的相似度（标准长度）</li>
<li>1-D和2-D都是旋转180°再计算相似度即可</li>
<li>局部链接、权值共享<ul>
<li>举例来说，32*32的</li>
</ul>
</li>
</ul>
<h3 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>one-hot编码向量化及还原</title>
    <url>/2023/09/12/du-re-bian-ma-xiang-liang-hua-ji-biao-qian-huan-yuan/</url>
    <content><![CDATA[<p>打kaggle遇到的一些细节</p>
<span id="more"></span>

<ul>
<li><p>有时候需要将特征中的文本等进行独热编码处理，但是pd.get_dummies函数有时会默认为bool，不便转为torch.tensor。这时候可以选择指定为uint8，这样独热编码就是0、1之类的了。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = pd.get_dummies(all_features,dummy_na=<span class="literal">True</span>, dtype=<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">trainset = torch.tensor(all_features[:<span class="number">1235</span>].values,dtype=torch.float32).cuda()</span><br></pre></td></tr></table></figure>
</li>
<li><p>分类问题最后的标签独热编码之后可能需要还原以便提交。</p>
<p>此时就可以采用以下做法，提取出每行值为1的列（经过了softmax），之后再通过预先定义的map映射回分类指标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred=model(testset)</span><br><span class="line">prediction = []</span><br><span class="line">class_indices = torch.argmax(pred, dim=<span class="number">1</span>)</span><br><span class="line">class_mapping = &#123;<span class="number">0</span>: <span class="string">&#x27;died&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;euthanized&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;lived&#x27;</span>&#125;</span><br><span class="line">prediction = [class_mapping[i.item()] <span class="keyword">for</span> i <span class="keyword">in</span> class_indices]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Tricks</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/2023/09/19/ji-qi-xue-xi/</url>
    <content><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><span id="more"></span>

<h3 id="Big-Data-amp-Simple-Model"><a href="#Big-Data-amp-Simple-Model" class="headerlink" title="Big Data &amp; Simple Model"></a>Big Data &amp; Simple Model</h3><p>why machine learning feasible 可行的</p>
<h4 id="Example-pick-red-and-green-marbles-from-a-bin"><a href="#Example-pick-red-and-green-marbles-from-a-bin" class="headerlink" title="Example : pick red and green marbles from a bin"></a>Example : pick red and green marbles from a bin</h4><ul>
<li>$\mathbb{P}[red marbles] &#x3D; \mu \qquad \mathbb{P}[green marbles] &#x3D; 1-\mu$</li>
<li>Hoeffding’s inequality：$P[|\nu-\mu|&gt;\epsilon]\leq2e^{-2\epsilon^2N}$<ul>
<li>其中$\nu$  是取样中红色球的比例  </li>
<li>可见实验结果误差大于$\epsilon$的概率并不取决于期望，而取决于实验的次数$N$</li>
<li>该例子具有一定的特殊性，因为其为Brenoulli实验，且所考虑的$\nu$和$\mu$均在0到1之间</li>
<li>关键：big data gives an accurate learning result</li>
</ul>
</li>
<li>广义Hoeffding’s inequality：$$\mathbb{P}\left(\left|S_n-\mathbb{E}\left[S_n\right]\right| \geq t\right) \leq 2 \exp \left(-\frac{2 t^2}{\sum_{i&#x3D;1}^n\left(b_i-a_i\right)^2}\right)<br>  \tag{1}$$<ul>
<li>$b_i ,a_i$分别为变量$X_i$的上限和下限</li>
<li>注意到$S_n$是变量之和，如果要与伯努利情况下的Koeffding比较，则可以考虑指数上下同除$n^2$</li>
<li>在广义的情况下考虑本例，$b_i-a_i&#x3D;1$，代入即得本例情况</li>
</ul>
</li>
</ul>
<h4 id="Example-拓展"><a href="#Example-拓展" class="headerlink" title="Example 拓展"></a>Example 拓展</h4><ul>
<li>将未知的$\mu$替换为$f(x)$，我们给出假设$h(x)$，从取球的角度来看，每一次取球，就相当于在定义域$X$中取一个$x$，取出红球意味着，$h(x)\ne f(x)$，而取出绿球则意味着$h(x) &#x3D; f(x)$ </li>
<li>在上述前提下，定义$E_{in}(h)$作为抽样错误率，也即“in sample error”，同时定义$E_{out}(h)$作为总错误率，也即对所有利用假设进行判断后$h(x) \ne f(x)$ 的比率。其实也就是之前所述的$\nu$和$\mu$，我们想要知道我们的假设$h$效果怎么样，但是没必要把所有$x$全带入。</li>
<li>$h$的参数空间记为$H$ ，其中最优的假设是$g$，经过训练后，$g$会尽可能接近$f$ </li>
<li>$$\mathbb{P}\left[\left|E_{\text {in }}(g)-E_{\text {out }}(g)\right|&gt;\epsilon\right] \leq \sum_{m&#x3D;1}^M \mathbb{P}\left[\left|E_{\text {in }}\left(h_m\right)-E_{\text {out }}\left(h_m\right)\right|&gt;\epsilon\right]$$</li>
<li>$$\mathbb{P}\left[\left|E_{\text {in }}(g)-E_{\text {out }}(g)\right|&gt;\epsilon\right] \leq 2 M e^{-2 \epsilon^2 N}<br>\tag{2}$$</li>
<li>这条式子的意思是，给出所有$M$个假设中，最优的一个，训练集和实际集的误差上限（这本身不能让$g$接近$f$）这很好理解，因为右侧包含左侧。<blockquote>
<p>Low complexity model and big data can give us a good generalization in machine learning</p>
</blockquote>
</li>
</ul>
<h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><blockquote>
<p>Use labelled dataset to train algorithms</p>
</blockquote>
<h4 id="Regression-Ⅰ"><a href="#Regression-Ⅰ" class="headerlink" title="Regression Ⅰ"></a>Regression Ⅰ</h4><ul>
<li>Linear Regression<ul>
<li>$y_{i}&#x3D;f_{\beta_{0},\beta_{1}}(x_{i})+\epsilon_{i}&#x3D;\beta_{0}+x_{i}\beta_{1}+\epsilon_{i}\quad$其中$\epsilon$即为残差<ul>
<li>$f_{\beta_{0},\beta_{1}}$是线性回归的函数，训练目标是最小化$\epsilon$的平方和</li>
</ul>
</li>
<li>Cost Function<ul>
<li>$C\left(\beta_0,\beta_1\right)&#x3D;\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0,\beta_1}\left(x_i\right)\right)^2$</li>
</ul>
</li>
<li>Assumptions<ul>
<li>Weak exogeneity 假设残差的期望为0</li>
<li>Linearity 假设WX+b&#x3D;y</li>
<li>Homoscedasticity 假设因变量方差不随自变量改变<ul>
<li>假设我们通过房子的面积等预测房价，我们可以假设不管面积是大是小，价格的方差不会变。而事实上也存在反例，例如我们一般认为，小房子的价格波动更大</li>
</ul>
</li>
</ul>
</li>
<li>Ordinary least squares<ul>
<li>Cost Function $(y_{i}-f_{\beta_{0},\beta_{1}}(x_{i}))^2$ </li>
<li>对两个参数分别求偏导及二阶偏导，最小化损失函数</li>
<li>Coefficient of determination $r^2$<ul>
<li>$$r^2&#x3D;1-\frac{\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0, \beta_1}\left(x_i\right)\right)^2}{\sum_{i&#x3D;1}^m\left(y_i-\bar{y}\right)^2} \times 100 % \tag{3}$$</li>
<li>也即比较线性回归模型残差平方和和原有数据残差平方和（相对于$\bar{y}$）故$r^2$越大越优</li>
<li>定性看一下，直接拿均值肯定很烂，然后$r^2&#x3D;1$的话，说明完全拟合上了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multiple Linear Regression<ul>
<li>额外假设，不存在多重共线性</li>
<li>同样思路，计算一阶导数并令之为0</li>
<li>$$\begin{gathered}C(\boldsymbol{\beta})&#x3D;(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})&#x3D;\boldsymbol{y}^T \boldsymbol{y}+\boldsymbol{\beta}^T X^T X \boldsymbol{\beta}-2 \boldsymbol{\beta}^T X^T \boldsymbol{y} \\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}&#x3D;2 X^T X \boldsymbol{\beta}-2 X^T \boldsymbol{y}\end{gathered}$$</li>
<li>据此给出 $\widehat{\boldsymbol{\beta}}$的表达式</li>
<li>$$\widehat{\boldsymbol{\beta}}&#x3D;\left(X^T X\right)^{-1} X^T \boldsymbol{y}$$</li>
<li>计算二阶导数</li>
<li>$$\mathcal{H}&#x3D;\frac{\partial^2 C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^2}&#x3D;2 X^T X$$</li>
<li>证明海森矩阵正定（$a$是任意给定非零向量）</li>
<li>$$\boldsymbol{a}^T X^T X \boldsymbol{a}&#x3D;(X \boldsymbol{a})^T X \boldsymbol{a}&#x3D;|X \boldsymbol{a}|_2^2 \geq 0$$</li>
<li>定义hat matrix</li>
<li>$$\widehat{\boldsymbol{y}}&#x3D;X \widehat{\boldsymbol{\beta}}&#x3D;X\left(X^T X\right)^{-1} X^T \boldsymbol{y}&#x3D;X\left(X^T X\right)^{-\mathbf{1}} X^T \boldsymbol{y}&#x3D;H \boldsymbol{y}$$</li>
<li>Properties of hat matrix<ul>
<li>$H$ 、$I-H$  均是正交投影矩阵</li>
<li>Idempotent $H&#x3D;H^2$、$(I-H)&#x3D;(I-H)^2$</li>
<li>残差由$\epsilon&#x3D;y-\hat{y}&#x3D;y-Hy&#x3D;(I-H)y$给出<ul>
<li>当然，残差的平方和$\epsilon^T \epsilon&#x3D;y^T(I-H)y$（依据幂等消掉了一个）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>最大似然估计<ul>
<li>Assumptions<ul>
<li>随机取样残差符合均值为0的高斯分布，在残差为0时，概率取最大值<ul>
<li>在假设残差符合高斯分布的前提下，假设其方差为$\sigma^2$</li>
</ul>
</li>
<li>$$p\left(\left(\boldsymbol{x}_{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}\right)&#x3D;\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}$$</li>
<li>复习的时候发现这里非常容易混淆，再次强调$p((x_i,y_i)|\beta)$其实指的是$x_i$在参数为$\beta$时给出的预测和真实的标签$y_i$的差距服从的分布</li>
<li>依据常规高斯分布公式分析，其实就是下式（对于残差，均值$\mu$为0）</li>
<li>$$p;(\left(\epsilon_i) \mid \boldsymbol{\beta}\right)&#x3D;\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(\epsilon_i-0)^2}{2 \sigma^2}}$$</li>
<li>$(x_i,y_i)$独立同分布</li>
</ul>
</li>
<li>最大似然估计最大化在所有取样点处的概率之和，这样约等于最小化残差</li>
<li>最大化以下式子：$$p\left(D \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;p\left(\left(\boldsymbol{x}<em>{\mathbf{1}}, y_1\right), \ldots\left(\boldsymbol{x}</em>{\boldsymbol{m}}, y_m\right) \mid \boldsymbol{\beta}, \sigma^2\right)$$</li>
<li>Calculate the expression of likelihood</li>
<li>Due to the assumption of independency$$p\left(\left(\boldsymbol{x}<em>1, y_1\right), \ldots\left(\boldsymbol{x}</em>{\boldsymbol{m}}, y_m\right) \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;\prod_{i&#x3D;1}^m p\left(\left(\boldsymbol{x}_{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}, \sigma^2\right)$$</li>
<li>Due to the assumption of Gaussian Distribution$$\prod_{i&#x3D;1}^m p\left(\left(\boldsymbol{x}<em>{\boldsymbol{i}}, y_i\right) \mid \boldsymbol{\beta}, \sigma^2\right)&#x3D;\prod</em>{i&#x3D;1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T \boldsymbol{x}_i\right)^2}{2 \sigma^2}}$$</li>
<li>Due to the assumption of homoscedasticity$$\prod_{i&#x3D;1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}&#x3D;\left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)^m e^{-\frac{\sum_{i&#x3D;1}^m\left(y_i-\boldsymbol{\beta}^T x_i\right)^2}{2 \sigma^2}}$$</li>
<li>Calculate the Log-likelihood$$\mathcal{L}\left(\boldsymbol{\beta}, \sigma^2\right)&#x3D;-\frac{m}{2} \ln \left(2 \pi \sigma^2\right)-\frac{1}{2 \sigma^2}(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})$$</li>
<li>到此步为止，为了最大化似然，需要最小化$\frac{m}{2} \ln \left(2 \pi \sigma^2\right)+\frac{1}{2 \sigma^2}(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})$ 也即最小化$(\boldsymbol{y}-X \boldsymbol{\beta})^T(\boldsymbol{y}-X \boldsymbol{\beta})\quad$这与多元线性回归类似（最小化残差平方和）</li>
<li>分别对$\beta$和$\sigma^2$求偏导，注意得到的$\sigma$是残差的标准差，又由于同分布假设，故假设最后的结果是$y_{predict}$则95%置信区间为$y_{predict}\pm 1.96\sigma$</li>
</ul>
</li>
</ul>
<h2 id="Gradient-Descent-Ⅰ"><a href="#Gradient-Descent-Ⅰ" class="headerlink" title="Gradient Descent Ⅰ"></a>Gradient Descent Ⅰ</h2><h3 id="Illustration-1D"><a href="#Illustration-1D" class="headerlink" title="Illustration(1D)"></a>Illustration(1D)</h3><p>对于1-D线性回归模型，我们可以发现，损失函数是一个下凸函数，所以我们可以先随机选定一组参数$(\beta_0,\beta_1)$ 相当于在碗的壁上放了一个乒乓球，之后按以下策略，迭代参数$(\beta_0,\beta_1)$<br>$$<br>\begin{gathered}<br>\beta_1^{(i+1)}&#x3D;\beta_1^{(i)}-\alpha \frac{\partial C\left(\beta_0^{(i)}, \beta_1^{(i)}\right)}{\partial \beta_1}&#x3D;\beta_1^{(i)}+2 \alpha \sum_{i&#x3D;1}^m\left(y_i-\beta_1^{(i)} x_i-\beta_0^{(i)}\right) x_i \<br>\beta_0^{(i+1)}&#x3D;\beta_0^{(i)}-\alpha \frac{\partial C\left(\beta_0^{(i)}, \beta_1^{(i)}\right)}{\partial \beta_0}&#x3D;\beta_0^{(i)}+2 \alpha \sum_{i&#x3D;1}^m\left(y_i-\beta_1^{(i)} x_i-\beta_0^{(i)}\right)<br>\end{gathered}<br>$$</p>
<ul>
<li>注意到有 $C\left(\beta_0,\beta_1\right)&#x3D;\sum_{i&#x3D;1}^m\left(y_i-f_{\beta_0,\beta_1}\left(x_i\right)\right)^2$</li>
<li>注意区分在优化问题中的参数$x$以及具体的数据点$x$两者容易搞混</li>
</ul>
<blockquote>
<p>第一讲到此结束</p>
</blockquote>
<hr>
<h2 id="Gradient-Descent-Ⅱ"><a href="#Gradient-Descent-Ⅱ" class="headerlink" title="Gradient Descent Ⅱ"></a>Gradient Descent Ⅱ</h2><blockquote>
<p>For General functions, the gradient descent algorithm is often stuck at a local minimum</p>
</blockquote>
<ul>
<li>数值方法求解梯度<ul>
<li>$\partial f&#x2F; \partial x &#x3D; (f(x+\epsilon)-f(x))&#x2F;\epsilon$</li>
</ul>
</li>
<li>Line search<ul>
<li>Exact Line Search<ul>
<li>$a_i^{*} &#x3D; argmin_{a_i&gt;&#x3D;0}f(x_i+a_i\Delta x_i)$</li>
<li>在一定范围内搜索$\alpha$让$f$最小</li>
<li>问题在于操作繁琐计算需求大，主要是为了找一个让函数沿着当前方向下降最大的步长</li>
</ul>
</li>
<li>Backtracking Search<ul>
<li>核心思想是“你至少下降这么多我才可以接受”，核心目标是确定一个还算不错的步长</li>
<li>设置一个值$\gamma$衡量所期望的函数下降的程度，同时设置一个值$\rho$ 来逐步缩小步长直到符合要求</li>
<li>目标是比较$f(x+\alpha \Delta x)$与$f(x)+\alpha \gamma \nabla f(x)^T \Delta x$，如果当前选择的步长未能使函数下降至小于期望，则将步长$\alpha$缩小为$\rho \alpha$ </li>
<li>$Wolfe ; 1^{st} ; and ; Wolfe ; 2^{nd} ; condition$<ul>
<li>第一条件，也就是在回溯线搜索中用到的条件，限制步长不要太长，以至于错过local minimum</li>
<li>第二条件，约束 $\frac{\phi^{\prime}(a)}{\phi^{\prime}(0)}&#x3D;\frac{\nabla f(x+a\Delta x)^{T}\Delta x}{\nabla f(x)^{T}\Delta x}\leq \eta$也即要求新的位置的斜率必须相较最初变得一定程度上平缓，这使得了步长不会过短<ul>
<li>注意到如果原地不动$\eta&#x3D;1$，如果移动至局部最小值，$\eta &#x3D; 0$所以一般取$\eta$在0到1之间</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Newton’s method<ul>
<li>牛顿法本用来通过数值方法找到函数根的近似解，但也可以稍加变化使之可以应用在梯度下降法中，即将求根的对象从$f$变为$f^{‘}$  </li>
<li>公式可写作$x_{i+1}&#x3D;x_{i}-\frac{f’(x_{i})}{f’’(x_{i})}$</li>
<li>在一维情况下阐述工作原理<ul>
<li>首先我们在某点计算目标函数的梯度和Hessian，之后用这些信息形成二阶泰勒展开，得到二次曲线</li>
<li>之后我们找到二次曲线的最小值位置，更新$x_{k+1}$</li>
<li>参考公式如下$f(x)\approx f(x_{i})+\boldsymbol{g}^{T}(x-\boldsymbol{x}<em>{i})+\frac{1}{2}(\boldsymbol{x}-\boldsymbol{x}</em>{i})^{T}H(\boldsymbol{x}-\boldsymbol{x}_{i})$ </li>
<li>一张很有启发意义的图</li>
<li>![[Pasted image 20231102110212.png]]</li>
<li>伪代码可以理解为$x_{i+1} &#x3D; x_i -H^{-1}g$</li>
<li>补充说明<ul>
<li>如果H并不是正定的，有两种方法解决此问题<ul>
<li>采用Dk矩阵替换$H^{-1}$对角元素，具体表达式如下$D_{k}(i,i)&#x3D;\max\left(\varepsilon,\frac{\partial f^{2}}{\partial x_{i}^{2}}\right)^{-1}$ </li>
<li>或采用LMA，在Hessian非正定的情况下，退化为gradient descent</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Stochastic gradient descent<ul>
<li>在大数据量的背景下，很难利用所有数据计算Cost Function</li>
<li>解决方案：Stochasitically sample from the dataset.</li>
<li>对比Linear regression背景下，GD和SGD的区别与联系<ul>
<li>GD略，见前</li>
<li>SGD的Cost Function只对一个点计算，每计算一个点就更新所有参数权重，一般对全数据做1到10次扫描，假设数据量为m则，则一轮扫描就对参数进行了m次更新，而GD此时只进行了一次更新</li>
<li>优势：快</li>
<li>劣势<ul>
<li>失去了向量化的优势，现代的硬件，例如CPU或者GPU对向量化运算进行了优化，能够同时对数组或向量的多个元素执行相同的操作，从而大大提高运算效率，但是SGD舍弃了这种优势</li>
<li>不是所有的迭代都朝向最优方向</li>
<li>收敛速度较慢，在最优解附近可能会发生震荡等<ul>
<li>解决方案，调整学习率为$\frac{C_1}{i+C_2}$，其中$C_1,C_2$均常数，$i$为迭代次数，这可以使得学习率逐渐下降，最终在最优解附近波动较小</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Mini-batch<ul>
<li>原理和思路类似SGD，但是不是取单一点而是取一组k个点，提高了稳定性，同时也保证了相对较快的速度和较小的计算资源消耗</li>
</ul>
</li>
<li>Momentum method<ul>
<li>提出的根本原因是一般的梯度下降由于震荡问题不能引入太大的$\alpha$，而且不具有记忆</li>
<li>对比一般梯度下降和动量梯度下降<ul>
<li>一般梯度下降$x^{(i+1)}&#x3D;x^{(i)}-\alpha\nabla f(x^{(i)})$</li>
<li>而动量梯度下降表达式为：<ul>
<li>$\boldsymbol{x}^{(i+1)}&#x3D;\boldsymbol{x}^{(i)}+\boldsymbol{v}^{(i)}$</li>
<li>$\boldsymbol{v}^{(i)}&#x3D;-\alpha\nabla f\big(\boldsymbol{x}^{(i)}\big)+\boldsymbol{\theta}\boldsymbol{v}^{(i-1)}$</li>
</ul>
</li>
<li>动量梯度下降利用速度项更新$x$，而速度项$v$和梯度以及上一时刻的速度$v_{i-1}$有关，相当于保有了“记忆”</li>
<li>事实上$v_i&#x3D;-\alpha\sum_{k&#x3D;0}^{i}\theta^{k}\nabla f(x^{(i-k)})$ 对之前的的速度进行了指数衰减级的记忆</li>
</ul>
</li>
<li>补充知识：条件数Condition Number<ul>
<li>定义，$k&#x3D;(\frac{a}{b})^2$</li>
<li>一般GD $O(klog(\frac{1}{\epsilon}))$</li>
<li>一般SGD $O(\sqrt{k}log(\frac{1}{\epsilon}))$</li>
</ul>
</li>
</ul>
</li>
<li>Nesterov’s accelerated<ul>
<li>尝试解决动量梯度下降存在的，在局部最优点可能具有较大速度导致错过最优点的现象</li>
<li>原理是通过在速度项中的梯度项中，将原来的参数空间$x_i$改为“下一步的位置”，也即$x_i\theta v_{i-1}$，起到了“看前一步”的效果</li>
</ul>
</li>
<li>AdaGrad&#x2F;RMSprop<ul>
<li>AdaGrad允许学习率随着参数调整，他的宗旨是，对于梯度大的参数，我们适当减小学习率，对于梯度小的参数，我们维护一个较大的学习率<ul>
<li>$x_j^{(i+1)}&#x3D;x_j^{(i)}-\frac{\alpha}{\sqrt{g_j^{(i)}+\epsilon}}\Delta x_j^{(i)}$ </li>
<li>$g_{j}^{(i)}&#x3D;g_{j}^{(i-1)}+\left(\Delta x_{j}^{(i)}\right)^{2}$</li>
<li>这里的$g_j$维护的是该参数的历史梯度平方和，一定程度上可以反映该参数的历史梯度情况</li>
</ul>
</li>
<li>RMSprop与AdaGrad逻辑相似，但是对于历史梯度的记忆是随时间衰减的<ul>
<li>$x_{j}^{(i+1)}&#x3D;x_{j}^{(i)}-\frac{\alpha}{\sqrt{l_{j}^{(i)}}+\epsilon}\Delta x_{j}^{(i)}$ </li>
<li>$l_{j}^{(i)}&#x3D;(1-\varphi)l_{j}^{(i-1)}+\varphi\left(\Delta x_{j}^{(i)}\right)^{2}$ </li>
<li>其中$l_j$保留了梯度平方历史的加权和，并按某参数衰减，类似Momentum GD对于历史速度的处理方法</li>
</ul>
</li>
</ul>
</li>
<li>Adam(Momentum&amp;RMSprop) Nadam(Nesterov’s accelerated&amp;RMSprop)</li>
</ul>
<h3 id="Induction-amp-Deduction"><a href="#Induction-amp-Deduction" class="headerlink" title="Induction &amp; Deduction"></a>Induction &amp; Deduction</h3><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ul>
<li>Ridge Regression<ul>
<li><p>在线性回归模型中，较大的参数往往会让模型变得更为敏感，我们一般添加一些正则化项惩罚$w$项的长度</p>
</li>
<li><p>一个添加了$L2$正则化处罚项的损失函数一般类似于</p>
</li>
<li><p>$$C(\boldsymbol{w})&#x3D;\frac{1}{2}\sum_{n&#x3D;1}^{N}\left(\boldsymbol{x}<em>{n}^{T}\boldsymbol{w}-y</em>{n}\right)^{2}+\frac{\lambda}{2}|w|_{2}^{2}&#x3D;\frac{1}{2}(X\boldsymbol{w}-\boldsymbol{y})^{T}(X\boldsymbol{w}-\boldsymbol{y})+\frac{\lambda}{2}w^{T}\boldsymbol{w}$$</p>
</li>
<li><p>当然，梯度也要随之更改：</p>
</li>
<li><p>$$\nabla C(\mathbf{w})&#x3D;\nabla\left{\frac{1}{2}(X\mathbf{w}-\mathbf{y})^{T}(X\mathbf{w}-\mathbf{y})+\frac{\lambda}{2}\mathbf{w}^{T}\mathbf{w}\right}&#x3D;0$$</p>
</li>
<li><p>解得：$\boldsymbol{w}^*&#x3D;(X^TX+\lambda I)^{-1}X^T\boldsymbol{y}$</p>
<ul>
<li>这个解是符合逻辑的，如果去除正则项，则为$\boldsymbol{w}^*&#x3D;(X^TX)^{-1}X^T\boldsymbol{y}$</li>
</ul>
</li>
<li><p>应用了正则化以后，training loss会变差，这是自然的，因为引入了新的损失项目，导致模型更难过拟合以下降training loss</p>
</li>
<li><p>必须指出当$\lambda$上升时，会导致函数趋近于$f(x)&#x3D;0$以减少惩罚</p>
</li>
<li><p>但是不会将任何系数压缩到0，而是将它们都逼近0，会在模型中保留所有特征，尽管有些值会很小</p>
</li>
</ul>
</li>
<li>Lasso回归<ul>
<li>与岭回归不同的是，Lasso回归采用L1正则项</li>
<li>$$C(\boldsymbol{w})&#x3D;{\frac12}\sum_{n&#x3D;1}^N(x_n^T\boldsymbol{w}-y_n)^2+{\frac\lambda2}|w|$$</li>
<li>Lasso可以实现特征选择，参考下图，蓝绿色区域是正则化误差，而红线是平方误差等高线，Lasso在坐标轴上取得了理想的解，一个特征因此被舍弃了</li>
<li>![[Pasted image 20231104102253.png]]</li>
<li>也可以这样比较</li>
<li>![[Pasted image 20231104103346.png]]</li>
<li>可以发现随着$\lambda$的增大，Lasso回归的x轴系数最终变为0</li>
</ul>
</li>
<li>Elastic-net regression（弹性网络回归）<ul>
<li>实际上就是将Lasso和Rigid结合</li>
<li>$$\frac{1}{2}\sum_{n&#x3D;1}^{N}\bigl(x_{n}^{T}w-y_{n}\bigr)^{2}+\frac{\lambda_{1}}{2}|w|+\frac{\lambda_{2}}{2}|w|_{2}^{2}$$<blockquote>
<p>相关性并不意味着因果性，使用回归技术并不意味着特征与标签有因果关系</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="Supervised-learning-1"><a href="#Supervised-learning-1" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ul>
<li>Logistic regression<ul>
<li>即训练一组参数W、b，使得WX+b带入sigmoid函数可以区分是两类中的哪一类。一般使用梯度上升，最大化似然函数</li>
<li>Sigmoid function<ul>
<li>$$g(z)&#x3D;\frac{1}{1+e^{-z}}$$</li>
<li>Property Ⅰ  $g^{‘}(z)&#x3D; g(z)(1-g(z))$</li>
<li>Property Ⅱ  $1-g(z)&#x3D;g(-z)$</li>
</ul>
</li>
<li>Maximum likelihood estimation<ul>
<li>其实就是在给定参数$\beta$下，数据点$x_i$观察到$y_i$标签的概率，严格来说应该是给定$\beta$在给定$x_i$模型返回$y_i$的概率</li>
<li>不妨假设$y_i$服从伯努利分布，则$m$条数据的概率表达式为</li>
<li>$$\prod_{i&#x3D;1}^{m}p((x_{i},y_{i})|\boldsymbol{\beta})&#x3D;\prod_{i&#x3D;1}^{m}f_{\boldsymbol{\beta}}(x_{i})^{\nu_{i}}(1-f_{\boldsymbol{\beta}}(x_{i}))^{1-\nu_{i}}$$<ul>
<li>注记一下，这里假设$y_i$有0和1两种可能，所以直接放在指数上了</li>
</ul>
</li>
<li>综上，最大似然函数的对数是</li>
<li>$$\mathcal{L}(\boldsymbol{\beta})&#x3D;\log\left(\prod_{i&#x3D;1}^mp((\boldsymbol{x}<em>i,y_i)|\boldsymbol{\beta})\right)&#x3D;\sum</em>{i&#x3D;1}^my_i\log\left(f_\beta(x_i)\right)+(1-y_i)\log\left(1-f_\beta(x_i)\right)$$<ul>
<li>我们的目标就是找到这样一个$\beta$使得$\mathcal{L}(\beta)$最大<ul>
<li>其实也就是提高全部预测准确的概率</li>
</ul>
</li>
<li>在我们使用了$sigmoid$函数的前提下，公式又可以如下转化</li>
<li>$$\boldsymbol{\beta}^{*}&#x3D;\underset{\boldsymbol{\beta}}{\mathrm{argmax}}\sum_{i&#x3D;1}^{m}y_{i}\log\Big(g(\boldsymbol{\beta}^{T}x_{i})\Big)+(1-y_{i})\mathrm{log}\Big(1-g\big(\boldsymbol{\beta}^{T}x_{i}\big)\Big)$$</li>
<li>如果我们应用SGD求解，则最终的更新公式为</li>
<li>$$\left.\beta^{(k+1)}&#x3D;\beta^{(k)}+\alpha\left[y_i-g\left(\beta^{(k)}\right.^Tx_i\right)\right]x_i$$</li>
<li>Logistic regression的评价标准<ul>
<li>$$\frac{number, of, correctly, classified, }{number, of, total, training, data}$$</li>
<li>或者采用Efron’s</li>
<li>$$r^{2}&#x3D;1-\frac{\sum_{i&#x3D;1}^{m}(f(x_{i})-y_{i})^{2}}{\sum_{i&#x3D;1}^{m}(y_{i}-\bar{y})^{2}}$$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi-classes多分类问题<ul>
<li>Softmax<ul>
<li>公式如下</li>
<li>$$\mathrm{P}(y|\theta)&#x3D;\mathrm{P}(y|x,{\boldsymbol{\beta}<em>{k}}</em>{k&#x3D;1}^{K})&#x3D;\prod_{k&#x3D;1}^{K}\left(\frac{\exp{\boldsymbol{x}^{T}\boldsymbol{\beta}<em>{k}}}{\sum</em>{k^{\prime}&#x3D;1}^{K}\exp{\boldsymbol{x}^{T}\boldsymbol{\beta}<em>{k^{\prime}}}}\right)^{y</em>{k}}$$</li>
<li>注：对需要划分的每一个类训练一组参数（注意和深度学习区别，深度学习中一般在网络结构的最后一层统一使用softmax，并不会为每一类单独训练参数）</li>
<li>注：$y$是一个标签向量，一般采用了独热编码，例如在五分类问题中表示为$（0，0，1，0，0）$ 这一般表示为第三类。等式表示在给定k组参数和$x$的前提下，对某条数据预测正确的概率</li>
<li>注：以标签作为指数，其实就是取出了独热编码中$y_k$&#x3D;1的概率，只是形式上更为统一</li>
<li>综上，我们可以给出Log loss function</li>
<li>$$\log\prod_{i&#x3D;1}^{m}\mathrm{P}(\mathbf{y}<em>{i}|\mathbf{x}</em>{i},{\boldsymbol{\beta}<em>{k}}</em>{k&#x3D;1}^{K})&#x3D;\log\left[\prod_{i&#x3D;1}^{m}\prod_{k&#x3D;1}^{K}\left(\frac{\exp(\boldsymbol{x}<em>{i}^{T}\boldsymbol{\beta}</em>{k})}{\sum_{k^{\prime}&#x3D;1}^{K}\exp(\boldsymbol{x}<em>{i}^{T}\boldsymbol{\beta}</em>{k^{\prime}})}\right)^{y_{i,k}}\right]$$</li>
<li>以及其对每组参数的梯度</li>
<li>$$&#x3D;\sum_{i&#x3D;1}^{m}[y_{i,k}-\theta_{k}]x_{i}$$</li>
</ul>
</li>
<li>concave(上凸的)、convex(下凸的)</li>
<li>Prceptron<ul>
<li>通过多层感知机可以实现数据区域的分割</li>
<li>本质上来讲，单层感知机可以视作一个线性分类器，把空间按照一定的规则划分为两半</li>
<li>$$f_{A}(x)&#x3D;\mathrm{sgn}\left(\sum_{i&#x3D;0}^{d}\beta_{i}x_{i}\right)&#x3D;\mathrm{sgn}(\beta^{T}x)$$</li>
</ul>
</li>
</ul>
</li>
<li>GDA高斯判别分析<ul>
<li>与一般的Discriminative learning算法不同，GDA尝试学习$p(x|y)(and ; p(y))$ ,之后利用其去构建后验分布</li>
<li>整体来说，分类问题的目标是求$argmax_y;p(y|x)$这一点是不变的，但是GDA做了如下转化</li>
<li>$$\begin{aligned}\operatorname{argmax}_yp(y|x)&#x3D;\operatorname{argmax}_y\frac{p(x|y)p(y)}{p(x)}&#x3D;\operatorname{argmax}_yp(x|y)p(y)\end{aligned}$$</li>
<li>所以我们可以转为计算$p(y)$和$p(x|y)$，然后对他们的乘积找一个$y$使其最大化，下分别计算之</li>
<li>$p(y)&#x3D;\phi^{1{y&#x3D;\times}}(1-\phi)^{1-1{y&#x3D;\times}}$</li>
<li>$p(\boldsymbol{x}|y&#x3D;\times)&#x3D;\frac{1}{(2\pi)^{d&#x2F;2}|\Sigma|^{1&#x2F;2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_\times)^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_\times)\right)$</li>
<li>$p(x|y&#x3D;{0})&#x3D;\frac1{(2\pi)^{d&#x2F;2}|\Sigma|^{1&#x2F;2}}\exp\left(-\frac12(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)$</li>
<li>$\Sigma^T&#x3D;\frac1m\Sigma_{i&#x3D;1}^m\left(x^{(i)}-\mu_{y^{(i)}}\right)\left(x^{(i)}-\mu_{y^{(i)}}\right)^T$</li>
<li>$\phi&#x3D;\frac1m\sum_{i&#x3D;1}^m1{y^{(i)}&#x3D;\times}$</li>
</ul>
</li>
</ul>
<h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ul>
<li>Logistic Regression只有在数据本身分布比较理想，具有明确的界限时，效果才会比较良好</li>
<li>决策树<ul>
<li>连通无环结构</li>
<li>树的节点表示属性测试</li>
<li>叶节点表示分类结果</li>
<li>每一次划分其实是对空间进行一次轴对齐的超平面划分，树可以向图转换，图也可以还原成树</li>
</ul>
</li>
<li>决策树的建立<ul>
<li>建立最小决策树是NP问题，我们使用贪心算法作为一个较好的近似模型<ul>
<li>首先我们从空的决策树开始</li>
<li>选择最优特征，同时选择最优的特征阈值</li>
<li>在划分出来的新节点上继续上述循环，直到到达终止条件</li>
</ul>
</li>
<li>所以重点就是“Splitting Criteria”</li>
</ul>
</li>
<li>$Classification; Error$（可以用来评价划分的质量）<ul>
<li>$Error(i|j,t_{j}) &#x3D; 1-\max <em>{k}P(k|R</em>{i})$ </li>
<li>注意这里P不是概率，而是$R_i$中k的比例，比方说一个完美的分类，$R_i$中只有k，那么$Error$就是0</li>
<li>其中，$j$是划分的维度，$t_j$是划分的阈值</li>
<li>树当前的每一个叶子节点都代表了一块区域，而算法的继续进行需要做的，就是将该空间继续划分成两块</li>
<li>$\min _{j, t_j}\left{\frac{N_1}{N} \operatorname{Error}\left(1 \mid j, t_j\right)+\frac{N_2}{N} \operatorname{Error}\left(2 \mid j, t_j\right)\right}$<ul>
<li>区域被划分为两块，分别是$R_1$和$R_2$，将两块区域的误差加权相加即得到需要最小化的误差函数</li>
</ul>
</li>
</ul>
</li>
<li>我们也可以使用$Gini;index$ <ul>
<li>通过测试每个区域的”纯度”，来判断划分质量</li>
<li>$Gini(i|j,t_j)&#x3D;1-\Sigma_k P(k|R_i)^2$</li>
<li>再次提醒，$P(k|R_i)$是指$k$类在$R_i$中的比例</li>
<li>$Gini$值越低，划分质量越高，所以我们也要最小化$Gini$函数的加权和</li>
</ul>
</li>
<li>我们也可以使用$Entropy$<ul>
<li>$\operatorname{Entropy}\left(i \mid j, t_j\right)&#x3D;-\sum_k P\left(k \mid R_i\right) \log _2 P\left(k \mid R_i\right)$<ul>
<li>也就是说$j$分类器下，以$t_j$为阈值，$i$区域的熵是$i$区域内所有$k$个class的占比乘上其占比对2求对数的和的倒数</li>
</ul>
</li>
<li>当然，我们要最小化$\min_{j,t_j}\left{\frac{N_1}N\operatorname{Entropy}(1|j,t_j)+\frac{N_2}N\operatorname{Entropy}(2|j,t_j)\right}$<ul>
<li>含义也与之前类似，就是通过某种方式找到一个最优分类器和分类阈值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Stopping-Condition"><a href="#Stopping-Condition" class="headerlink" title="Stopping Condition"></a>Stopping Condition</h5><ul>
<li>如果不加以限制，决策树会不断进行划分，直到每个叶子都只包含一个节点为止，这也达成了训练集上的100%准确率<ul>
<li>为了避免以上情况，我们提出一些合理的停止策略</li>
<li>例如，我们可以限制决策树的深度</li>
<li>当某一区域内全都为同一种类时停止划分</li>
<li>对每一区域内的数量设置下限</li>
<li>对总叶子数量设置上限</li>
<li>计算划分收益，对收益设置下限<ul>
<li>注，收益一般可表示为</li>
<li>$\mathrm{Gain}(R)&#x3D;\Delta(R)&#x3D;m(R)-\frac{N_1}Nm(R_1)-\frac{N_2}Nm(R_2)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="For-Regression"><a href="#For-Regression" class="headerlink" title="For Regression"></a>For Regression</h5><blockquote>
<p>当然，决策树也可以用来做回归问题</p>
</blockquote>
<ul>
<li>同样是对于问题空间进行分割，分割后的每一块区域取其中元素的平均值作为回归决策树对于这一块数据的预测值</li>
<li>这一次，我们需要最小化的是区域内的点的值和我们预测的值（也就是我们框起来的区域的点的平均值）的差距，我们一般采用以下计算方法</li>
<li>$$\operatorname{argmin}_{j,t_j}\left{\frac{N_1}NMSE(R_1)+\frac{N_2}NMSE(R_2)\right}$$</li>
<li>或者</li>
<li>$$\text{ argmin}_{j,t_j}\left{\frac{N_1}NVar(y|x\in R_1)+\frac{N_2}NVar(y|x\in R_1)\right}$$</li>
<li>其实，如果我们将预测值选为区域内$y$的均值的话，二者结果是一样的</li>
</ul>
<h5 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h5><blockquote>
<p>说一下有关名字的由来，其实是Bootstrap aggregating的缩写，意思分别是“自助采样”和 “聚合”</p>
</blockquote>
<ul>
<li>较深的或者较大的决策树常常会过拟合，在新的数据集上有较大的方差</li>
<li>一般来说，我们可以采取以下方法<ul>
<li>对训练数据采样，对于每一组采样（一般来说，一个自助样本（有放回抽样的样本）大约包含原始数据集中约63.2%的唯一实例），训练一个决策树<ul>
<li>这里解释一下采样的相关问题</li>
<li>采样与数据集大小完全一样，但由于是有放回采样，所以依据公式计算，每个数据点不被选中的概率连续乘以自身N次，即$(1-\frac{1}{N})^N$ 趋近于 $e^{-1}$，也就是说，抽到与原始数据集一样大的话，包含着六成多一点的无重复数据</li>
</ul>
</li>
<li>之后对于一个给定的输入，我们将其放入所有的决策树并将结果取平均作为输出</li>
</ul>
</li>
<li>当然，这样做也存在着缺点，我们在提高模型表现的同时，损失了模型的可解释性</li>
</ul>
<h5 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h5><ul>
<li>为什么需要Random Forest<ul>
<li>bagging得到的树都是在同一个数据集中抽样得到的数据上训练的，而且采用的都是相同的贪心策略，这往往会导致生成的树在结构上非常相似，以至于会拥有极其类似的分布</li>
<li>当我们有B棵树，两两相关系数为$\rho$，自身方差为$\sigma^2$时，他们的均值的方差为$\rho\sigma^2+\frac{1-\rho}B\sigma^2$</li>
<li>从这个式子可以看出，即使在bagging算法中，我们使用了再大的B，也无法消除前一项的影响，而假设树之间的相关性较高，则优化程度有限，本质上是因为，决策树不独立</li>
</ul>
</li>
<li>每棵树每步分裂限制特征集，限制每一步的“视角”，在限制的特征集中选择最优的特征和阈值，最终融合不同视角进行投票<ul>
<li>实操上，我们在每一步分裂时，随机选择一组特征，在其中选择最优的特征</li>
<li>补充说明一些可以调整的超参数<ul>
<li>每次随机挑选的特征数量</li>
<li>随机森林中树的总数</li>
<li>叶结点的最小规模</li>
</ul>
</li>
</ul>
</li>
<li>如何进一步调整随机森林<ul>
<li>交叉验证<ul>
<li>随机森林天然可以交叉验证，因为我们使用一部分数据来构建决策树，剩下的数据就可以用来评估</li>
</ul>
</li>
<li>随机排列特征数据<ul>
<li>我们想要评估一个特征在整个随机森林中的重要性，我们可以对袋外数据中的某一个特征进行随机排列，然后测试决策树因为特征被随机排列而造成的计算准确度下降情况</li>
<li>对随机森林中所有的树做如上操作后，计算平均情况，即可得到该特征在整个随机森林中的平均重要性</li>
</ul>
</li>
</ul>
</li>
<li>随机森林仍然不是完美的算法<ul>
<li>如果特征的数量有很多，但是有用的特征很少的时候，随机森林的效果可能会很差，因为我们随机选择特征很容易选中很多没用的特征而漏选有用特征，构建出来的树很可能完全无法有效学习数据格式</li>
<li>树的数量极大时，又会回到树相关性提高，方差变大的问题</li>
</ul>
</li>
</ul>
<h5 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h5><blockquote>
<p>The strength of week learners</p>
</blockquote>
<ul>
<li>原理是，首先使用week classifier进行学习，之后依据预测结果，提高分类错误的数据点的权重，再此基础上再次训练一个分类器，并提高分类错误的数据点的权重。如此往复，在最后将所有的树分类器以一定的权重累加，得到强大的分类器</li>
</ul>
<h5 id="GBoosting"><a href="#GBoosting" class="headerlink" title="GBoosting"></a>GBoosting</h5><ul>
<li>GBoosting的概念比想象的还要深刻一点</li>
<li>用于回归的GBoosting原理如下<ul>
<li>分类器为$T$</li>
<li>先训练一个弱分类器$T_0$，并令$T&#x3D;T_0$</li>
<li>计算残差，将残差作为目标训练一个新的分类器$T_i$</li>
<li>将弱分类器以一定的参数加入$T$，残差会因此减少</li>
<li>$F_{m}(x)&#x3D;F_{m-1}(x)+\lambda\sum_{j&#x3D;1}^{J_{m}}\gamma_{jm}I\big(x\in R_{jm}\big)$<ul>
<li>$F_m(x)$是$m$轮迭代后，树给出的预测值，它的值等于上一轮结果加上一定比例的本轮训练（对残差）的预测结果</li>
<li>$\gamma_{jm}$的意思是，第$m$棵树中第$j$个区域的最优值，后面的$I\big(x\in R_{jm}\big)$是示性函数，判断$x$是否在第$m$棵树中第$j$个区域</li>
</ul>
</li>
<li>重复此步骤即可</li>
</ul>
</li>
<li>为什么叫Gradient Boosting？<ul>
<li>因为在MSE作为$Loss$函数时，残差同时也指出了$Loss$函数梯度的下降方向</li>
</ul>
</li>
<li>XGBoost是GBoosting的一种优化版本</li>
</ul>
<h3 id="一种视角"><a href="#一种视角" class="headerlink" title="一种视角"></a>一种视角</h3><img src="https://raw.githubusercontent.com/sunyrain/ForPicGo/main/img/image-20231111145655554.png" alt="image-20231111145655554" style="zoom: 67%;" />

<ul>
<li>我们该如何看待这张图呢？</li>
<li>其实这里展现的是经典的“偏差-方差”平衡</li>
<li>左侧展现的是欠拟合的情况，所以我们要通过Boosting的策略，去降低偏差</li>
<li>右侧则是过拟合的情况，也就是所谓的”高方差”<ul>
<li>在“偏差-方差”平衡的语境下，方差具体指的是模型预测相对于模型预测的期望值的变异度</li>
<li>高方差指的是在不同数据集上训练得到的模型，对于同一个输入点，得到的输出的方差</li>
<li>$\mathrm{Var}(x)&#x3D;E[(f(x;D)-E[f(x;D)])^2]$ 其中$D$指很多不同的数据集</li>
</ul>
</li>
<li>在过拟合的情况下，我们就需要引入Bagging去减小方差，乃至引入随机森林去限制特征，使集成的模型降低对特定样本和噪声的敏感性</li>
</ul>
<h5 id="Multivariate-splits"><a href="#Multivariate-splits" class="headerlink" title="Multivariate splits"></a>Multivariate splits</h5><ul>
<li>多变量线性组合的决策树</li>
<li>可以视作决策树的变种，区别在于不再平行于坐标轴划分空间，但是分类器和分类阈值的搜索成本也可能随之上升</li>
</ul>
<h4 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h4><blockquote>
<p>The most significant application that reflects the characteristics of decision trees </p>
<p>is often considered to be Medical Diagnosis</p>
</blockquote>
<ul>
<li>可解释性（让人信赖）、透明性（易于遵循）、层次决策（逐步评估）</li>
</ul>
<h2 id="K-nearest-neighbourhood"><a href="#K-nearest-neighbourhood" class="headerlink" title="K-nearest neighbourhood"></a>K-nearest neighbourhood</h2><blockquote>
<p>K近邻算法是一个用于回归与分类的非参方法</p>
</blockquote>
<ul>
<li><p>关键思想：一个样本的类别可以通过其K个最近邻居的类别来决定</p>
</li>
<li><p>也可以固定邻居数、固定搜索半径等</p>
</li>
<li><p>也可以使用soft boundary，距离点越近权重越高</p>
</li>
<li><p>但同时，测算点的距离对于计算机是较为繁琐的</p>
<ul>
<li><p>K-D Tree 方法，将数据在不同维度上划为分区，仅在test Point所在的分区计算K近邻</p>
<ul>
<li>维度的诅咒：在高维空间中，几乎所有点都相对较远</li>
</ul>
</li>
<li><p>Locality-Sensitive Hashing, LSH，采用超平面随机划分，在局部区域中计算最近邻</p>
</li>
</ul>
</li>
</ul>
<h5 id="Applications-1"><a href="#Applications-1" class="headerlink" title="Applications"></a>Applications</h5><blockquote>
<p>K-Nearest Neighbours can be applied to Recommendation Systems</p>
</blockquote>
<ul>
<li>个性化、用户相似度、懒惰学习</li>
</ul>
<p>​    </p>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><h3 id="Clustering（聚类）"><a href="#Clustering（聚类）" class="headerlink" title="Clustering（聚类）"></a>Clustering（聚类）</h3><h4 id="K-means（no-label）"><a href="#K-means（no-label）" class="headerlink" title="K-means（no label）"></a>K-means（no label）</h4><ul>
<li>首先在数据点中，随机选K个点作为聚类中心</li>
<li>将每个数据点分配到距离它最近的聚类中心</li>
<li>对于每个聚类，计算其所有数据点的平均位置，更新聚类中心的位置</li>
<li>直达聚类中心不发生变化或者达到预定的迭代次数</li>
</ul>
<h4 id="Hierarchical-clustering（层次聚类）"><a href="#Hierarchical-clustering（层次聚类）" class="headerlink" title="Hierarchical clustering（层次聚类）"></a>Hierarchical clustering（层次聚类）</h4><ul>
<li>Hierarchical agglomerative clustering（层次凝聚聚类）<ul>
<li>初始状态下，每个数据点都被认为是一个单独的聚类。</li>
<li>计算每对聚类之间的相似度或距离。</li>
<li>根据相似度或距离的度量规则，选择最相似的两个聚类进行合并。</li>
<li>合并后的聚类形成一个新的聚类，替代原来的两个聚类。</li>
<li>重复步骤，直到所有数据点都合并为一个聚类或达到预设的聚类数量。</li>
</ul>
</li>
<li>Divisive clustering（分裂聚类）<ul>
<li>初始状态下，所有数据点都被归为一个聚类。</li>
<li>计算当前聚类中的数据点的相似度或距离。</li>
<li>根据相似度或距离的度量规则，选择一个聚类进行分裂。</li>
<li>分裂所选的聚类，将其划分为两个或多个子聚类。</li>
<li>重复步骤2-4，直到每个数据点都成为一个单独的聚类或达到预设的聚类数量。</li>
</ul>
</li>
</ul>
<h3 id="Big-Data-Clustering"><a href="#Big-Data-Clustering" class="headerlink" title="Big Data Clustering"></a>Big Data Clustering</h3><ul>
<li><h4 id="BFR（人名算法）"><a href="#BFR（人名算法）" class="headerlink" title="BFR（人名算法）"></a>BFR（人名算法）</h4><ul>
<li>初始阶段，将整个数据集分为几个较小的子集。</li>
<li>在每个子集上应用一个聚类算法，如K-means或层次聚类。</li>
<li>分析每个子集的聚类结果，根据一些评估指标（如聚类质量、聚类数量等）来决定是否需要进一步划分或合并聚类。</li>
<li>如果需要划分，将子集进一步划分为更小的子集，并重复步骤2-3。<br>  如果需要合并，将具有相似性质的聚类合并在一起，并重复步骤2-3。</li>
</ul>
</li>
<li>Clustering using representatives (CURE)</li>
</ul>
<h2 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a>Support vector</h2><blockquote>
<p>核心逻辑，尝试最大化两个类别中的间隔，同时确保所有的数据点都被正确分类<br>使用Lagrange对偶性，我们可以将这个问题转化为对偶问题<br>一旦我们解决了对偶问题，我们可以使用互补松弛性来确定哪些数据点是支持向量，从而确定决策边界</p>
</blockquote>
<ul>
<li>Support vector regression<ul>
<li>让马路盖住所有点，但是越窄越好</li>
</ul>
</li>
<li>Support vector clustering</li>
<li>Transudative support vector machine</li>
</ul>
<h3 id="Kernel-方法"><a href="#Kernel-方法" class="headerlink" title="Kernel 方法"></a>Kernel 方法</h3><blockquote>
<p>将点向高维映射，有时可以使数据点明显分开</p>
</blockquote>
<h2 id="Learning-with-Probabilistic-Graphic-Model"><a href="#Learning-with-Probabilistic-Graphic-Model" class="headerlink" title="Learning with Probabilistic Graphic Model"></a>Learning with Probabilistic Graphic Model</h2><h3 id="Directed-graphs-Baysian-network"><a href="#Directed-graphs-Baysian-network" class="headerlink" title="Directed graphs (Baysian network)"></a>Directed graphs (Baysian network)</h3><ul>
<li>逻辑是，对于多个事件，建立很多小的表，以避免建立所有情况合成的大表</li>
<li>从独立到条件独立</li>
</ul>
<h3 id="Undirected-graphs-Markov-random-field"><a href="#Undirected-graphs-Markov-random-field" class="headerlink" title="Undirected graphs (Markov random field)"></a>Undirected graphs (Markov random field)</h3><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h3 id="马尔可夫奖励过程（MRP-Markov-reward-process）"><a href="#马尔可夫奖励过程（MRP-Markov-reward-process）" class="headerlink" title="马尔可夫奖励过程（MRP Markov reward process）"></a>马尔可夫奖励过程（MRP Markov reward process）</h3><ul>
<li>首先进行基本定义，一般可以定义为$\langle S,P,R,\gamma \rangle$<ul>
<li>$S$是一个有限的状态集合</li>
<li>$P$是转移概率矩阵，$P_{ss’}$记录了从$s$状态出发，到达$s’$状态的概率</li>
<li>$R$是奖励函数<ul>
<li>$R_s$是当前在$s$状态下，下一时刻能获得的奖励期望</li>
</ul>
</li>
<li>$\gamma$是折现参数<ul>
<li>对远期能获得的奖励乘系数“折现”</li>
</ul>
</li>
</ul>
</li>
<li>关键方程<ul>
<li>位置价值函数$v(s) &#x3D;\mathbb{E}[G_t|S_t&#x3D;s]$ 也就是现在在状态$s$，之后总的收益期望</li>
<li>也可以递归定义如下：$$v(s)&#x3D;\mathbb{E}[R_t|S_t&#x3D;s]+\gamma\sum_{s^{\prime}}v(s^{\prime})P[S_{t+1}&#x3D;s^{\prime}|S_t&#x3D;s]$$</li>
<li>第一项是在$t$时刻处于状态$s$时，$t+1$时刻获得的即时$Reward$的期望，后面的累加项统计了下一步的所有可能，并将它们的价值函数折现求和。</li>
<li>这里的$\gamma$是折现函数</li>
</ul>
</li>
<li>两种计算$v$的方法<ul>
<li>矩阵计算<ul>
<li>$v&#x3D;R+\gamma pv$ </li>
<li>故$v&#x3D;(I-\gamma\mathcal{P})^{-1}\mathcal{R}$</li>
</ul>
</li>
<li>动态规划<ul>
<li>在收敛之前，依据公式反复迭代<ul>
<li>$v^{(k)}(s)&#x3D;R(s)+\gamma\sum_{s’\in S}P(s’|s)v^{(k-1)}\left(s’\right)$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="马尔可夫决策过程-Markov-decision-process（MDP）"><a href="#马尔可夫决策过程-Markov-decision-process（MDP）" class="headerlink" title="马尔可夫决策过程 Markov decision process（MDP）"></a>马尔可夫决策过程 Markov decision process（MDP）</h3><blockquote>
<p>在MRP基础上引入了决策过程</p>
</blockquote>
<ul>
<li>同样可给出定义$\langle S,A,P,R,\gamma \rangle$<ul>
<li>补充定义了$A$，$A$是有限的行为的集合，例如平面游戏中的上下左右之类的</li>
</ul>
</li>
<li>策略(policy)<ul>
<li>策略就是在给定状态下，决策的分布，一般记作$\pi$，定义是$\left.\pi(a|s)&#x3D;\mathbb{P}\left[A_{t}&#x3D;a\right|S_{t}&#x3D;s\right]$</li>
</ul>
</li>
<li>在引入了决策过程后，状态价值函数发生了改变，因为此时其同样与policy相关<ul>
<li>定义$v_{\pi}(s)&#x3D;\mathbb{E}<em>{\pi}[G</em>{t}|S_{t}&#x3D;s]$ ，这是新的状态价值函数</li>
<li>定义$q_\pi(s,a)&#x3D;\mathbb{E}_\pi[G_t|S_t&#x3D;s,A_t&#x3D;\alpha]$ ，这被称作决策价值函数<ul>
<li>含义是：在时间$t$处于状态$s$时，做出决策$\alpha$且之后遵循policy $\pi$的预期收益</li>
</ul>
</li>
<li>在如上定义之后，我们可以把$v$的公式改写为<ul>
<li>$v_\pi(s)&#x3D;\mathbb{E}<em>\pi[G_t|S_t&#x3D;s]&#x3D;\sum</em>{a\in\mathcal{A}}\mathbb{E}_\pi[G_t,A_t&#x3D;a|S_t&#x3D;s]$<ul>
<li>穷举所有的决策求和</li>
</ul>
</li>
<li>$&#x3D;\sum_{a\in\mathcal{A}}\mathbb{E}<em>{\pi}[G</em>{t}|S_{t}&#x3D;s,A_{t}&#x3D;a]\mathbb{P}[A_{t}&#x3D;a|S_{t}&#x3D;s]$</li>
<li>$&#x3D;\sum_{a\in\mathcal{A}}q_{\pi}(s,a)\pi(a|s)$ <ul>
<li>当然，我们也可以将$q_{\pi}$定义为递归形式</li>
<li>${\mathcal R}<em>{s}^{a}+\gamma\sum</em>{\forall s\prime}v_{\pi}(s^{\prime}){\mathcal P}_{ss^{\prime}}^{a}$<ul>
<li>思路与之前类似</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>小网格世界示例<ul>
<li>![[Pasted image 20231107164046.png]]<ul>
<li>agent服从全局随机策略，即上下左右均为0.25概率</li>
<li>如果操作走出了网格，则状态不变</li>
<li>多次迭代直到收敛，我们就对每个点建立了$v_{\pi(random)}$ 但是，基于这种策略的答案，是最好的吗？<ul>
<li>显然不是，需要优化</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="MDP-Policy-Iteration"><a href="#MDP-Policy-Iteration" class="headerlink" title="MDP: Policy Iteration"></a>MDP: Policy Iteration</h3><ul>
<li>主体思想是，对于每一种策略$\pi$，先计算$v_{\pi}$再将$greedy(\pi)$作为下一个策略，反复进行策略改进，直到收敛为止<ul>
<li>一般可以用$\pi(a|s)&#x3D;\frac1{A(s)}$进行初始化（$A(s)$指的是$s$状态下，可执行的决策总数）</li>
<li>伪代码表示策略更新<ul>
<li>$\pi^{<em>}(a|s)&#x3D;\operatorname</em>{argmax}<em>{\mathrm{a}}q</em>{\pi}(s,a)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="MDP-Value-Iteration"><a href="#MDP-Value-Iteration" class="headerlink" title="MDP: Value Iteration"></a>MDP: Value Iteration</h3><ul>
<li>主体思想，不依赖更新Policy进而更新$v$，而是直接在每次迭代中更新$v$，以此隐性地更新策略</li>
<li>具体来说，每次更新$v$的方法是，选取能最大化该轮$v(s)$的动作$a$，那其实也就是选取了最优决策，感觉是上一种方法的升级版</li>
</ul>
<h2 id="Mathematical-Foundation-for-Deep-Learning"><a href="#Mathematical-Foundation-for-Deep-Learning" class="headerlink" title="Mathematical Foundation for Deep Learning"></a>Mathematical Foundation for Deep Learning</h2><h3 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h3><ul>
<li>Initial Guess $f(x)&#x3D;log(# ;of ;possible ;outcomes)$</li>
<li>修正后的$f(x)&#x3D;log\frac{1}{p(x)}$<ul>
<li>发生概率越低的事情发生了，该信息所含的信息量越高</li>
</ul>
</li>
<li>联合熵<ul>
<li>指的是随机向量的信息量，它衡量了一组随机变量作为一个整体时的不确定性</li>
<li>公式表示为$H(X)&#x3D;E_{X∼p}[log⁡(\frac{1}{p(x)})]$，这里 $H(X)$ 是指随机向量 $X&#x3D;(X1,…,Xd)$的联合熵。</li>
<li>联合熵是每个结果$x$发生概率的倒数的对数的期望值。</li>
</ul>
</li>
<li>条件熵<ul>
<li>条件熵描述了在已知随机变量 YY 的条件下，随机变量 XX 的额外不确定性。</li>
<li>当 Y&#x3D;y时，X的条件熵公式为 $$H(X∣Y&#x3D;y)&#x3D;\Sigma_{x∈X}P_{X|Y}(x∣y)log⁡(\frac{1}{p_{X|Y}(x∣y)})$$，它是给定 Y 的值后 X的熵的加权平均。</li>
<li>条件熵的总公式是 H(X∣Y)&#x3D;∑y∈Yp(y)H(X∣Y&#x3D;y)，这里 p(y) 是 Y 的概率分布，H(X∣Y&#x3D;y) 是在Y&#x3D;y 条件下 XX 的熵。</li>
<li>最后，联合熵 H(X,Y) 可以分解为 X 的独立熵 H(X) 和 X 的条件熵 H(Y∣X) 的和，也可以写作 H(Y)+H(X∣Y)，表明两个随机变量的总不确定性等于一个变量的不确定性加上给定另一个变量后剩余的不确定性。<ul>
<li>证明，条件概率展开即可</li>
</ul>
</li>
<li>互信息<ul>
<li>$I(X,Y)&#x3D;H(X)-H(X|Y)&#x3D;H(Y)-H(Y|X)$</li>
<li>$I(X,Y)&#x3D;\mathbb{E}\left[\log\frac1{p_X(x)p_Y(y)}-\log\frac1{p_{XY}(x,y)}\right]$</li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
<h3 id="Signal-Analysis"><a href="#Signal-Analysis" class="headerlink" title="Signal Analysis"></a>Signal Analysis</h3><h1 id="深度学习篇"><a href="#深度学习篇" class="headerlink" title="深度学习篇"></a>深度学习篇</h1><h2 id="Neuron-Shallow-Neural-Networks"><a href="#Neuron-Shallow-Neural-Networks" class="headerlink" title="Neuron,Shallow Neural Networks"></a>Neuron,Shallow Neural Networks</h2><h3 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h3><ul>
<li>Activation Function <ul>
<li>Step Function<ul>
<li>$sgn(x)&#x3D;\begin{cases}1,x\geq0\0,x&lt;0&amp;\end{cases}$</li>
</ul>
</li>
<li>Sigmoid Function<ul>
<li>$\mathrm{sigmoid}(x)&#x3D;\frac1{1+e^{-x}}$</li>
<li>$\sigma^{\prime}(x)&#x3D;\sigma(1-\sigma)$</li>
</ul>
</li>
<li>Hyperbolic Tangent Function<ul>
<li>$y&#x3D;\frac{e^x-e^{-x}}{e^x+e^{-x}}$</li>
<li>$y^{\prime}(x)&#x3D;1-y^2$</li>
</ul>
</li>
<li>Rectified Linear Unit<ul>
<li>$y&#x3D;w\cdot\max(0,x)$</li>
<li>$y^{\prime}(x)&#x3D;\max(0,w)$</li>
</ul>
</li>
</ul>
</li>
<li>Example<ul>
<li>线性拟合XOR<ul>
<li>Data<ul>
<li>$\mathbf{X}&#x3D;\binom{0,1,0,1}{0,0,1,1}^T$</li>
<li>$y&#x3D;(0,1,1,0)^T$</li>
</ul>
</li>
<li>以$\theta$为参数的损失函数<ul>
<li>$J(\theta)&#x3D;\frac14\sum_{x\in\mathbb{X}}(y-f(x;\theta))^2$</li>
</ul>
</li>
<li>$f(x;w,b)&#x3D;w^Tx+b$<ul>
<li>$\to J(\theta)&#x3D;\frac14\sum_{x\in\mathbb{X}}(f^*(x)-f(x;\theta))^2$</li>
<li>$\to\boldsymbol{w}&#x3D;(0,0)^T,\boldsymbol{b}&#x3D;0.5$</li>
</ul>
</li>
</ul>
</li>
<li>浅层神经网络拟合XOR<ul>
<li>考虑调用ReLU</li>
<li>$x\to U \to h\to w\to \hat{y}$</li>
<li>也即$f(x;U,c,w,b)&#x3D;w^T\max{0,U^Tx+c}+b$</li>
<li>$\max{0,U^Tx+c}$此为ReLU层（注意前一个0是零向量，max逐元素取）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Shallow-Neural-Networks"><a href="#Shallow-Neural-Networks" class="headerlink" title="Shallow Neural Networks"></a>Shallow Neural Networks</h3><h3 id="Training-Neural-Networks"><a href="#Training-Neural-Networks" class="headerlink" title="Training Neural Networks"></a>Training Neural Networks</h3><h3 id="CNN-in-Action"><a href="#CNN-in-Action" class="headerlink" title="CNN in Action"></a>CNN in Action</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>现代生物学导论</title>
    <url>/2023/09/19/xian-dai-sheng-wu-xue-dao-lun/</url>
    <content><![CDATA[<h2 id="DNA与蛋白质的结构与功能"><a href="#DNA与蛋白质的结构与功能" class="headerlink" title="DNA与蛋白质的结构与功能"></a>DNA与蛋白质的结构与功能</h2><span id="more"></span>
<h3 id="生物系统的弱化学作用"><a href="#生物系统的弱化学作用" class="headerlink" title="生物系统的弱化学作用"></a>生物系统的弱化学作用</h3><ul>
<li>强弱化学作用<ul>
<li>强化学作用<ul>
<li>生理温度下稳定</li>
<li>包括所有的化学键</li>
</ul>
</li>
<li>弱化学作用<ul>
<li>生物体内不断生成和断裂</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="DNA的结构和功能"><a href="#DNA的结构和功能" class="headerlink" title="DNA的结构和功能"></a>DNA的结构和功能</h3><ul>
<li>DNA的化学组成<ul>
<li>磷酸、五碳糖、含氮碱基</li>
</ul>
</li>
<li>DNA的功能<ul>
<li>多样性、存储遗传信息、可复制、稳定</li>
</ul>
</li>
<li>DNA的一级结构（存储遗传信息）<ul>
<li>多聚核苷酸单链</li>
<li>一级结构的应用<ul>
<li>寻找生物间演化关系<ul>
<li>人类（23对）、黑猩猩（及后均为24对）、大猩猩、红毛猩猩</li>
<li>假说：人类染色体在演化过程中发生了融合<ul>
<li>可能更长</li>
<li>可能含有不止一处着丝粒、端粒</li>
<li>比较序列等</li>
<li>……</li>
</ul>
</li>
</ul>
</li>
<li>DNA指纹鉴定</li>
</ul>
</li>
</ul>
</li>
<li>DNA的二级结构</li>
<li>DNA的三级结构</li>
</ul>
<h3 id="蛋白质的结构与功能"><a href="#蛋白质的结构与功能" class="headerlink" title="蛋白质的结构与功能"></a>蛋白质的结构与功能</h3><ul>
<li>蛋白质的四级结构</li>
<li>蛋白质功能</li>
<li>蛋白质折叠</li>
<li>分子伴侣蛋白<ul>
<li>hsp60工作机制</li>
</ul>
</li>
<li>蛋白质折叠与疾病</li>
<li>结构与功能的关系<ul>
<li>例如：DNA聚合酶</li>
</ul>
</li>
</ul>
<h2 id="细胞和癌"><a href="#细胞和癌" class="headerlink" title="细胞和癌"></a>细胞和癌</h2><h3 id="细胞通信"><a href="#细胞通信" class="headerlink" title="细胞通信"></a>细胞通信</h3><p>（单细胞生物之间存在细胞通讯）</p>
<h4 id="细胞通讯的种类"><a href="#细胞通讯的种类" class="headerlink" title="细胞通讯的种类"></a>细胞通讯的种类</h4><ul>
<li>旁分泌信号</li>
<li>内分泌信号</li>
</ul>
<h4 id="细胞通信的过程"><a href="#细胞通信的过程" class="headerlink" title="细胞通信的过程"></a>细胞通信的过程</h4><ul>
<li>信号接受<ul>
<li>信号分子</li>
<li>受体<ul>
<li>细胞表面受体<ul>
<li>G蛋白耦合受体</li>
<li>受体络氨酸激酶</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>信号转导<ul>
<li>磷酸化级联</li>
<li>第二信使<ul>
<li>非蛋白类小分子</li>
<li>cAMP环腺苷酸</li>
<li>cGMP环鸟苷酸</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="细胞周期"><a href="#细胞周期" class="headerlink" title="细胞周期"></a>细胞周期</h3><h4 id="调控细胞周期的分子机制"><a href="#调控细胞周期的分子机制" class="headerlink" title="调控细胞周期的分子机制"></a>调控细胞周期的分子机制</h4><ul>
<li>周期素和蛋白激酶</li>
<li>生长因子</li>
</ul>
<h3 id="细胞凋亡"><a href="#细胞凋亡" class="headerlink" title="细胞凋亡"></a>细胞凋亡</h3><ul>
<li>凋亡过程<ul>
<li>细胞收缩、染色体凝结</li>
<li>膜起泡</li>
<li>细胞核塌陷</li>
<li>凋亡体形成</li>
<li>凋亡小体裂解</li>
</ul>
</li>
<li>三种信号通路<ul>
<li>线粒体通路</li>
<li>死亡受体通路</li>
<li>内质网通路</li>
</ul>
</li>
</ul>
<h3 id="细胞自噬"><a href="#细胞自噬" class="headerlink" title="细胞自噬"></a>细胞自噬</h3><ul>
<li>自噬过程<ul>
<li>自噬泡形成</li>
<li>自噬体形成</li>
<li>自噬体和溶酶体融合</li>
<li>自噬溶酶体形成</li>
<li>降解和再吸收</li>
</ul>
</li>
<li>细胞自噬与疾病</li>
</ul>
<h3 id="液-液相分离"><a href="#液-液相分离" class="headerlink" title="液-液相分离"></a>液-液相分离</h3><ul>
<li>无膜细胞器</li>
</ul>
<h4 id="DNA和基因"><a href="#DNA和基因" class="headerlink" title="DNA和基因"></a>DNA和基因</h4><ul>
<li>基因<ul>
<li>控制性状</li>
</ul>
</li>
<li>基因的构成<ul>
<li>调控区域<ul>
<li>指导基因是否能够产生蛋白</li>
</ul>
</li>
<li>启动子<ul>
<li>和转录酶结合的序列</li>
</ul>
</li>
<li>编码区域</li>
<li>断裂基因<ul>
<li>细菌基因：连续</li>
<li>动物基因：断裂</li>
</ul>
</li>
<li>原核基因</li>
<li>真核基因<ul>
<li>产生蛋白质的序列<ul>
<li>内含子&amp;外显子</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>染色质与染色体<ul>
<li>从DNA到染色质</li>
</ul>
</li>
<li>遗传物质保证稳定机制<ul>
<li>DNA精确复制<ul>
<li>碱基配对</li>
<li>DNA聚合酶的校正</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="遗传密码的特点"><a href="#遗传密码的特点" class="headerlink" title="遗传密码的特点"></a>遗传密码的特点</h3><ul>
<li>多个遗传密码对应同一个氨基酸</li>
</ul>
<h4 id="调控从基因到蛋白质的过程"><a href="#调控从基因到蛋白质的过程" class="headerlink" title="调控从基因到蛋白质的过程"></a>调控从基因到蛋白质的过程</h4><ul>
<li>对环境刺激做出应答</li>
<li>发育</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo git问题</title>
    <url>/2023/11/01/jie-jue-git-wen-ti-failed-to-connect-to-127.0.0.1-port-7890-after-2070-ms-connection-refused/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global http.proxy</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tricks</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>量子与统计</title>
    <url>/2023/10/28/liang-zi-yu-tong-ji/</url>
    <content><![CDATA[<p>习题课相关内容[[量子与统计习题课]]</p>
<span id="more"></span>

<h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><blockquote>
<p>快速通过</p>
</blockquote>
<h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h3 id="薛定谔方程"><a href="#薛定谔方程" class="headerlink" title="薛定谔方程"></a>薛定谔方程</h3><ul>
<li><h6 id="薛定谔方程-1"><a href="#薛定谔方程-1" class="headerlink" title="薛定谔方程"></a>薛定谔方程</h6><ul>
<li>含时薛定谔方程<ul>
<li>$$\mathrm{i}\hbar\frac{\partial}{\partial t}\psi(\boldsymbol{r},t)&#x3D;H\psi(\boldsymbol{r},t)$$</li>
</ul>
</li>
<li>不含时薛定谔方程<ul>
<li>$$H\psi&#x3D;E\psi$$</li>
</ul>
</li>
<li>一些说明<ul>
<li>薛定谔方程不适用于相对论效应，所以能量的计算方式一般依据其经典对应即可</li>
<li>薛定谔方程是量子力学的一个基本假定</li>
</ul>
</li>
</ul>
</li>
<li>薛定谔方程的讨论<ul>
<li>空间概率密度$\Psi^{*}\left(\boldsymbol{r},t\right)\Psi(\boldsymbol{r},t)$</li>
<li>概率流密度矢量$\frac{\partial w}{\partial t}&#x3D;\Psi^{<em>}\frac{\partial\Psi}{\partial t}+\frac{\partial\Psi^{</em>}}{\partial t}\Psi$</li>
<li>代入薛定谔方程化简，得$\frac{\partial w}{\partial t}&#x3D;\vec{J}\equiv\frac{i\hbar}{2\mu}(\Psi\nabla\Psi^*-\Psi^*\nabla\Psi)$<ul>
<li>注1：$\frac{\partial\Psi}{\partial t}&#x3D;\frac{i\hbar}{2\mu}\nabla^{2}\Psi+\frac{1}{i\hbar}U\Psi$</li>
<li>注2：$\frac{\partial\Psi^{<em>}}{\partial t}&#x3D;-\frac{i\hbar}{2\mu}\nabla^{2}\Psi^{</em>}-\frac{1}{i\hbar}U\Psi^{*}$</li>
</ul>
</li>
</ul>
</li>
<li>定态薛定谔方程<ul>
<li>此处开始涉及定态的概念</li>
<li>若势能函数与时间无关，则薛定谔方程可以分离变量求解</li>
<li>原本的薛定谔方程如下：$\begin{aligned}i\hbar\frac{\partial\Psi}{\partial t}&amp;&#x3D;-\frac{\hbar^2}{2\mu}\nabla^2\Psi+U(\vec{r})\Psi\end{aligned}$<ul>
<li>分离变量设：$\Psi(\vec{r},t)&#x3D;f(t)\psi(\vec{r})$</li>
<li>得：$\frac{i\hbar}{f(t)}\frac{df}{dt}&#x3D;\frac{1}{\psi(\vec{r})}\Bigg[-\frac{\hbar^{2}}{2\mu}\nabla^{2}\psi+U(\vec{r})\psi\Bigg]\color{red}{&#x3D;E}$</li>
<li>接下来按照分离变量的解法分别求解两个方程</li>
<li>时间项的解是一般的$i\hbar\frac{df}{dt}&#x3D;Ef\left(t\right),f\left(t\right)&#x3D;c\mathrm{e}^{-\frac ihEt}$</li>
<li>另一个即是所谓的定态薛定谔方程（不含时薛定谔方程）<ul>
<li>$-\frac{\hbar^{2}}{2\mu}\nabla^{2}\psi+U(\vec{r})\psi&#x3D;E\psi(\vec{r})$</li>
<li>这是在势场$U(\vec{r})$中粒子的能量本征方程<ul>
<li>定态是能量的本征态</li>
</ul>
</li>
<li>定态波函数都具有这样的形式<ul>
<li>$\Psi(\vec{r},t)&#x3D;\mathrm{e}^{-\frac i\hbar Et}\psi(\vec{r})$</li>
<li>其中$E$的物理意义是粒子的能量</li>
</ul>
</li>
</ul>
</li>
<li>一些定义区分<ul>
<li>定态：体系的能量有确定值的状态</li>
<li>非定态：由若干个能量不同的本征态叠加所形成的态</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="一维运动问题的一般分析"><a href="#一维运动问题的一般分析" class="headerlink" title="一维运动问题的一般分析"></a>一维运动问题的一般分析</h3><h4 id="一维定态薛定谔方程的一般性质"><a href="#一维定态薛定谔方程的一般性质" class="headerlink" title="一维定态薛定谔方程的一般性质"></a>一维定态薛定谔方程的一般性质</h4><ul>
<li>一维定态的分类<ul>
<li>束缚态：粒子局限在有限的空间中, 即粒子在无穷远处出现的概率等于零的状态</li>
<li>非束缚态(或称散射态): 粒子可以出现在无穷远处的状态</li>
</ul>
</li>
</ul>
<h4 id="一维有限深势阱和有限深势阱"><a href="#一维有限深势阱和有限深势阱" class="headerlink" title="一维有限深势阱和有限深势阱"></a>一维有限深势阱和有限深势阱</h4><ul>
<li><p>一维无限深势阱</p>
<ul>
<li><p>先依据图示写出势能函数，再写出哈密顿量，最后写出薛定谔方程</p>
<ul>
<li>$$V\big(x\big)&#x3D;\begin{cases}0,&amp;0&lt;x&lt;a\\infty,&amp;x&lt;0,x&gt;a\end{cases} $$</li>
<li>$$\hat{H}&#x3D;-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+V(x) $$</li>
</ul>
</li>
<li><p>由于势能函数不随时间变化，所以是定态薛定谔方程</p>
<ul>
<li>$$-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}\psi(x)&#x3D;E\psi(x)$$</li>
<li>一组常见的化简方式是$k^2&#x3D;\frac{2mE}{\hbar^2}$</li>
</ul>
</li>
<li><p>最终阱内方程化简为</p>
<ul>
<li>$\psi^{\prime\prime}(x)+k^2\psi(x)&#x3D;0$</li>
<li>解得$\psi(x)&#x3D;A\sin(kx+\delta)$</li>
</ul>
</li>
<li><p>阱外情况</p>
<ul>
<li>$\left[-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+\infty\right]\psi(x){&#x3D;}E\psi(x)$</li>
<li>故$\psi$只能为0</li>
</ul>
</li>
<li><p>列出方程并求解后，考虑边界条件，波函数在$x&#x3D;0, a$处得0</p>
<ul>
<li><p>解得$sinka &#x3D; 0$ 故 $ka &#x3D; n\pi$, $n$取$1,2,3,…$</p>
<ul>
<li>为什么不取0？因为取0后波函数为0，无意义</li>
<li>为什么不取负数？因为并没有给出新的波函数</li>
</ul>
</li>
<li><p>最终解出</p>
<ul>
<li><p>$E&#x3D;E_n&#x3D;\frac{\hbar^2\pi^2n^2}{2ma^2},\quad n&#x3D;1,2,3,\cdots $</p>
<ul>
<li>这里可以看出，粒子具有最低能级，也就是零点能，可以用不确定性关系解释</li>
<li>（势阱中$\Delta x\approx a$, 动量具有不确定度，能量不能为0）</li>
</ul>
</li>
<li><p>$\psi_n\left(x\right)&#x3D;A_n\sin\left(\frac{n\pi}ax\right)$</p>
<ul>
<li>对波函数归一化得</li>
</ul>
<p>$$\left.\psi_n(x)&#x3D;\left{\begin{aligned}\sqrt{\frac2a}\sin(\frac{n\pi}ax),\quad&amp;0&lt;x&lt;a\0,\quad&amp;x&lt;0,x&gt;a\end{aligned}\right.\right.$$</p>
<ul>
<li>最终，定态解的形式是$$\Psi_n\left(x,t\right)&#x3D;\sqrt{\frac2a}\sin(\frac{n\pi}ax)e^{-\frac i\hbar E_nt}$$</li>
</ul>
</li>
</ul>
</li>
<li><p>补充说明，如果原点在势阱中心，则边界条件会改变，同样方法求解即可</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>有限深对称方势阱</p>
<ul>
<li>同样列出势能函数和哈密顿量</li>
<li>在$E&lt;V_0$的情况下，势阱内外定态薛定谔方程如下式<ul>
<li>$$\begin{aligned}&amp;\frac{d^2\psi}{dx^2}+k^2\psi(x)&#x3D;0,\quad(k&#x3D;\sqrt{2mE}&#x2F;\hbar,\left|x\right|&lt;a&#x2F;2)\\&amp;\frac{d^2\psi}{dx^2}-\beta^2\psi(x)&#x3D;0,\quad(\beta&#x3D;\sqrt{2m\left(V_0-E\right)}&#x2F;\hbar,\left|x\right|&gt;a&#x2F;2)\end{aligned}$$<ul>
<li>第二个方程的一般解$\psi(x)&#x3D;C\operatorname{e}^{\beta x}+D\operatorname{e}^{-\beta x}$</li>
<li>但是考虑到束缚态的性质，在无穷远处粒子出现的概率为0</li>
<li>最终解得$$\left.\psi(x)&#x3D;\left{\begin{aligned}C\operatorname{e}^{\beta x},\quad&amp;(x&lt;-a&#x2F;2)\A\cos kx+B\sin kx,\quad&amp;(-a&#x2F;2&lt;x&lt;a&#x2F;2)\D\operatorname{e}^{-\beta x}.\quad&amp;(a&#x2F;2&lt;x)\end{aligned}\right.\right.$$<ul>
<li>对于束缚态，能级是不简并的，同时对称势阱$V(x)$具有空间反射对称性，故能量本征态必有确定的宇称，分别讨论即可</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="第二章课堂思考题"><a href="#第二章课堂思考题" class="headerlink" title="第二章课堂思考题"></a>第二章课堂思考题</h4><ul>
<li>写出自由粒子、氢原子、氦原子的哈密顿量<ul>
<li>自由粒子$\hat{H}&#x3D;\frac{\hat{\mathbf{p}}^2}{2m}&#x3D;-\frac{\hbar^2}{2m}\nabla^2$</li>
<li>$\hat{H}&#x3D;-\frac{\hbar^2}{2m_e}\nabla^2-\frac{e^2}{4\pi\epsilon_0r}$</li>
<li>$\hat{H}&#x3D;-\frac{\hbar^2}{2m_e}\left(\nabla_1^2+\nabla_2^2\right)-\frac{2e^2}{4\pi\epsilon_0r_1}-\frac{2e^2}{4\pi\epsilon_0r_2}+\frac{e^2}{4\pi\epsilon_0|\mathbf{r}_1-\mathbf{r}_2|}$<ul>
<li>在一些情况下，也可以不写$4\pi\epsilon_0$（特殊单位制）</li>
</ul>
</li>
</ul>
</li>
<li>假设一个粒子的初始态是两个定态的线性组合$$\psi\left(x,0\right)&#x3D;c_1\psi_1\left(x\right)+c_2\psi_2\left(x\right)$$那么任意时刻的波函数是什么？求出概率密度并描述其运动形式.<ul>
<li>其实只要分别乘上时间项，再求平方即可$$\psi\left(x,t\right)&#x3D;c_1\psi_1\left(x\right)e^{-iE_1t&#x2F;\hbar}+c_2\psi_2\left(x\right)e^{-iE_2t&#x2F;\hbar}$$</li>
<li>有一个有趣的点，就是平方后可得$$c_1^2\psi_1^2\left(x\right)+c_2^2\psi_2^2\left(x\right)+2c_1c_2\psi_1\left(x\right)\psi_2\left(x\right)\mathrm{cos}\left[\left(E_2-E_1\right)t&#x2F;\hbar\right]$$</li>
<li>概率密度的振动被称为“量子节拍”</li>
</ul>
</li>
<li>在阱宽为$a$的无限深势阱中, 粒子的状态为<ul>
<li>$$\Psi(x)&#x3D;\frac1{\sqrt{2}}\left[\sqrt{\frac2a}\sin\frac{\pi x}a-\sqrt{\frac2a}\sin\frac{2\pi x}a\right]$$</li>
<li>多次测量其能量<ul>
<li>每次可能测到的值和相应概率以及能量的平均值<ul>
<li>解答问题的关键在于将波函数写成按无限深势阱哈密顿量的本征函数展开形式，之后常规计算即可</li>
<li>$$\Psi(x)&#x3D;\frac1{\sqrt{2}}\psi_1(x)-\frac1{\sqrt{2}}\psi_2(x)$$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h3 id="第三章习题"><a href="#第三章习题" class="headerlink" title="第三章习题"></a>第三章习题</h3><ul>
<li>有关氢原子的计算<ul>
<li>半径的均值往往在球坐标系下计算<ul>
<li>注意变换坐标系的相关细节</li>
</ul>
</li>
<li>最概然半径，也即氢原子电子出现概率最大的半径值，可以通过计算不同$r$处的概率密度得到，注意$w(r)&#x3D;R^2(r)r^2$ </li>
<li>动能：利用动能算符转化</li>
</ul>
</li>
<li>有关经典物理中的计算<ul>
<li>将能量表示式转化为算符</li>
</ul>
</li>
<li>从态函数中读出信息<ul>
<li>$\psi(r, \theta, \psi)&#x3D;\frac{1}{2} R_{21}(r) Y_{10}(\theta, \varphi)-\frac{\sqrt{3}}{2} R_{21}(r) Y_{1-1}(\theta, \varphi)$<ul>
<li>$R_{nl},Y_{lm}$通过表达式确定$n、l、m$三个参数</li>
<li>$n$可以确定$E$</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="Pasted-image-20231031184458-png"><a href="#Pasted-image-20231031184458-png" class="headerlink" title="![[Pasted image 20231031184458.png]]"></a>![[Pasted image 20231031184458.png]]</h3></li>
</ul>
<h2 id="量子力学的矩阵形式与表象理论"><a href="#量子力学的矩阵形式与表象理论" class="headerlink" title="量子力学的矩阵形式与表象理论"></a>量子力学的矩阵形式与表象理论</h2><h3 id="Q表象"><a href="#Q表象" class="headerlink" title="Q表象"></a>Q表象</h3><ul>
<li>将某态转变为力学量Q表象下的表示<ul>
<li>$\hat{Q}\mu_i&#x3D;q_i\mu_i$，${ \mu_{i} }$是其本征态函数，构成态空间正交归一完备的基矢</li>
<li>将$\Psi$向本征函数系${ \mu_i,… }$上投影，得系数$a_n(t)$<ul>
<li>而一组系数构成的向量，就是$\hat{Q}$表象下的态矢量</li>
</ul>
</li>
<li>$|a_n|^2$就是在$\Psi(x,t)$所描写的态中测量力学量$\hat{Q}$所得结果为$q_n$的概率</li>
<li>对连续谱和多自由度也成立</li>
</ul>
</li>
<li>考虑某力学量表象$Q’$ <ul>
<li>同一个量子态$\Psi$也可用$Q’$的本征态展开，同样作投影得一组表示$(q_n^{‘})$</li>
</ul>
</li>
<li>一个领悟<ul>
<li>“波函数的模方代表在某处找到粒子概率”仅是在坐标表象下成立的表述。原因是，事实上这个东西的模方代表的是位于$x\rangle$上的概率。其实与$\hat{Q}$表象下的${a_n}$没什么区别，只不过离散的时候是累加，而连续的时候是积分</li>
<li>所以说，${a_n}$也可以视作$\hat{Q}$表象下的波函数</li>
</ul>
</li>
</ul>
<h3 id="算符的矩阵表示"><a href="#算符的矩阵表示" class="headerlink" title="算符的矩阵表示"></a>算符的矩阵表示</h3><ul>
<li>坐标表象下，算符一般可表示为$\hat{F}(x,-i\hbar\frac{\partial}{\partial x})$ </li>
<li>表示力学量$F$的矩阵是厄米矩阵</li>
<li>力学量在自身表象中为对角的厄米矩阵</li>
</ul>
<h3 id="量子力学的矩阵形式"><a href="#量子力学的矩阵形式" class="headerlink" title="量子力学的矩阵形式"></a>量子力学的矩阵形式</h3><h3 id="Dirac-符号"><a href="#Dirac-符号" class="headerlink" title="Dirac 符号"></a>Dirac 符号</h3><ul>
<li>右矢与左矢</li>
<li>标积<ul>
<li>$\langle \psi|\phi \rangle$</li>
</ul>
</li>
</ul>
<h2 id="第四章习题"><a href="#第四章习题" class="headerlink" title="第四章习题"></a>第四章习题</h2><h3 id="课堂习题"><a href="#课堂习题" class="headerlink" title="课堂习题"></a>课堂习题</h3><ul>
<li>$\hat{A}^2&#x3D;1$求本征值及矩阵<ul>
<li>分别作用于某波函数即可</li>
</ul>
</li>
<li>$\hat{A}^2&#x3D;\hat{A}$求本征值及矩阵<ul>
<li>方法同上</li>
</ul>
</li>
<li>坐标与动量表象下算符的矩阵元</li>
<li>粒子以某态在一维无限深势阱中运动，求该态在能量表象下的矩阵表示（态变换）</li>
<li>一维谐振子的坐标、动量以及H在能量表象中的表示（算符变换）</li>
</ul>
<h3 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h3><h1 id="自旋与全同粒子体系"><a href="#自旋与全同粒子体系" class="headerlink" title="自旋与全同粒子体系"></a>自旋与全同粒子体系</h1><h3 id="Stern-Gerlach实验"><a href="#Stern-Gerlach实验" class="headerlink" title="Stern-Gerlach实验"></a>Stern-Gerlach实验</h3><ul>
<li>银原子最外层价电子在$5s$轨道上，由$L&#x3D;\sqrt{l(l+1)}\hbar$，而$s$轨道$l&#x3D;0$知银原子基态轨道磁矩为$0$</li>
<li>但是实验结果表明，当银原子束通过非均匀磁场时，它们的磁矩方向并不是连续分布的，而是分为两个明显分离的部分，这意味着银原子的磁矩有两个可能的取向</li>
<li>这个实验结果揭示了除了轨道角动量之外还有另一种形式的角动量，即自旋角动量。自旋角动量与磁矩有关，而且对于银原子这个磁矩有两个可能的取值，表明自旋角动量磁量子数$s_m$为$\pm 1&#x2F;2$(注：自旋角量子数为$\frac{1}{2}$)</li>
</ul>
<h3 id="电子自旋假设"><a href="#电子自旋假设" class="headerlink" title="电子自旋假设"></a>电子自旋假设</h3><ul>
<li>电子并非质点，具有内禀运动自旋，相应具有自旋角动量和自旋磁矩<ul>
<li>$|\vec S|&#x3D;\sqrt{s(s+1)}\hbar$ , s为自旋量子数<ul>
<li>注意区别电子的自旋量子数和磁量子数</li>
</ul>
</li>
<li>$S_z&#x3D;\pm\frac{\hbar}{2}$<ul>
<li>也就是$m_s&#x3D;\pm\frac{1}{2}$</li>
</ul>
</li>
</ul>
</li>
<li>自旋的表达——自旋算符与Pauli算符<ul>
<li>与轨道角动量类似<ul>
<li>${\hat{S^2},\hat{S_z}}$共同本征态$|Sm\rangle$</li>
</ul>
</li>
<li>Pauli算符<ul>
<li>$\hat{\vec{\boldsymbol{S}}}&#x3D;\frac\hbar2\hat{\boldsymbol{\sigma}}$</li>
<li>在${\hat{S^2},\hat{S_z}}$或者${\hat{\sigma^2},\hat{\sigma_z}}$表象下</li>
<li>$\sigma_x&#x3D;\begin{pmatrix}0&amp;1\1&amp;0\end{pmatrix}$,$\sigma_y&#x3D;\begin{pmatrix}0&amp;-i\ i&amp;0\end{pmatrix}$,$\sigma_z&#x3D;\begin{pmatrix}1&amp;\mathbf{0}\0&amp;-1\end{pmatrix}$<ul>
<li>证明利用厄米性和反对易关系</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="电子自旋态"><a href="#电子自旋态" class="headerlink" title="电子自旋态"></a>电子自旋态</h3><ul>
<li>电子的二分量波函数<ul>
<li>为了描述电子所处的状态，还需要变量描写自旋态，由于自旋只有两种可能，所以可以写为两个分量并排成一个矩阵$$\psi(\vec{r},t)&#x3D;\begin{pmatrix}\psi_1(\vec{r},t)\\psi_2(\vec{r},t)\end{pmatrix}$$<ul>
<li>已进行归一化$\int\psi^\dagger\psi d\tau&#x3D;\int\left(\left|\psi_1\right|^2+\left|\psi_2\right|^2\right)d\tau&#x3D;1$</li>
</ul>
</li>
<li>第一行对应$S_z&#x3D;\frac{\hbar}{2}$,第二行对应$S_z&#x3D;-\frac{\hbar}{2}$</li>
</ul>
</li>
<li>电子的自旋磁矩<ul>
<li>电子自旋磁矩等于一个玻尔磁子<ul>
<li>$|\mu_z|$&#x3D;$\mu_B$&#x3D;$\frac{e\hbar}{2m_e}$</li>
<li>电子自旋磁矩算符$\hat{\vec{\mu}}_s&#x3D;-2\cdot\frac{\mu_B}\hbar\hat{\vec{\vec{S}}}&#x3D;-\frac e{m_e}\hat{\vec{S}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="角动量的合成"><a href="#角动量的合成" class="headerlink" title="角动量的合成"></a>角动量的合成</h3><ul>
<li>两个角动量体系的力学量完全集<ul>
<li>非耦合表象${\hat{J}<em>1^2,\hat{J}</em>{1z},\hat{J}<em>2^2,\hat{J}</em>{2z}}$<ul>
<li>即没有用到耦合量的表象</li>
<li>基底$|j_1m_{1}j_2m_{2}\rangle$</li>
</ul>
</li>
<li>耦合表象${\hat{J}_1^2,\hat{J}<em>2^2,\hat{J}^{2},\hat{J}</em>{z}}$<ul>
<li>基底$|j_1j_2jm_j\rangle$</li>
</ul>
</li>
</ul>
</li>
<li>总角动量${\hat{J^2},\hat{J_z}}$的本征谱<ul>
<li>对于确定的$j_1$和$j_2$，总角量子数$j$的取值范围是$\begin{aligned}j&#x3D;&amp;\big|j_1-j_2\big|,\big|j_1-j_2\big|+1,\cdots,j_1+j_2\end{aligned}$</li>
</ul>
</li>
</ul>
<h3 id="全同粒子体系"><a href="#全同粒子体系" class="headerlink" title="全同粒子体系"></a>全同粒子体系</h3><ul>
<li>全同性假设<ul>
<li>全同粒子体系中任意两个全同粒子的交换，都不改变体系的物理状态</li>
</ul>
</li>
<li>交换算符$\hat{P_{ij}}$<ul>
<li>作用是把波函数中第$i$个和第$j$个粒子的坐标交换</li>
</ul>
</li>
<li>玻色子和费米子<ul>
<li>Bose子，自旋量子数为$\hbar$的整数倍</li>
<li>Fermi子，自旋量子数为$\hbar$的半整数倍</li>
</ul>
</li>
<li>全同性假设对哈密顿量的要求<ul>
<li>任意交换两个全同粒子，体系的哈密顿量不变</li>
</ul>
</li>
<li>交换对称或反对称波函数的构成<ul>
<li>前提：</li>
<li>单粒子近似<ul>
<li>无耦合体系：体系的总波函数是单个粒子波函数的乘积</li>
</ul>
</li>
<li>二粒子体系</li>
</ul>
</li>
</ul>
<h2 id="双电子的自旋函数"><a href="#双电子的自旋函数" class="headerlink" title="双电子的自旋函数"></a>双电子的自旋函数</h2><ul>
<li><p>单体近似下，两个电子的自旋函数</p>
<ul>
<li>$\chi\left(s_{1z},s_{2z}\right)&#x3D;\chi_{m_s}\left(s_{1z}\right)\chi_{m_s^{\prime}}\left(s_{2z}\right)$</li>
<li>由角动量耦合相关知识，两电子总角动量S为0或1<ul>
<li>S&#x3D;0时$m_s&#x3D;0$</li>
<li>S&#x3D;1时$m_s&#x3D;-1,0,1$</li>
</ul>
</li>
</ul>
</li>
<li><p>无耦合表象${\hat{S}<em>{1}^{2},\hat{S}</em>{1z},\hat{S}<em>{2}^{2},\hat{S}</em>{2z}}$</p>
</li>
<li><p>耦合表象${S_{1}^{2},S_{2}^{2},S^{2},S_{z}}$</p>
</li>
<li><p>说明</p>
<ul>
<li>在这两个表象中，均可以省略$S_{1}^{2},S_{2}^{2}$,因为这两个为常数算符，值为$\frac{3\hbar^2}{4}$</li>
<li>有关总自旋角动量，补充说明，其本征值也是$\sqrt{s(s+1)}\hbar$</li>
</ul>
</li>
<li><p>耦合与非耦合表象的选择</p>
<ul>
<li>例如$\lambda \hat{S}·\hat{L}$<ul>
<li>选择耦合表象，为$\hat{J}^2-\hat{S}^2-\hat{L}^2$</li>
</ul>
</li>
</ul>
</li>
<li><p>无耦合表象${\hat{S}<em>{1}^{2},\hat{S}</em>{1z},\hat{S}<em>{2}^{2},\hat{S}</em>{2z}}$的基底</p>
</li>
<li><p>$$\begin{cases}\chi_S^{(1)}&#x3D;\chi_{1&#x2F;2}\left(s_{1z}\right)\chi_{1&#x2F;2}\left(s_{2z}\right)\\chi_S^{(2)}&#x3D;\chi_{-1&#x2F;2}\left(s_{1z}\right)\chi_{-1&#x2F;2}\left(s_{2z}\right)\\chi_S^{(3)}&#x3D;\frac{1}{\sqrt{2}}\Big[\chi_{1&#x2F;2}\left(s_{1z}\right)\chi_{-1&#x2F;2}\left(s_{2z}\right)+\chi_{-1&#x2F;2}\left(s_{1z}\right)\chi_{1&#x2F;2}\left(s_{2z}\right)\Big]\\chi_A&#x3D;\frac{1}{\sqrt{2}}\Big[\chi_{1&#x2F;2}\left(s_{1z}\right)\chi_{-1&#x2F;2}\left(s_{2z}\right)-\chi_{-1&#x2F;2}\left(s_{1z}\right)\chi_{1&#x2F;2}\left(s_{2z}\right)\Big]\end{cases}$$</p>
<ul>
<li>说明：上述组成正交归一系</li>
<li>说明：上述波函数的本征态，也可作为耦合表象${S_{1}^{2},S_{2}^{2},S^{2},S_{z}}$的基底</li>
</ul>
</li>
<li><p>纠缠态</p>
<ul>
<li><p>由两个粒子组成的复合体系的量子态,不能表示为每个粒子的量子态的乘积,则称为纠缠态</p>
</li>
<li><p>$X_{10},X_{00}$</p>
</li>
</ul>
</li>
<li><p>可分离态</p>
<ul>
<li>$X_{1-1},X_{11}$</li>
</ul>
</li>
<li><p>有关双电子体系波函数的说明</p>
<ul>
<li>全同费米子体系反对称，故自旋对称则空间反对称，空间对称则自旋反对称</li>
</ul>
</li>
</ul>
<p>注：收看第23讲</p>
<h1 id="微扰论"><a href="#微扰论" class="headerlink" title="微扰论"></a>微扰论</h1><h3 id="定态微扰论Ⅰ：非简并情形"><a href="#定态微扰论Ⅰ：非简并情形" class="headerlink" title="定态微扰论Ⅰ：非简并情形"></a>定态微扰论Ⅰ：非简并情形</h3><ul>
<li><p>一级微扰公式</p>
<ul>
<li>$\hat{H}\psi_n&#x3D;E_n\psi_n$中，$\hat{H}$较为复杂无法精确求解时</li>
<li>如果$\hat{H}$可以表达为$\hat{H_0}+\hat{H}’$（其中$\hat{H_0}$可解$\hat{H}’$是小的修正）<ul>
<li>考虑将$E_n$和$\psi_n$展开为$\lambda$的幂级数，并把$\hat{H}’$写作$\lambda \hat{H}^{(1)}$</li>
<li>$E_n&#x3D;E_n^{(0)}+\lambda E_n^{(1)}+\lambda^2E_n^{(2)}+\cdots$</li>
<li>$\psi_n&#x3D;\psi_n^{(0)}+\lambda \psi_n^{(1)}+\lambda^2\psi_n^{(2)}+\cdots$</li>
</ul>
</li>
<li>$(\hat{H}_0+\lambda \hat{H}^{(1)})(\psi_n^{(0)}+\lambda\psi_n^{(1)}+\lambda^2\psi_n^{(2)}+\cdots)&#x3D;(E_n^{(0)}+\lambda E_n^{(1)}+\lambda^2E_n^{(2)}+\cdots)(\psi_n^{(0)}+\lambda\psi_n^{(1)}+\lambda^2\psi_n^{(2)}+\cdots)$</li>
</ul>
</li>
<li><p>一级方程:  $(\hat{H}_0-E_n^{(0)})\psi_n^{(1)}&#x3D;-(\hat{H}^{(1)}-E_n^{(1)})\psi_n^{(0)}$</p>
</li>
<li><p>约定：波函数的高级近似解与零级近似解都正交</p>
</li>
<li><p>非简并情形下</p>
<ul>
<li>将一级近似解$\psi_n^{(1)}$用零级解展开为$\Sigma_m a_{nm}^{(1)}\psi_m^{(0)}$ 代入回一级方程，$\hat{H_0}\psi_m$均化简为$E_n\psi_m$</li>
<li>得到$\sum_ma_{nm}^{(1)}(E_m^{(0)}-E_n^{(0)})\psi_m^{(0)}&#x3D;-(\hat{H^{\prime}}-E_n^{(1)})\psi_n^{(0)}$</li>
<li>等式两边同时左乘$\psi_k^{(0)*}$，并考虑正交归一性</li>
<li>得到$a_{nk}^{(1)}(E_k^{(0)}-E_n^{(0)})&#x3D;-\int\psi_k^{(0)*}\hat{H^{\prime}}\psi_n^{(0)}d\tau+E_n^{(1)}\delta_{kn}$</li>
<li>$k&#x3D;n$时，得一级微扰能<ul>
<li>$\begin{aligned}E_n^{(1)}&#x3D;\int\psi_n^{(0)*}\hat{H}’\psi_n^{(0)}d\tau&#x3D;H_{nn}’\end{aligned}$</li>
<li>这里的$n$可以理解为能级，求哪个能级，就在那个能级的波函数下求均值</li>
</ul>
</li>
<li>$k\neq n$时，得$a_{nk}^{(1)}$也就是用零级近似解展开的展开系数</li>
<li>一级微扰波函数<ul>
<li>$\begin{aligned}\psi_n^{(1)}&#x3D;\sum_m’\frac{H_{mn}^{\prime}}{E_n^{(0)}-E_m^{(0)}}\psi_m^{(0)}\end{aligned}$</li>
</ul>
</li>
</ul>
</li>
<li><p>二级微扰能</p>
<ul>
<li>计算方法与一级微扰能类似，比较系数列式后，带入$\psi _n^{(1)}$，将$\psi_n^{(2)}$展开为${\psi_n^{(0)}}$的线性组合,然后在方程两端乘以$\psi_n^{(0)}$并积分即可 </li>
<li>$E_n^{(2)}&#x3D;\sum_m^{\prime}\frac{|H_{mn}^{\prime}|^2}{E_n^{(0)}-E_m^{(0)}}$</li>
</ul>
</li>
<li><p>例题1 电解质的极化率</p>
</li>
<li><p>例题2 氦原子以及氦原子的基态能量</p>
</li>
</ul>
<h3 id="定态微扰论Ⅱ：简并情形"><a href="#定态微扰论Ⅱ：简并情形" class="headerlink" title="定态微扰论Ⅱ：简并情形"></a>定态微扰论Ⅱ：简并情形</h3><ul>
<li>一级微扰能和零级波函数<ul>
<li>简并情况下，应设$\psi_n^{(0)}&#x3D;\sum_{i&#x3D;1}^kc_i^{(0)}\varphi_{ni}^{(0)}$<ul>
<li>说明：在能级为$n$的正交化简并子空间内考虑问题</li>
<li>假设该能级$k$度简并</li>
</ul>
</li>
<li>$H^{‘}$起到部分消除简并的作用<ul>
<li>$E_n&#x3D;E_n^{(0)}+E_{nj}^{(1)}$, $j&#x3D;1,2,…,k$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Stark效应"><a href="#Stark效应" class="headerlink" title="Stark效应"></a>Stark效应</h3><ul>
<li>一级Stark效应<ul>
<li>氢原子受到恒定外电场$\vec{\varepsilon}&#x3D;\varepsilon\vec{e}_z$作用</li>
<li>$\hat{H}^{\prime}&#x3D;-(-e\vec{r})\cdot\vec{\varepsilon}&#x3D;e\varepsilon r\cos\theta$<ul>
<li>设外电场足够弱，可视作微扰</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="量子态随时间的演化"><a href="#量子态随时间的演化" class="headerlink" title="量子态随时间的演化"></a>量子态随时间的演化</h3><ul>
<li>Hamilton量不含时的体系</li>
</ul>
<h1 id="统计部分"><a href="#统计部分" class="headerlink" title="统计部分"></a>统计部分</h1><h3 id="近独立粒子系统的统计分布"><a href="#近独立粒子系统的统计分布" class="headerlink" title="近独立粒子系统的统计分布"></a>近独立粒子系统的统计分布</h3><h3 id="粒子和系统状态的量子力学描述"><a href="#粒子和系统状态的量子力学描述" class="headerlink" title="粒子和系统状态的量子力学描述"></a>粒子和系统状态的量子力学描述</h3><h3 id="分布与微观状态数"><a href="#分布与微观状态数" class="headerlink" title="分布与微观状态数"></a>分布与微观状态数</h3><h3 id="玻尔兹曼分布"><a href="#玻尔兹曼分布" class="headerlink" title="玻尔兹曼分布"></a>玻尔兹曼分布</h3><p>粒子在量子本性上是全同的，但是为可以区分的，单粒子态上可容纳的粒子数是不受限制的</p>
<h3 id="Bose-分布"><a href="#Bose-分布" class="headerlink" title="Bose 分布"></a>Bose 分布</h3><p>由全同的Bose子构成，与玻尔兹曼分布的区别在于，认为粒子不可区分，单粒子态上可容纳的粒子数也是不受限制的</p>
<h3 id="Fermi-分布"><a href="#Fermi-分布" class="headerlink" title="Fermi 分布"></a>Fermi 分布</h3><p>由全同的Fermi子构成，单粒子态只能容纳一个粒子</p>
<h3 id="半经典分布和玻尔兹曼分布"><a href="#半经典分布和玻尔兹曼分布" class="headerlink" title="半经典分布和玻尔兹曼分布"></a>半经典分布和玻尔兹曼分布</h3><ul>
<li>半经典分布与玻尔兹曼分布一致，但是在半经典统计中，粒子被认为是不可区分的</li>
</ul>
<h3 id="乘子的物理意义"><a href="#乘子的物理意义" class="headerlink" title="乘子的物理意义"></a>乘子的物理意义</h3><ul>
<li>$\beta$的物理意义</li>
<li>$\alpha$的物理意义</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>量子与统计习题课</title>
    <url>/2023/11/21/liang-zi-yu-tong-ji-xi-ti-ke/</url>
    <content><![CDATA[<p>课程内容[[量子与统计]]</p>
<span id="more"></span>
<h1 id="第一次习题课"><a href="#第一次习题课" class="headerlink" title="第一次习题课"></a>第一次习题课</h1><h3 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h3><h4 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h4><ul>
<li><p>量子态</p>
</li>
<li><p>定态</p>
</li>
<li><p>束缚态</p>
</li>
<li><p>散射态</p>
</li>
<li><p>宇称</p>
</li>
<li><p>德布罗意假设</p>
</li>
<li><p>隧穿（隧道效应）</p>
</li>
</ul>
<h4 id="原理与假设（书后）"><a href="#原理与假设（书后）" class="headerlink" title="原理与假设（书后）"></a>原理与假设（书后）</h4><ul>
<li><p>波粒二象性</p>
</li>
<li><p>量子力学基本假定*</p>
</li>
<li><p>量子态叠加</p>
</li>
<li><p>不确定度*</p>
</li>
<li><p>互补</p>
</li>
<li><p>对应</p>
</li>
</ul>
<h4 id="重点模型"><a href="#重点模型" class="headerlink" title="重点模型"></a>重点模型</h4><ul>
<li><p>自由粒子模型</p>
</li>
<li><p>无限深势阱&#x2F;有限深</p>
</li>
<li><p>\delta势模型</p>
</li>
<li><p>定点、定轴转动</p>
</li>
<li><p>一维二维三维线性谐振子</p>
</li>
<li><p>氢原子（能级公式、波函数特征、电流密度及磁矩）</p>
</li>
</ul>
<h4 id="两类问题"><a href="#两类问题" class="headerlink" title="两类问题"></a>两类问题</h4><ul>
<li><p>束缚态问题</p>
</li>
<li><p>散射问题</p>
</li>
</ul>
<h4 id="讨论题"><a href="#讨论题" class="headerlink" title="讨论题"></a>讨论题</h4><h5 id="随想"><a href="#随想" class="headerlink" title="随想"></a>随想</h5><ul>
<li><p>束缚态与坐标关系的问题</p>
</li>
<li><p>力学量完全集、守恒量完全集（是力学量完全集的子集）</p>
</li>
</ul>
<h5 id="本征方程的建立与求解"><a href="#本征方程的建立与求解" class="headerlink" title="本征方程的建立与求解"></a>本征方程的建立与求解</h5><ul>
<li><p>定轴转动模型</p>
<ul>
<li>$\mu$在平面上沿R绕z轴转动，初始时刻粒子波函数为$\Psi(\psi,0)&#x3D;Acos^2\psi$，求解下列问题<ul>
<li>写出这个系统的哈密顿量算符</li>
<li>求出粒子的能级和对应的归一化能量本征函数，给出各能级的简并度</li>
<li>粒子在任意$_{t\geq0}$时刻的波函数以及处于第二能量激发态的概率</li>
<li>角动量的可能值、相应的概率及平均值，并回答这些值是否受时间影响</li>
</ul>
</li>
</ul>
</li>
<li><p>线性谐振子模型</p>
<ul>
<li>荷电$\mathscr{q}$的谐振子,受到外电场$\mathscr{\varepsilon}的作用 V\left(x\right)&#x3D;\frac12m\omega^2x^2-q\varepsilon x$</li>
</ul>
</li>
<li><p>求束缚态能量本征值和本征函数</p>
</li>
<li><p>散射问题</p>
<ul>
<li><p>考虑双 \delta 函数势，其中\alpha 和\alpha 都是正的常数</p>
</li>
<li><p>$$<br>  V\left(x\right)&#x3D;-\alpha{\left[\delta\left(x+a\right)+\delta\left(x-a\right)\right]}<br>  $$</p>
<ul>
<li><p>画出这个势</p>
</li>
<li><p>若考虑具有能量为E&gt;0的粒子入射该势垒，求透射率</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="算符理论、测量、不确定度关系"><a href="#算符理论、测量、不确定度关系" class="headerlink" title="算符理论、测量、不确定度关系"></a>算符理论、测量、不确定度关系</h5><ul>
<li><p>一些概念</p>
<ul>
<li><p>厄米算符定义、性质、判定及构造</p>
</li>
<li><p>算符对易关系的计算</p>
</li>
<li><p>平均值的计算</p>
</li>
<li><p>对测量的理解和应用</p>
</li>
<li><p>对不确定关系的理解及应用</p>
</li>
</ul>
</li>
<li><p>参考例题</p>
<ul>
<li><p>连续测量问题</p>
<ul>
<li><p>已知某微观体系的力学量 A有两个归一化本征态 \psi_{1}和 \psi_{2}, 相应的本征值为a_1和a_2.力学量B有两个归一化本征态\phi_{1}和\phi_{2}, 相应的本征值为b_1和b_2.两组本征态有如下关系</p>
</li>
<li><p>$\psi_1&#x3D;\frac15(3\phi_1+4\phi_2),\quad\psi_2&#x3D;\frac15(4\phi_1-3\phi_2)$</p>
</li>
<li><p>测量可观测量 A, 所得结果为 \alpha_{1} ,那么在测量之后(瞬时)体系处于什么态？</p>
</li>
<li><p>如果现在再测量 B,可能的结果是什么？ 它们出现的概率是多少？</p>
</li>
<li><p>在恰好测出 B 后，再次测量 A ,那么结果为\alpha_{1}的概率为多少？</p>
</li>
</ul>
</li>
<li><p>守恒量判定</p>
<ul>
<li><p>设某一量子体系的Hamilton量为$\widehat{H}&#x3D;\frac{1}{2m}\widehat{P}+\omega\widehat{ L}_{z}$,其中 m 为质量，\omega 为频率. 分析并给出下列这些力学量中哪些是守恒量？</p>
</li>
<li><p>$H,p_x,p_y,p_z,p^2,\hat{L}_x,\hat{L}_y,\hat{L}_z,\hat{L}^2$</p>
</li>
</ul>
</li>
<li><p>测量假设、不确定性关系</p>
</li>
</ul>
</li>
</ul>
<h5 id="矩阵力学及表象理论"><a href="#矩阵力学及表象理论" class="headerlink" title="矩阵力学及表象理论"></a>矩阵力学及表象理论</h5><ul>
<li><p>重点内容</p>
<ul>
<li><p>量子态的矩阵表示</p>
</li>
<li><p>力学量的矩阵表示</p>
</li>
<li><p>本征方程（矩阵形式）的求解、平均值的计算</p>
</li>
</ul>
</li>
<li><p>参考例题</p>
<ul>
<li><p>表象与矩阵元的求解</p>
<ul>
<li><p>设一维粒子Hamilton量为H&#x3D;\frac{p_x^2}{2m}+V(x)</p>
</li>
<li><p>分别写出在\hat{x} 和p_x表象中，\hat{x},\hat{P}_x,\hat{H} 的矩阵元.</p>
</li>
</ul>
</li>
<li><p>设粒子处于下列势阱$V\left(x\right)&#x3D;\begin{cases}\infty,&amp;x&lt;0;\Fx,&amp;x&gt;0,\end{cases}$其中^{F}为常数，求粒子的能量本征函数。</p>
</li>
</ul>
</li>
</ul>
<h5 id="自旋与角动量理论"><a href="#自旋与角动量理论" class="headerlink" title="自旋与角动量理论"></a>自旋与角动量理论</h5><h3 id="综合练习"><a href="#综合练习" class="headerlink" title="综合练习"></a>综合练习</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
</search>
